{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with Pandas and Numpy\n",
    "\n",
    "This notebook provides a comprehensive guide to data manipulation using two of Python's most powerful libraries: NumPy and Pandas. You'll learn how to efficiently work with arrays, dataframes, clean data, perform transformations, and prepare datasets for analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to NumPy Arrays and Operations\n",
    "\n",
    "NumPy (Numerical Python) is the fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays efficiently.\n",
    "\n",
    "### Why NumPy?\n",
    "- **Speed**: NumPy arrays are stored more efficiently and operations are significantly faster than Python lists\n",
    "- **Vectorization**: Perform operations on entire arrays without explicit loops\n",
    "- **Broadcasting**: Perform operations on arrays of different shapes\n",
    "- **Foundation**: NumPy is the foundation for Pandas, SciPy, scikit-learn, and many other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy library\n",
    "import numpy as np\n",
    "\n",
    "# Display NumPy version\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NumPy Arrays\n",
    "\n",
    "There are multiple ways to create NumPy arrays depending on your data source and needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays from Python lists\n",
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "print(\"1D Array:\", arr1)\n",
    "print(\"Data type:\", arr1.dtype)\n",
    "\n",
    "# Creating a 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"\\n2D Array:\\n\", arr2)\n",
    "print(\"Shape:\", arr2.shape)  # Returns (rows, columns)\n",
    "print(\"Dimensions:\", arr2.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays with built-in functions\n",
    "\n",
    "# Array of zeros\n",
    "zeros = np.zeros((3, 4))  # 3 rows, 4 columns\n",
    "print(\"Zeros array:\\n\", zeros)\n",
    "\n",
    "# Array of ones\n",
    "ones = np.ones((2, 3))\n",
    "print(\"\\nOnes array:\\n\", ones)\n",
    "\n",
    "# Identity matrix\n",
    "identity = np.eye(3)\n",
    "print(\"\\nIdentity matrix:\\n\", identity)\n",
    "\n",
    "# Array with a range of values\n",
    "range_arr = np.arange(0, 10, 2)  # Start, stop, step\n",
    "print(\"\\nRange array:\", range_arr)\n",
    "\n",
    "# Array with evenly spaced values\n",
    "linspace_arr = np.linspace(0, 1, 5)  # Start, stop, number of values\n",
    "print(\"\\nLinspace array:\", linspace_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random arrays\n",
    "\n",
    "# Random values between 0 and 1\n",
    "random_arr = np.random.random((2, 3))\n",
    "print(\"Random array:\\n\", random_arr)\n",
    "\n",
    "# Random integers\n",
    "random_int = np.random.randint(1, 100, size=(3, 3))  # Between 1 and 100\n",
    "print(\"\\nRandom integers:\\n\", random_int)\n",
    "\n",
    "# Random values from normal distribution\n",
    "normal_arr = np.random.randn(4)  # Mean=0, Std=1\n",
    "print(\"\\nNormal distribution:\", normal_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Operations and Manipulation\n",
    "\n",
    "NumPy allows you to perform element-wise operations efficiently without explicit loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic arithmetic operations (element-wise)\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "\n",
    "print(\"Array a:\", a)\n",
    "print(\"Array b:\", b)\n",
    "\n",
    "print(\"\\nAddition:\", a + b)\n",
    "print(\"Subtraction:\", b - a)\n",
    "print(\"Multiplication:\", a * b)\n",
    "print(\"Division:\", b / a)\n",
    "print(\"Power:\", a ** 2)\n",
    "\n",
    "# Operations with scalars\n",
    "print(\"\\nScalar multiplication:\", a * 10)\n",
    "print(\"Scalar addition:\", a + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal functions (ufuncs)\n",
    "arr = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "print(\"Original array:\", arr)\n",
    "print(\"Square root:\", np.sqrt(arr))\n",
    "print(\"Exponential:\", np.exp(arr[:3]))  # First 3 elements only\n",
    "print(\"Logarithm:\", np.log(arr))\n",
    "print(\"Sine:\", np.sin(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical operations\n",
    "data = np.array([12, 15, 18, 22, 25, 30, 35, 40])\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(\"\\nMean:\", np.mean(data))\n",
    "print(\"Median:\", np.median(data))\n",
    "print(\"Standard deviation:\", np.std(data))\n",
    "print(\"Variance:\", np.var(data))\n",
    "print(\"Min:\", np.min(data))\n",
    "print(\"Max:\", np.max(data))\n",
    "print(\"Sum:\", np.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array indexing and slicing\n",
    "arr = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "\n",
    "print(\"Original array:\", arr)\n",
    "print(\"First element:\", arr[0])\n",
    "print(\"Last element:\", arr[-1])\n",
    "print(\"Slice [2:5]:\", arr[2:5])  # Elements at index 2, 3, 4\n",
    "print(\"Every other element:\", arr[::2])\n",
    "print(\"Reversed:\", arr[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array indexing\n",
    "matrix = np.array([[1, 2, 3], \n",
    "                   [4, 5, 6], \n",
    "                   [7, 8, 9]])\n",
    "\n",
    "print(\"Matrix:\\n\", matrix)\n",
    "print(\"\\nElement at [0, 0]:\", matrix[0, 0])  # First row, first column\n",
    "print(\"Element at [1, 2]:\", matrix[1, 2])  # Second row, third column\n",
    "print(\"\\nFirst row:\", matrix[0, :])  # All columns of first row\n",
    "print(\"Second column:\", matrix[:, 1])  # All rows of second column\n",
    "print(\"\\nSubmatrix:\\n\", matrix[0:2, 1:3])  # First 2 rows, columns 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing (filtering)\n",
    "scores = np.array([45, 78, 92, 65, 88, 55, 73, 95])\n",
    "\n",
    "print(\"All scores:\", scores)\n",
    "\n",
    "# Create a boolean mask\n",
    "passing = scores >= 60\n",
    "print(\"\\nPassing mask:\", passing)\n",
    "\n",
    "# Filter using the mask\n",
    "passing_scores = scores[passing]\n",
    "print(\"Passing scores:\", passing_scores)\n",
    "\n",
    "# One-line filtering\n",
    "high_scores = scores[scores > 80]\n",
    "print(\"High scores (>80):\", high_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping arrays\n",
    "arr = np.arange(12)  # Creates [0, 1, 2, ..., 11]\n",
    "print(\"Original array:\", arr)\n",
    "print(\"Shape:\", arr.shape)\n",
    "\n",
    "# Reshape to 2D\n",
    "reshaped = arr.reshape(3, 4)  # 3 rows, 4 columns\n",
    "print(\"\\nReshaped (3x4):\\n\", reshaped)\n",
    "\n",
    "# Reshape to 3D\n",
    "reshaped_3d = arr.reshape(2, 2, 3)  # 2 blocks, 2 rows, 3 columns\n",
    "print(\"\\nReshaped (2x2x3):\\n\", reshaped_3d)\n",
    "\n",
    "# Flatten back to 1D\n",
    "flattened = reshaped.flatten()\n",
    "print(\"\\nFlattened:\", flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Concatenate\n",
    "concatenated = np.concatenate([a, b])\n",
    "print(\"Concatenated:\", concatenated)\n",
    "\n",
    "# Vertical stack (rows)\n",
    "vstacked = np.vstack([a, b])\n",
    "print(\"\\nVertically stacked:\\n\", vstacked)\n",
    "\n",
    "# Horizontal stack (columns)\n",
    "hstacked = np.hstack([a, b])\n",
    "print(\"\\nHorizontally stacked:\", hstacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Pandas Series and DataFrames\n",
    "\n",
    "Pandas is built on top of NumPy and provides two primary data structures: **Series** (1-dimensional) and **DataFrame** (2-dimensional). These structures are designed specifically for data analysis and manipulation.\n",
    "\n",
    "### Why Pandas?\n",
    "- **Labeled data**: Work with row and column labels instead of just numeric indices\n",
    "- **Heterogeneous data**: Store different data types in the same structure\n",
    "- **Missing data handling**: Built-in methods for dealing with missing values\n",
    "- **Data alignment**: Automatic alignment based on labels during operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Display Pandas version\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series\n",
    "\n",
    "A Series is a one-dimensional labeled array that can hold any data type. Think of it as a column in a spreadsheet or a labeled NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series from a list\n",
    "temperatures = pd.Series([22, 25, 28, 24, 26])\n",
    "print(\"Temperature Series:\")\n",
    "print(temperatures)\n",
    "print(\"\\nData type:\", temperatures.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series with custom index\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "temperatures = pd.Series([22, 25, 28, 24, 26], index=days)\n",
    "print(\"Temperature by day:\")\n",
    "print(temperatures)\n",
    "\n",
    "# Accessing values by label\n",
    "print(\"\\nWednesday temperature:\", temperatures['Wed'])\n",
    "\n",
    "# Accessing values by position\n",
    "print(\"First temperature:\", temperatures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series from a dictionary\n",
    "population = {\n",
    "    'Tokyo': 37400000,\n",
    "    'Delhi': 30290000,\n",
    "    'Shanghai': 27058000,\n",
    "    'Mumbai': 20410000\n",
    "}\n",
    "\n",
    "pop_series = pd.Series(population)\n",
    "print(\"City populations:\")\n",
    "print(pop_series)\n",
    "\n",
    "# Series attributes\n",
    "print(\"\\nValues:\", pop_series.values)  # Returns NumPy array\n",
    "print(\"Index:\", pop_series.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series operations\n",
    "prices = pd.Series([100, 200, 150, 300], index=['A', 'B', 'C', 'D'])\n",
    "\n",
    "print(\"Original prices:\")\n",
    "print(prices)\n",
    "\n",
    "# Arithmetic operations\n",
    "print(\"\\nAfter 10% discount:\")\n",
    "print(prices * 0.9)\n",
    "\n",
    "# Boolean filtering\n",
    "print(\"\\nPrices above 150:\")\n",
    "print(prices[prices > 150])\n",
    "\n",
    "# Statistical methods\n",
    "print(\"\\nMean price:\", prices.mean())\n",
    "print(\"Max price:\", prices.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DataFrames\n",
    "\n",
    "A DataFrame is a 2-dimensional labeled data structure with columns that can be of different types. It's similar to a spreadsheet, SQL table, or a dictionary of Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Mumbai'],\n",
    "    'Salary': [70000, 80000, 75000, 90000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Employee DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame attributes and methods\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nIndex:\", df.index.tolist())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "\n",
    "# Quick statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing data\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nLast 2 rows:\")\n",
    "print(df.tail(2))\n",
    "\n",
    "# Getting information about the DataFrame\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing columns\n",
    "print(\"Names column:\")\n",
    "print(df['Name'])  # Returns a Series\n",
    "\n",
    "# Alternative syntax (works only if column name has no spaces)\n",
    "print(\"\\nAges:\")\n",
    "print(df.Age)\n",
    "\n",
    "# Accessing multiple columns\n",
    "print(\"\\nName and City:\")\n",
    "print(df[['Name', 'City']])  # Returns a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "df['Department'] = ['IT', 'HR', 'IT', 'Finance', 'HR']\n",
    "print(\"After adding Department:\")\n",
    "print(df)\n",
    "\n",
    "# Creating calculated columns\n",
    "df['Salary_Thousands'] = df['Salary'] / 1000\n",
    "print(\"\\nWith calculated column:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting columns\n",
    "df_copy = df.copy()  # Create a copy to preserve original\n",
    "\n",
    "# Method 1: Using drop (returns new DataFrame)\n",
    "df_dropped = df_copy.drop('Salary_Thousands', axis=1)\n",
    "print(\"After dropping column:\")\n",
    "print(df_dropped)\n",
    "\n",
    "# Method 2: Using del (modifies in-place)\n",
    "del df_copy['Salary_Thousands']\n",
    "print(\"\\nUsing del:\")\n",
    "print(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading and Exploration\n",
    "\n",
    "Pandas can read data from various file formats including CSV, Excel, JSON, SQL databases, and more. This section covers the most common data loading scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Data for Examples\n",
    "\n",
    "First, let's create sample datasets that we'll use throughout this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = {\n",
    "    'Date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch'], 100),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'Sales': np.random.randint(100, 1000, 100),\n",
    "    'Quantity': np.random.randint(1, 20, 100)\n",
    "}\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "print(\"Sample sales data created:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) is one of the most common file formats for data exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV for demonstration\n",
    "sales_df.to_csv('sales_data.csv', index=False)\n",
    "print(\"CSV file created successfully!\")\n",
    "\n",
    "# Read CSV file\n",
    "df_from_csv = pd.read_csv('sales_data.csv')\n",
    "print(\"\\nData loaded from CSV:\")\n",
    "print(df_from_csv.head())\n",
    "print(f\"\\nShape: {df_from_csv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV reading with options\n",
    "\n",
    "# Read only first 10 rows\n",
    "df_limited = pd.read_csv('sales_data.csv', nrows=10)\n",
    "print(\"First 10 rows only:\")\n",
    "print(df_limited)\n",
    "\n",
    "# Read specific columns\n",
    "df_columns = pd.read_csv('sales_data.csv', usecols=['Product', 'Sales', 'Quantity'])\n",
    "print(\"\\nSpecific columns:\")\n",
    "print(df_columns.head())\n",
    "\n",
    "# Read with Date column as datetime\n",
    "df_dates = pd.read_csv('sales_data.csv', parse_dates=['Date'])\n",
    "print(\"\\nWith parsed dates:\")\n",
    "print(df_dates.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Excel Files\n",
    "\n",
    "Pandas can read and write Excel files (.xlsx, .xls) using the `openpyxl` or `xlrd` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to Excel (requires openpyxl)\n",
    "try:\n",
    "    sales_df.to_excel('sales_data.xlsx', sheet_name='Sales', index=False)\n",
    "    print(\"Excel file created successfully!\")\n",
    "    \n",
    "    # Read Excel file\n",
    "    df_from_excel = pd.read_excel('sales_data.xlsx', sheet_name='Sales')\n",
    "    print(\"\\nData loaded from Excel:\")\n",
    "    print(df_from_excel.head())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Note: Install openpyxl to work with Excel files\")\n",
    "    print(\"Run: pip install openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Other Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON\n",
    "json_data = sales_df.head().to_json(orient='records')\n",
    "df_from_json = pd.read_json(json_data)\n",
    "print(\"From JSON:\")\n",
    "print(df_from_json)\n",
    "\n",
    "# Reading from dictionary\n",
    "dict_data = {'A': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "df_from_dict = pd.DataFrame(dict_data)\n",
    "print(\"\\nFrom dictionary:\")\n",
    "print(df_from_dict)\n",
    "\n",
    "# Reading from clipboard (useful for quick data pasting)\n",
    "# df_from_clipboard = pd.read_clipboard()  # Uncomment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration Methods\n",
    "\n",
    "Once data is loaded, it's essential to explore and understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our sample data for exploration\n",
    "df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMemory usage:\")\n",
    "print(df.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Include non-numeric columns\n",
    "print(\"\\nAll columns summary:\")\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((df.isnull().sum() / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical columns\n",
    "print(\"Product distribution:\")\n",
    "print(df['Product'].value_counts())\n",
    "\n",
    "print(\"\\nRegion distribution:\")\n",
    "print(df['Region'].value_counts())\n",
    "\n",
    "# Proportions instead of counts\n",
    "print(\"\\nProduct proportions:\")\n",
    "print(df['Product'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values\n",
    "print(\"Unique products:\", df['Product'].unique())\n",
    "print(\"Number of unique products:\", df['Product'].nunique())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Selection and Indexing\n",
    "\n",
    "Pandas provides multiple ways to select and filter data. Understanding these methods is crucial for efficient data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label-based Indexing with .loc\n",
    "\n",
    "`.loc[]` is used for label-based indexing. It selects data based on row and column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame with custom index\n",
    "employees = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'Department': ['IT', 'HR', 'IT', 'Finance', 'HR'],\n",
    "    'Salary': [70000, 60000, 75000, 80000, 65000]\n",
    "}, index=['E001', 'E002', 'E003', 'E004', 'E005'])\n",
    "\n",
    "print(\"Employee DataFrame:\")\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single row by label\n",
    "print(\"Employee E003:\")\n",
    "print(employees.loc['E003'])\n",
    "\n",
    "# Select multiple rows\n",
    "print(\"\\nEmployees E001 and E003:\")\n",
    "print(employees.loc[['E001', 'E003']])\n",
    "\n",
    "# Select range of rows (inclusive)\n",
    "print(\"\\nEmployees E002 to E004:\")\n",
    "print(employees.loc['E002':'E004'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific rows and columns\n",
    "print(\"Name and Age for E001 and E005:\")\n",
    "print(employees.loc[['E001', 'E005'], ['Name', 'Age']])\n",
    "\n",
    "# Select all rows with specific columns\n",
    "print(\"\\nAll employees' names and salaries:\")\n",
    "print(employees.loc[:, ['Name', 'Salary']])\n",
    "\n",
    "# Select specific rows with all columns\n",
    "print(\"\\nEmployee E002 (all details):\")\n",
    "print(employees.loc['E002', :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying data with .loc\n",
    "employees_copy = employees.copy()\n",
    "\n",
    "# Update a single value\n",
    "employees_copy.loc['E001', 'Salary'] = 72000\n",
    "print(\"After updating E001's salary:\")\n",
    "print(employees_copy.loc['E001'])\n",
    "\n",
    "# Update multiple values\n",
    "employees_copy.loc['E002', ['Age', 'Salary']] = [31, 62000]\n",
    "print(\"\\nAfter updating E002:\")\n",
    "print(employees_copy.loc['E002'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-based Indexing with .iloc\n",
    "\n",
    "`.iloc[]` is used for integer position-based indexing. It works with integer positions (0-based) like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by position\n",
    "print(\"First row (position 0):\")\n",
    "print(employees.iloc[0])\n",
    "\n",
    "# Select multiple rows by position\n",
    "print(\"\\nFirst and third rows:\")\n",
    "print(employees.iloc[[0, 2]])\n",
    "\n",
    "# Select range of rows (exclusive end)\n",
    "print(\"\\nRows 1 to 3 (positions 1, 2):\")\n",
    "print(employees.iloc[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows and columns by position\n",
    "print(\"First two rows, first two columns:\")\n",
    "print(employees.iloc[0:2, 0:2])\n",
    "\n",
    "# Select specific positions\n",
    "print(\"\\nRows 0 and 2, columns 1 and 3:\")\n",
    "print(employees.iloc[[0, 2], [1, 3]])\n",
    "\n",
    "# Get last row\n",
    "print(\"\\nLast row:\")\n",
    "print(employees.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining slicing techniques\n",
    "print(\"Every other row, all columns:\")\n",
    "print(employees.iloc[::2, :])\n",
    "\n",
    "print(\"\\nAll rows, every other column:\")\n",
    "print(employees.iloc[:, ::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing (Filtering)\n",
    "\n",
    "Boolean indexing allows you to filter data based on conditions. This is one of the most powerful features for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple condition\n",
    "print(\"Employees with salary > 65000:\")\n",
    "high_salary = employees[employees['Salary'] > 65000]\n",
    "print(high_salary)\n",
    "\n",
    "# Multiple conditions (AND)\n",
    "print(\"\\nIT employees with age > 30:\")\n",
    "it_senior = employees[(employees['Department'] == 'IT') & (employees['Age'] > 30)]\n",
    "print(it_senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions (OR)\n",
    "print(\"HR employees OR salary > 70000:\")\n",
    "hr_or_high = employees[(employees['Department'] == 'HR') | (employees['Salary'] > 70000)]\n",
    "print(hr_or_high)\n",
    "\n",
    "# NOT condition\n",
    "print(\"\\nNon-IT employees:\")\n",
    "non_it = employees[~(employees['Department'] == 'IT')]\n",
    "print(non_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isin() for multiple values\n",
    "print(\"IT or HR employees:\")\n",
    "it_hr = employees[employees['Department'].isin(['IT', 'HR'])]\n",
    "print(it_hr)\n",
    "\n",
    "# String methods for filtering\n",
    "print(\"\\nEmployees whose name starts with 'A' or 'C':\")\n",
    "names_ac = employees[employees['Name'].str.startswith(('A', 'C'))]\n",
    "print(names_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between condition\n",
    "print(\"Employees aged between 28 and 32 (inclusive):\")\n",
    "age_range = employees[employees['Age'].between(28, 32)]\n",
    "print(age_range)\n",
    "\n",
    "# Combining .loc with boolean indexing\n",
    "print(\"\\nNames of employees with salary < 70000:\")\n",
    "low_salary_names = employees.loc[employees['Salary'] < 70000, 'Name']\n",
    "print(low_salary_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Method\n",
    "\n",
    "The `.query()` method provides a convenient way to filter data using string expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "print(\"Salary > 65000 using query:\")\n",
    "print(employees.query('Salary > 65000'))\n",
    "\n",
    "# Multiple conditions\n",
    "print(\"\\nAge > 28 and Department == 'IT':\")\n",
    "print(employees.query('Age > 28 and Department == \"IT\"'))\n",
    "\n",
    "# Using variables in query\n",
    "min_salary = 70000\n",
    "print(f\"\\nSalary >= {min_salary}:\")\n",
    "print(employees.query('Salary >= @min_salary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Cleaning\n",
    "\n",
    "Real-world data is often messy and requires cleaning before analysis. This section covers handling missing values, duplicates, and data type conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Missing data is represented as `NaN` (Not a Number) in Pandas. It's crucial to identify and handle missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "data_with_nulls = {\n",
    "    'Name': ['Alice', 'Bob', None, 'David', 'Eve', 'Frank'],\n",
    "    'Age': [25, np.nan, 35, 28, np.nan, 40],\n",
    "    'City': ['NYC', 'LA', 'Chicago', None, 'Boston', 'Seattle'],\n",
    "    'Salary': [70000, 60000, np.nan, 80000, 65000, 75000]\n",
    "}\n",
    "\n",
    "df_nulls = pd.DataFrame(data_with_nulls)\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting missing values\n",
    "print(\"Is null (True where missing):\")\n",
    "print(df_nulls.isnull())\n",
    "\n",
    "print(\"\\nCount of missing values per column:\")\n",
    "print(df_nulls.isnull().sum())\n",
    "\n",
    "print(\"\\nTotal missing values:\", df_nulls.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with any missing values\n",
    "print(\"After dropping rows with ANY missing values:\")\n",
    "df_dropped_any = df_nulls.dropna()\n",
    "print(df_dropped_any)\n",
    "print(f\"Rows remaining: {len(df_dropped_any)} out of {len(df_nulls)}\")\n",
    "\n",
    "# Dropping rows where ALL values are missing\n",
    "print(\"\\nDropping rows where ALL values are missing:\")\n",
    "df_dropped_all = df_nulls.dropna(how='all')\n",
    "print(df_dropped_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows based on specific columns\n",
    "print(\"Drop rows where 'Name' is missing:\")\n",
    "df_dropped_name = df_nulls.dropna(subset=['Name'])\n",
    "print(df_dropped_name)\n",
    "\n",
    "# Dropping columns with missing values\n",
    "print(\"\\nDrop columns with any missing values:\")\n",
    "df_dropped_cols = df_nulls.dropna(axis=1)\n",
    "print(df_dropped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with a constant\n",
    "df_filled = df_nulls.copy()\n",
    "\n",
    "# Fill all missing values with a specific value\n",
    "df_filled_zero = df_filled.fillna(0)\n",
    "print(\"Filled with 0:\")\n",
    "print(df_filled_zero)\n",
    "\n",
    "# Fill different columns with different values\n",
    "df_filled_custom = df_nulls.fillna({\n",
    "    'Name': 'Unknown',\n",
    "    'Age': df_nulls['Age'].mean(),  # Mean age\n",
    "    'City': 'Not Specified',\n",
    "    'Salary': df_nulls['Salary'].median()  # Median salary\n",
    "})\n",
    "print(\"\\nFilled with custom values:\")\n",
    "print(df_filled_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill and backward fill\n",
    "time_series = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=6),\n",
    "    'Value': [100, np.nan, np.nan, 200, np.nan, 300]\n",
    "})\n",
    "\n",
    "print(\"Original time series:\")\n",
    "print(time_series)\n",
    "\n",
    "# Forward fill (use previous valid value)\n",
    "print(\"\\nForward fill:\")\n",
    "print(time_series.fillna(method='ffill'))\n",
    "\n",
    "# Backward fill (use next valid value)\n",
    "print(\"\\nBackward fill:\")\n",
    "print(time_series.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation for numeric data\n",
    "print(\"Linear interpolation:\")\n",
    "time_series['Value_Interpolated'] = time_series['Value'].interpolate()\n",
    "print(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates\n",
    "\n",
    "Duplicate rows can skew analysis results and should be identified and handled appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with duplicates\n",
    "data_with_dupes = {\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'David'],\n",
    "    'Age': [25, 30, 25, 35, 30, 28],\n",
    "    'City': ['NYC', 'LA', 'NYC', 'Chicago', 'LA', 'Boston']\n",
    "}\n",
    "\n",
    "df_dupes = pd.DataFrame(data_with_dupes)\n",
    "print(\"DataFrame with duplicates:\")\n",
    "print(df_dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicates\n",
    "print(\"Duplicate rows (True if duplicate):\")\n",
    "print(df_dupes.duplicated())\n",
    "\n",
    "print(\"\\nNumber of duplicate rows:\", df_dupes.duplicated().sum())\n",
    "\n",
    "# Show duplicate rows\n",
    "print(\"\\nActual duplicate rows:\")\n",
    "print(df_dupes[df_dupes.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates (keeps first occurrence)\n",
    "df_no_dupes = df_dupes.drop_duplicates()\n",
    "print(\"After removing duplicates:\")\n",
    "print(df_no_dupes)\n",
    "\n",
    "# Keep last occurrence instead\n",
    "df_keep_last = df_dupes.drop_duplicates(keep='last')\n",
    "print(\"\\nKeeping last occurrence:\")\n",
    "print(df_keep_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates based on specific columns\n",
    "print(\"Duplicates based on 'Name' only:\")\n",
    "print(df_dupes[df_dupes.duplicated(subset=['Name'])])\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "df_unique_names = df_dupes.drop_duplicates(subset=['Name'])\n",
    "print(\"\\nUnique names only:\")\n",
    "print(df_unique_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Conversion\n",
    "\n",
    "Ensuring correct data types is essential for proper analysis and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with mixed types\n",
    "mixed_data = {\n",
    "    'ID': ['1', '2', '3', '4', '5'],\n",
    "    'Price': ['100.5', '200.3', '150.0', '300.7', '250.2'],\n",
    "    'Quantity': ['10', '20', '15', '30', '25'],\n",
    "    'Date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05']\n",
    "}\n",
    "\n",
    "df_mixed = pd.DataFrame(mixed_data)\n",
    "print(\"Original data types:\")\n",
    "print(df_mixed.dtypes)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to appropriate types\n",
    "df_converted = df_mixed.copy()\n",
    "\n",
    "# Convert to numeric\n",
    "df_converted['ID'] = df_converted['ID'].astype(int)\n",
    "df_converted['Price'] = df_converted['Price'].astype(float)\n",
    "df_converted['Quantity'] = pd.to_numeric(df_converted['Quantity'])\n",
    "\n",
    "# Convert to datetime\n",
    "df_converted['Date'] = pd.to_datetime(df_converted['Date'])\n",
    "\n",
    "print(\"Converted data types:\")\n",
    "print(df_converted.dtypes)\n",
    "print(\"\\nConverted DataFrame:\")\n",
    "print(df_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling errors during conversion\n",
    "messy_numbers = pd.Series(['1', '2', 'three', '4', '5'])\n",
    "\n",
    "# This will raise an error: messy_numbers.astype(int)\n",
    "\n",
    "# Use to_numeric with error handling\n",
    "clean_numbers = pd.to_numeric(messy_numbers, errors='coerce')  # Invalid = NaN\n",
    "print(\"With 'coerce' (invalid become NaN):\")\n",
    "print(clean_numbers)\n",
    "\n",
    "# Ignore errors (keep original)\n",
    "ignore_errors = pd.to_numeric(messy_numbers, errors='ignore')\n",
    "print(\"\\nWith 'ignore' (keep original):\")\n",
    "print(ignore_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Cleaning\n",
    "\n",
    "Text data often requires cleaning to standardize format and remove unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with messy strings\n",
    "messy_strings = pd.DataFrame({\n",
    "    'Name': ['  Alice  ', 'BOB', 'charlie', '  DAVID'],\n",
    "    'Email': ['alice@GMAIL.com', 'bob@yahoo.COM', 'charlie@Gmail.Com', 'david@YAHOO.com']\n",
    "})\n",
    "\n",
    "print(\"Messy strings:\")\n",
    "print(messy_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String cleaning operations\n",
    "cleaned = messy_strings.copy()\n",
    "\n",
    "# Remove leading/trailing whitespace\n",
    "cleaned['Name'] = cleaned['Name'].str.strip()\n",
    "\n",
    "# Standardize case\n",
    "cleaned['Name'] = cleaned['Name'].str.title()  # Title Case\n",
    "cleaned['Email'] = cleaned['Email'].str.lower()  # Lowercase\n",
    "\n",
    "print(\"Cleaned strings:\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More string operations\n",
    "text_data = pd.Series(['Hello World', 'Python-Programming', 'Data_Science'])\n",
    "\n",
    "print(\"Original:\", text_data.tolist())\n",
    "print(\"Replace space with underscore:\", text_data.str.replace(' ', '_').tolist())\n",
    "print(\"Replace dash and underscore:\", text_data.str.replace('[-_]', ' ', regex=True).tolist())\n",
    "print(\"Extract first word:\", text_data.str.split().str[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Transformation\n",
    "\n",
    "Data transformation involves reshaping, sorting, grouping, and aggregating data to extract insights and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data\n",
    "\n",
    "Sorting helps organize data for better visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for transformation\n",
    "transform_data = {\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Laptop', 'Phone'],\n",
    "    'Region': ['North', 'South', 'East', 'West', 'South', 'North'],\n",
    "    'Sales': [1000, 1500, 800, 600, 1200, 1300],\n",
    "    'Quantity': [5, 10, 8, 12, 6, 9]\n",
    "}\n",
    "\n",
    "df_transform = pd.DataFrame(transform_data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by single column\n",
    "print(\"Sorted by Sales (ascending):\")\n",
    "print(df_transform.sort_values('Sales'))\n",
    "\n",
    "print(\"\\nSorted by Sales (descending):\")\n",
    "print(df_transform.sort_values('Sales', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns\n",
    "print(\"Sorted by Product (asc) then Sales (desc):\")\n",
    "sorted_df = df_transform.sort_values(['Product', 'Sales'], ascending=[True, False])\n",
    "print(sorted_df)\n",
    "\n",
    "# Sort by index\n",
    "print(\"\\nSorted by index:\")\n",
    "print(sorted_df.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregation\n",
    "\n",
    "GroupBy operations allow you to split data into groups, apply functions, and combine results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple groupby with single aggregation\n",
    "print(\"Total sales by Product:\")\n",
    "product_sales = df_transform.groupby('Product')['Sales'].sum()\n",
    "print(product_sales)\n",
    "\n",
    "print(\"\\nAverage quantity by Region:\")\n",
    "region_avg = df_transform.groupby('Region')['Quantity'].mean()\n",
    "print(region_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "print(\"Multiple statistics by Product:\")\n",
    "product_stats = df_transform.groupby('Product')['Sales'].agg(['sum', 'mean', 'count', 'min', 'max'])\n",
    "print(product_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different aggregations for different columns\n",
    "print(\"Custom aggregations:\")\n",
    "custom_agg = df_transform.groupby('Product').agg({\n",
    "    'Sales': ['sum', 'mean'],\n",
    "    'Quantity': ['sum', 'max']\n",
    "})\n",
    "print(custom_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby multiple columns\n",
    "print(\"Sales by Product and Region:\")\n",
    "product_region = df_transform.groupby(['Product', 'Region'])['Sales'].sum()\n",
    "print(product_region)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "print(\"\\nAs DataFrame:\")\n",
    "print(product_region.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom functions\n",
    "def sales_range(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "print(\"Sales range by Product:\")\n",
    "range_by_product = df_transform.groupby('Product')['Sales'].apply(sales_range)\n",
    "print(range_by_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "Pivot tables provide a spreadsheet-style way to aggregate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "print(\"Pivot table: Sales by Product and Region:\")\n",
    "pivot = df_transform.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0  # Fill missing combinations with 0\n",
    ")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot with multiple aggregations\n",
    "print(\"Pivot with multiple aggregations:\")\n",
    "multi_pivot = df_transform.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc=['sum', 'mean'],\n",
    "    fill_value=0\n",
    ")\n",
    "print(multi_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Calculated Columns\n",
    "\n",
    "Create new columns based on existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple calculated column\n",
    "df_calc = df_transform.copy()\n",
    "df_calc['Revenue_per_Unit'] = df_calc['Sales'] / df_calc['Quantity']\n",
    "print(\"With calculated column:\")\n",
    "print(df_calc)\n",
    "\n",
    "# Using apply with lambda\n",
    "df_calc['Sales_Category'] = df_calc['Sales'].apply(\n",
    "    lambda x: 'High' if x > 1000 else 'Low'\n",
    ")\n",
    "print(\"\\nWith category column:\")\n",
    "print(df_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional column with np.where\n",
    "df_calc['Performance'] = np.where(\n",
    "    df_calc['Sales'] > 1000,\n",
    "    'Excellent',\n",
    "    np.where(df_calc['Sales'] > 800, 'Good', 'Needs Improvement')\n",
    ")\n",
    "print(\"Multi-condition column:\")\n",
    "print(df_calc[['Product', 'Sales', 'Performance']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning and Categorization\n",
    "\n",
    "Convert continuous values into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for sales\n",
    "bins = [0, 800, 1200, 2000]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "df_binned = df_transform.copy()\n",
    "df_binned['Sales_Bin'] = pd.cut(df_binned['Sales'], bins=bins, labels=labels)\n",
    "\n",
    "print(\"With binned sales:\")\n",
    "print(df_binned)\n",
    "\n",
    "print(\"\\nValue counts per bin:\")\n",
    "print(df_binned['Sales_Bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile-based binning\n",
    "df_quantile = df_transform.copy()\n",
    "df_quantile['Sales_Quartile'] = pd.qcut(\n",
    "    df_quantile['Sales'],\n",
    "    q=3,  # Split into 3 equal-sized groups\n",
    "    labels=['Bottom', 'Middle', 'Top']\n",
    ")\n",
    "\n",
    "print(\"Quantile-based bins:\")\n",
    "print(df_quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Merging and Joining Datasets\n",
    "\n",
    "Combining multiple datasets is essential for comprehensive analysis. Pandas provides several methods to merge, join, and concatenate DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "Concatenation stacks DataFrames vertically (rows) or horizontally (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Score': [85, 90, 88]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': [4, 5, 6],\n",
    "    'Name': ['David', 'Eve', 'Frank'],\n",
    "    'Score': [92, 87, 95]\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical concatenation (default)\n",
    "print(\"Concatenated vertically:\")\n",
    "vertical_concat = pd.concat([df1, df2])\n",
    "print(vertical_concat)\n",
    "\n",
    "# Reset index after concatenation\n",
    "print(\"\\nWith reset index:\")\n",
    "print(vertical_concat.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal concatenation\n",
    "df_extra = pd.DataFrame({\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['NYC', 'LA', 'Chicago']\n",
    "})\n",
    "\n",
    "print(\"Concatenated horizontally:\")\n",
    "horizontal_concat = pd.concat([df1, df_extra], axis=1)\n",
    "print(horizontal_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with keys (hierarchical index)\n",
    "print(\"With hierarchical index:\")\n",
    "keyed_concat = pd.concat([df1, df2], keys=['Group1', 'Group2'])\n",
    "print(keyed_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames\n",
    "\n",
    "Merging combines DataFrames based on common columns (similar to SQL joins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames for merging\n",
    "employees = pd.DataFrame({\n",
    "    'EmployeeID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'DepartmentID': [101, 102, 101, 103]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'DepartmentID': [101, 102, 103, 104],\n",
    "    'Department': ['IT', 'HR', 'Finance', 'Marketing'],\n",
    "    'Location': ['NYC', 'LA', 'Chicago', 'Boston']\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (default) - only matching rows\n",
    "print(\"Inner join:\")\n",
    "inner_merge = pd.merge(employees, departments, on='DepartmentID', how='inner')\n",
    "print(inner_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - all rows from left DataFrame\n",
    "print(\"Left join:\")\n",
    "left_merge = pd.merge(employees, departments, on='DepartmentID', how='left')\n",
    "print(left_merge)\n",
    "\n",
    "# Right join - all rows from right DataFrame\n",
    "print(\"\\nRight join:\")\n",
    "right_merge = pd.merge(employees, departments, on='DepartmentID', how='right')\n",
    "print(right_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join - all rows from both DataFrames\n",
    "print(\"Outer join:\")\n",
    "outer_merge = pd.merge(employees, departments, on='DepartmentID', how='outer')\n",
    "print(outer_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on different column names\n",
    "sales = pd.DataFrame({\n",
    "    'EmpID': [1, 2, 3],\n",
    "    'Sales': [50000, 60000, 55000]\n",
    "})\n",
    "\n",
    "print(\"Merging on different column names:\")\n",
    "merged_diff = pd.merge(\n",
    "    employees,\n",
    "    sales,\n",
    "    left_on='EmployeeID',\n",
    "    right_on='EmpID',\n",
    "    how='left'\n",
    ")\n",
    "print(merged_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging on index\n",
    "df_indexed1 = pd.DataFrame(\n",
    "    {'A': [1, 2, 3]},\n",
    "    index=['a', 'b', 'c']\n",
    ")\n",
    "\n",
    "df_indexed2 = pd.DataFrame(\n",
    "    {'B': [4, 5, 6]},\n",
    "    index=['a', 'b', 'd']\n",
    ")\n",
    "\n",
    "print(\"Merging on index:\")\n",
    "merged_index = pd.merge(\n",
    "    df_indexed1,\n",
    "    df_indexed2,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer'\n",
    ")\n",
    "print(merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Method\n",
    "\n",
    "The `.join()` method is a convenient way to merge DataFrames on their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index and use join\n",
    "emp_indexed = employees.set_index('EmployeeID')\n",
    "sales_indexed = sales.set_index('EmpID')\n",
    "\n",
    "print(\"Using join method:\")\n",
    "joined = emp_indexed.join(sales_indexed, how='left')\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Time Series Basics\n",
    "\n",
    "Time series data has timestamps as the index. Pandas provides powerful tools for working with dates, times, and time-indexed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Dates and Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating date ranges\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\n",
    "print(\"Daily date range:\")\n",
    "print(date_range)\n",
    "\n",
    "# Different frequencies\n",
    "print(\"\\nWeekly dates:\")\n",
    "weekly = pd.date_range(start='2024-01-01', periods=5, freq='W')\n",
    "print(weekly)\n",
    "\n",
    "print(\"\\nHourly dates:\")\n",
    "hourly = pd.date_range(start='2024-01-01', periods=5, freq='H')\n",
    "print(hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting strings to datetime\n",
    "date_strings = ['2024-01-01', '2024-02-15', '2024-03-20']\n",
    "dates = pd.to_datetime(date_strings)\n",
    "print(\"Converted to datetime:\")\n",
    "print(dates)\n",
    "print(\"Data type:\", dates.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing different date formats\n",
    "custom_format = ['01-01-2024', '15-02-2024', '20-03-2024']\n",
    "parsed_dates = pd.to_datetime(custom_format, format='%d-%m-%Y')\n",
    "print(\"Parsed custom format:\")\n",
    "print(parsed_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Time Series DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series DataFrame\n",
    "dates = pd.date_range('2024-01-01', periods=30, freq='D')\n",
    "np.random.seed(42)\n",
    "values = np.random.randn(30).cumsum() + 100  # Random walk\n",
    "\n",
    "ts_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Value': values\n",
    "})\n",
    "\n",
    "print(\"Time series DataFrame:\")\n",
    "print(ts_df.head(10))\n",
    "\n",
    "# Set date as index\n",
    "ts_df.set_index('Date', inplace=True)\n",
    "print(\"\\nWith date index:\")\n",
    "print(ts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Date Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "ts_analysis = ts_df.copy()\n",
    "ts_analysis['Year'] = ts_analysis.index.year\n",
    "ts_analysis['Month'] = ts_analysis.index.month\n",
    "ts_analysis['Day'] = ts_analysis.index.day\n",
    "ts_analysis['DayOfWeek'] = ts_analysis.index.dayofweek  # Monday=0, Sunday=6\n",
    "ts_analysis['DayName'] = ts_analysis.index.day_name()\n",
    "\n",
    "print(\"With extracted components:\")\n",
    "print(ts_analysis.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data by date\n",
    "print(\"Data for January 5, 2024:\")\n",
    "print(ts_df.loc['2024-01-05'])\n",
    "\n",
    "# Select date range\n",
    "print(\"\\nData from Jan 10 to Jan 15:\")\n",
    "print(ts_df.loc['2024-01-10':'2024-01-15'])\n",
    "\n",
    "# Select by month\n",
    "print(\"\\nAll January data:\")\n",
    "print(ts_df.loc['2024-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Time Series\n",
    "\n",
    "Resampling allows you to change the frequency of time series data (e.g., daily to weekly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to weekly (downsampling)\n",
    "print(\"Weekly mean values:\")\n",
    "weekly_mean = ts_df.resample('W').mean()\n",
    "print(weekly_mean)\n",
    "\n",
    "print(\"\\nWeekly sum:\")\n",
    "weekly_sum = ts_df.resample('W').sum()\n",
    "print(weekly_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample to hourly (forward fill)\n",
    "print(\"Upsampled to 12-hour frequency (first 10 rows):\")\n",
    "upsampled = ts_df.resample('12H').ffill()  # Forward fill missing values\n",
    "print(upsampled.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Window Calculations\n",
    "\n",
    "Rolling windows compute statistics over a sliding window of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling mean (moving average)\n",
    "ts_rolling = ts_df.copy()\n",
    "ts_rolling['Rolling_Mean_7'] = ts_rolling['Value'].rolling(window=7).mean()\n",
    "ts_rolling['Rolling_Std_7'] = ts_rolling['Value'].rolling(window=7).std()\n",
    "\n",
    "print(\"With rolling statistics (7-day window):\")\n",
    "print(ts_rolling.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling sum and other aggregations\n",
    "print(\"7-day rolling sum:\")\n",
    "print(ts_df['Value'].rolling(window=7).sum().head(10))\n",
    "\n",
    "print(\"\\n7-day rolling max:\")\n",
    "print(ts_df['Value'].rolling(window=7).max().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Data\n",
    "\n",
    "Shifting moves data forward or backward in time, useful for calculating changes and lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift data\n",
    "ts_shift = ts_df.copy()\n",
    "ts_shift['Previous_Day'] = ts_shift['Value'].shift(1)  # Shift forward (lag)\n",
    "ts_shift['Next_Day'] = ts_shift['Value'].shift(-1)  # Shift backward (lead)\n",
    "ts_shift['Daily_Change'] = ts_shift['Value'] - ts_shift['Previous_Day']\n",
    "\n",
    "print(\"With shifted values:\")\n",
    "print(ts_shift.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Practical Examples with Real-World Scenarios\n",
    "\n",
    "Let's apply everything we've learned to realistic data analysis scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Sales Analysis\n",
    "\n",
    "Analyze sales data to identify trends, top products, and regional performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sales dataset\n",
    "np.random.seed(42)\n",
    "n_records = 200\n",
    "\n",
    "sales_complete = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=n_records, freq='D'),\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'], n_records),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], n_records),\n",
    "    'Sales_Amount': np.random.randint(100, 2000, n_records),\n",
    "    'Quantity': np.random.randint(1, 20, n_records),\n",
    "    'Customer_Segment': np.random.choice(['Individual', 'Corporate', 'Government'], n_records)\n",
    "})\n",
    "\n",
    "# Add some missing values to make it realistic\n",
    "missing_indices = np.random.choice(sales_complete.index, size=10, replace=False)\n",
    "sales_complete.loc[missing_indices, 'Sales_Amount'] = np.nan\n",
    "\n",
    "print(\"Sales dataset:\")\n",
    "print(sales_complete.head())\n",
    "print(f\"\\nShape: {sales_complete.shape}\")\n",
    "print(f\"Missing values: {sales_complete.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "sales_clean = sales_complete.copy()\n",
    "\n",
    "# Fill missing sales amounts with median\n",
    "sales_clean['Sales_Amount'].fillna(sales_clean['Sales_Amount'].median(), inplace=True)\n",
    "\n",
    "# Add calculated columns\n",
    "sales_clean['Price_per_Unit'] = sales_clean['Sales_Amount'] / sales_clean['Quantity']\n",
    "sales_clean['Month'] = sales_clean['Date'].dt.month\n",
    "sales_clean['Quarter'] = sales_clean['Date'].dt.quarter\n",
    "\n",
    "print(\"Cleaned dataset:\")\n",
    "print(sales_clean.head())\n",
    "print(f\"\\nMissing values after cleaning: {sales_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 1: Top products by revenue\n",
    "print(\"Top 5 products by total sales:\")\n",
    "top_products = sales_clean.groupby('Product')['Sales_Amount'].sum().sort_values(ascending=False)\n",
    "print(top_products)\n",
    "\n",
    "# Analysis 2: Regional performance\n",
    "print(\"\\nSales by region:\")\n",
    "regional_sales = sales_clean.groupby('Region').agg({\n",
    "    'Sales_Amount': ['sum', 'mean', 'count'],\n",
    "    'Quantity': 'sum'\n",
    "}).round(2)\n",
    "print(regional_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 3: Monthly trend\n",
    "print(\"Monthly sales trend:\")\n",
    "monthly_sales = sales_clean.groupby('Month')['Sales_Amount'].agg(['sum', 'mean']).round(2)\n",
    "monthly_sales.columns = ['Total_Sales', 'Average_Sales']\n",
    "print(monthly_sales)\n",
    "\n",
    "# Analysis 4: Best performing product-region combination\n",
    "print(\"\\nTop 10 Product-Region combinations:\")\n",
    "product_region = sales_clean.groupby(['Product', 'Region'])['Sales_Amount'].sum().sort_values(ascending=False).head(10)\n",
    "print(product_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 5: Customer segment analysis\n",
    "print(\"Sales by customer segment:\")\n",
    "segment_pivot = sales_clean.pivot_table(\n",
    "    values='Sales_Amount',\n",
    "    index='Product',\n",
    "    columns='Customer_Segment',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").round(2)\n",
    "print(segment_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Customer Data Analysis\n",
    "\n",
    "Analyze customer demographics and purchase behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'CustomerID': range(1, 101),\n",
    "    'Age': np.random.randint(18, 70, 100),\n",
    "    'Gender': np.random.choice(['M', 'F', 'Other'], 100),\n",
    "    'City': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], 100),\n",
    "    'Total_Purchases': np.random.randint(1, 50, 100),\n",
    "    'Total_Spend': np.random.randint(100, 10000, 100),\n",
    "    'Join_Date': pd.date_range('2023-01-01', periods=100, freq='3D')\n",
    "})\n",
    "\n",
    "print(\"Customer dataset:\")\n",
    "print(customers.head())\n",
    "print(f\"\\nBasic statistics:\\n{customers.describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 1: Age segmentation\n",
    "customers['Age_Group'] = pd.cut(\n",
    "    customers['Age'],\n",
    "    bins=[0, 25, 40, 60, 100],\n",
    "    labels=['18-25', '26-40', '41-60', '60+']\n",
    ")\n",
    "\n",
    "print(\"Spending by age group:\")\n",
    "age_analysis = customers.groupby('Age_Group').agg({\n",
    "    'Total_Spend': ['mean', 'sum'],\n",
    "    'Total_Purchases': 'mean',\n",
    "    'CustomerID': 'count'\n",
    "}).round(2)\n",
    "age_analysis.columns = ['Avg_Spend', 'Total_Spend', 'Avg_Purchases', 'Customer_Count']\n",
    "print(age_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 2: Calculate metrics\n",
    "customers['Avg_Order_Value'] = customers['Total_Spend'] / customers['Total_Purchases']\n",
    "customers['Days_Since_Join'] = (pd.Timestamp('2024-07-18') - customers['Join_Date']).dt.days\n",
    "\n",
    "print(\"Top 10 customers by average order value:\")\n",
    "top_aov = customers.nlargest(10, 'Avg_Order_Value')[['CustomerID', 'Age', 'City', 'Avg_Order_Value']]\n",
    "print(top_aov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 3: Geographic analysis\n",
    "print(\"City-wise performance:\")\n",
    "city_analysis = customers.groupby('City').agg({\n",
    "    'CustomerID': 'count',\n",
    "    'Total_Spend': 'sum',\n",
    "    'Avg_Order_Value': 'mean'\n",
    "}).round(2)\n",
    "city_analysis.columns = ['Customer_Count', 'Total_Revenue', 'Avg_Order_Value']\n",
    "city_analysis = city_analysis.sort_values('Total_Revenue', ascending=False)\n",
    "print(city_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 4: Customer lifetime value segments\n",
    "customers['CLV_Segment'] = pd.qcut(\n",
    "    customers['Total_Spend'],\n",
    "    q=4,\n",
    "    labels=['Low', 'Medium', 'High', 'Premium']\n",
    ")\n",
    "\n",
    "print(\"Customer distribution by value segment:\")\n",
    "print(customers['CLV_Segment'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSegment characteristics:\")\n",
    "segment_stats = customers.groupby('CLV_Segment').agg({\n",
    "    'Age': 'mean',\n",
    "    'Total_Purchases': 'mean',\n",
    "    'Total_Spend': 'mean',\n",
    "    'Avg_Order_Value': 'mean'\n",
    "}).round(2)\n",
    "print(segment_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Time Series Forecasting Preparation\n",
    "\n",
    "Prepare time series data for forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily sales time series\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', '2024-06-30', freq='D')\n",
    "\n",
    "# Simulate sales with trend and seasonality\n",
    "trend = np.linspace(1000, 2000, len(dates))\n",
    "seasonal = 200 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365)\n",
    "noise = np.random.normal(0, 100, len(dates))\n",
    "sales_values = trend + seasonal + noise\n",
    "\n",
    "ts_sales = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Sales': sales_values\n",
    "}).set_index('Date')\n",
    "\n",
    "print(\"Time series sales data:\")\n",
    "print(ts_sales.head())\n",
    "print(f\"\\nDate range: {ts_sales.index.min()} to {ts_sales.index.max()}\")\n",
    "print(f\"Total days: {len(ts_sales)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for time series\n",
    "ts_features = ts_sales.copy()\n",
    "\n",
    "# Date features\n",
    "ts_features['Year'] = ts_features.index.year\n",
    "ts_features['Month'] = ts_features.index.month\n",
    "ts_features['Day'] = ts_features.index.day\n",
    "ts_features['DayOfWeek'] = ts_features.index.dayofweek\n",
    "ts_features['Quarter'] = ts_features.index.quarter\n",
    "ts_features['WeekOfYear'] = ts_features.index.isocalendar().week\n",
    "\n",
    "# Lag features\n",
    "ts_features['Sales_Lag1'] = ts_features['Sales'].shift(1)\n",
    "ts_features['Sales_Lag7'] = ts_features['Sales'].shift(7)\n",
    "ts_features['Sales_Lag30'] = ts_features['Sales'].shift(30)\n",
    "\n",
    "# Rolling statistics\n",
    "ts_features['Sales_Rolling7_Mean'] = ts_features['Sales'].rolling(window=7).mean()\n",
    "ts_features['Sales_Rolling30_Mean'] = ts_features['Sales'].rolling(window=30).mean()\n",
    "ts_features['Sales_Rolling7_Std'] = ts_features['Sales'].rolling(window=7).std()\n",
    "\n",
    "print(\"With engineered features:\")\n",
    "print(ts_features.head(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly aggregation\n",
    "monthly_agg = ts_sales.resample('M').agg({\n",
    "    'Sales': ['sum', 'mean', 'min', 'max', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Monthly aggregated data:\")\n",
    "print(monthly_agg.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate growth rates\n",
    "ts_growth = ts_sales.copy()\n",
    "ts_growth['Daily_Change'] = ts_growth['Sales'].diff()\n",
    "ts_growth['Daily_Pct_Change'] = ts_growth['Sales'].pct_change() * 100\n",
    "ts_growth['Weekly_Pct_Change'] = ts_growth['Sales'].pct_change(periods=7) * 100\n",
    "\n",
    "print(\"Growth metrics:\")\n",
    "print(ts_growth.head(10))\n",
    "\n",
    "print(\"\\nSummary statistics for growth:\")\n",
    "print(ts_growth[['Daily_Pct_Change', 'Weekly_Pct_Change']].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "In this comprehensive guide, you've learned the essential skills for data manipulation with Pandas and NumPy:\n",
    "\n",
    "1. **NumPy Fundamentals**\n",
    "   - Creating and manipulating arrays efficiently\n",
    "   - Performing vectorized operations for speed\n",
    "   - Using universal functions and statistical operations\n",
    "   - Boolean indexing and array reshaping\n",
    "\n",
    "2. **Pandas Data Structures**\n",
    "   - Working with Series (1D labeled arrays)\n",
    "   - Creating and manipulating DataFrames (2D labeled tables)\n",
    "   - Understanding the power of labeled data\n",
    "\n",
    "3. **Data Loading and Exploration**\n",
    "   - Reading data from various formats (CSV, Excel, JSON)\n",
    "   - Exploring data with `.head()`, `.info()`, `.describe()`\n",
    "   - Identifying data characteristics and quality issues\n",
    "\n",
    "4. **Data Selection Techniques**\n",
    "   - Label-based selection with `.loc[]`\n",
    "   - Position-based selection with `.iloc[]`\n",
    "   - Boolean indexing for filtering data\n",
    "   - Using the `.query()` method for readable filters\n",
    "\n",
    "5. **Data Cleaning**\n",
    "   - Detecting and handling missing values (dropna, fillna)\n",
    "   - Identifying and removing duplicates\n",
    "   - Converting data types appropriately\n",
    "   - Cleaning and standardizing text data\n",
    "\n",
    "6. **Data Transformation**\n",
    "   - Sorting data by single or multiple columns\n",
    "   - Grouping data with `.groupby()` and aggregating\n",
    "   - Creating pivot tables for summarization\n",
    "   - Adding calculated columns and categorizing data\n",
    "\n",
    "7. **Combining Datasets**\n",
    "   - Concatenating DataFrames vertically and horizontally\n",
    "   - Merging data with different join types (inner, left, right, outer)\n",
    "   - Understanding when to use merge vs. join\n",
    "\n",
    "8. **Time Series Analysis**\n",
    "   - Working with datetime objects and date ranges\n",
    "   - Extracting date components (year, month, day)\n",
    "   - Resampling time series to different frequencies\n",
    "   - Calculating rolling statistics and shifts\n",
    "\n",
    "9. **Real-World Applications**\n",
    "   - Sales analysis: identifying trends and top performers\n",
    "   - Customer analytics: segmentation and behavior analysis\n",
    "   - Time series preparation: feature engineering for forecasting\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue your data manipulation journey:\n",
    "\n",
    "- **Practice**: Work with real datasets from sources like Kaggle, UCI ML Repository, or government open data portals\n",
    "- **Visualization**: Learn data visualization libraries (Matplotlib, Seaborn, Plotly) to visualize your findings\n",
    "- **Advanced Pandas**: Explore multi-indexing, categorical data, and performance optimization\n",
    "- **Statistical Analysis**: Study statistical methods and hypothesis testing\n",
    "- **Machine Learning**: Apply these skills to prepare data for machine learning models\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Official documentation: [Pandas](https://pandas.pydata.org/docs/) and [NumPy](https://numpy.org/doc/)\n",
    "- Practice datasets: [Kaggle](https://www.kaggle.com/datasets), [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)\n",
    "- Community: Stack Overflow, Reddit (r/datascience, r/learnpython)\n",
    "\n",
    "Remember: Data manipulation is a skill that improves with practice. Start with simple datasets and gradually work towards more complex analyses!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
