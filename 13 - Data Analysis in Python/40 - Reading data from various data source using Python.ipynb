{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading Data from Various Data Sources using Python\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction to Data Sources](#introduction)\n",
        "2. [Reading CSV Files](#csv)\n",
        "3. [Reading Excel Files](#excel)\n",
        "4. [Reading JSON Files](#json)\n",
        "5. [Reading from Databases](#databases)\n",
        "6. [Reading from Web](#web)\n",
        "7. [Reading Other Formats](#other-formats)\n",
        "8. [Writing Data](#writing)\n",
        "9. [Data Import Best Practices](#best-practices)\n",
        "10. [Practical Examples](#examples)\n",
        "11. [Summary](#summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Introduction to Data Sources <a id='introduction'></a>\n",
        "\n",
        "In data analysis and machine learning, **reading data from various sources** is the first critical step. Data can come from multiple formats and locations, and Python provides powerful tools to handle them all.\n",
        "\n",
        "### Common Data Sources:\n",
        "\n",
        "| Source Type | Format | Common Use Cases |\n",
        "|-------------|--------|------------------|\n",
        "| **Files** | CSV, Excel, JSON, Parquet | Local data storage, data exchange |\n",
        "| **Databases** | SQL, NoSQL | Structured data storage, enterprise systems |\n",
        "| **Web** | HTML, APIs, Scraping | Real-time data, web services |\n",
        "| **Other** | Pickle, HDF5, Text | Specialized storage, large datasets |\n",
        "\n",
        "### Primary Library: Pandas\n",
        "\n",
        "**Pandas** is the go-to library for data manipulation in Python. It provides:\n",
        "- **DataFrame**: 2D labeled data structure (like Excel spreadsheet)\n",
        "- **Series**: 1D labeled array\n",
        "- Built-in functions for reading/writing various formats\n",
        "- Powerful data manipulation capabilities\n",
        "\n",
        "### Why Different Formats?\n",
        "\n",
        "- **CSV**: Simple, universal, human-readable\n",
        "- **Excel**: Business standard, supports multiple sheets\n",
        "- **JSON**: Web APIs, nested data structures\n",
        "- **Parquet**: Big data, efficient compression\n",
        "- **SQL**: Relational data, complex queries\n",
        "\n",
        "Let's explore each format in detail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Display pandas version\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Reading CSV Files <a id='csv'></a>\n",
        "\n",
        "**CSV (Comma-Separated Values)** is the most common data format for storing tabular data. It's simple, universal, and human-readable.\n",
        "\n",
        "### Basic Syntax:\n",
        "```python\n",
        "df = pd.read_csv('filename.csv')\n",
        "```\n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|----------|\n",
        "| `filepath` | Path to file | `'data.csv'` or `'C:/data/file.csv'` |\n",
        "| `sep` | Delimiter | `sep=','` (default), `sep='\\t'` (tab) |\n",
        "| `header` | Row number for column names | `header=0` (default), `header=None` |\n",
        "| `names` | Custom column names | `names=['A', 'B', 'C']` |\n",
        "| `usecols` | Columns to read | `usecols=[0, 1, 2]` or `usecols=['Name', 'Age']` |\n",
        "| `skiprows` | Rows to skip | `skiprows=5` or `skiprows=[0, 2]` |\n",
        "| `nrows` | Number of rows to read | `nrows=100` |\n",
        "| `index_col` | Column to use as index | `index_col=0` or `index_col='ID'` |\n",
        "| `dtype` | Data type for columns | `dtype={'Age': int}` |\n",
        "| `parse_dates` | Parse date columns | `parse_dates=['Date']` |\n",
        "| `na_values` | Additional NA values | `na_values=['NA', 'missing']` |\n",
        "| `encoding` | File encoding | `encoding='utf-8'` or `encoding='latin1'` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Basic CSV reading\n",
        "# First, let's create a sample CSV file\n",
        "\n",
        "import io\n",
        "\n",
        "# Sample CSV data as string\n",
        "csv_data = \"\"\"Name,Age,City,Salary\n",
        "John,28,New York,75000\n",
        "Alice,34,Los Angeles,85000\n",
        "Bob,45,Chicago,65000\n",
        "Emma,29,Houston,70000\n",
        "Michael,52,Phoenix,90000\"\"\"\n",
        "\n",
        "# Read from string (simulating file)\n",
        "df = pd.read_csv(io.StringIO(csv_data))\n",
        "\n",
        "print(\"Basic CSV Reading:\")\n",
        "print(df)\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Reading with custom delimiter (tab-separated)\n",
        "\n",
        "tsv_data = \"\"\"Name\\tAge\\tCity\\tSalary\n",
        "John\\t28\\tNew York\\t75000\n",
        "Alice\\t34\\tLos Angeles\\t85000\n",
        "Bob\\t45\\tChicago\\t65000\"\"\"\n",
        "\n",
        "df_tsv = pd.read_csv(io.StringIO(tsv_data), sep='\\t')\n",
        "print(\"Tab-Separated Data:\")\n",
        "print(df_tsv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Reading specific columns only\n",
        "\n",
        "# Method 1: Using column indices\n",
        "df_cols_idx = pd.read_csv(io.StringIO(csv_data), usecols=[0, 1])\n",
        "print(\"Reading columns by index [0, 1]:\")\n",
        "print(df_cols_idx)\n",
        "print()\n",
        "\n",
        "# Method 2: Using column names\n",
        "df_cols_names = pd.read_csv(io.StringIO(csv_data), usecols=['Name', 'Salary'])\n",
        "print(\"Reading columns by name ['Name', 'Salary']:\")\n",
        "print(df_cols_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Handling missing headers and custom column names\n",
        "\n",
        "# CSV without headers\n",
        "csv_no_header = \"\"\"John,28,New York,75000\n",
        "Alice,34,Los Angeles,85000\n",
        "Bob,45,Chicago,65000\"\"\"\n",
        "\n",
        "# Read without header and assign custom names\n",
        "df_custom = pd.read_csv(\n",
        "    io.StringIO(csv_no_header), \n",
        "    header=None,  # No header in file\n",
        "    names=['Employee_Name', 'Employee_Age', 'Location', 'Annual_Salary']\n",
        ")\n",
        "\n",
        "print(\"CSV with custom column names:\")\n",
        "print(df_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Skipping rows and reading limited rows\n",
        "\n",
        "csv_with_comments = \"\"\"# This is a comment\n",
        "# Data collected on 2024-01-01\n",
        "Name,Age,City,Salary\n",
        "John,28,New York,75000\n",
        "Alice,34,Los Angeles,85000\n",
        "Bob,45,Chicago,65000\n",
        "Emma,29,Houston,70000\"\"\"\n",
        "\n",
        "# Skip first 2 rows (comments) and read only 2 data rows\n",
        "df_skip = pd.read_csv(\n",
        "    io.StringIO(csv_with_comments), \n",
        "    skiprows=2,  # Skip first 2 rows\n",
        "    nrows=2      # Read only 2 data rows\n",
        ")\n",
        "\n",
        "print(\"Skipping rows and limiting read:\")\n",
        "print(df_skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Setting index column\n",
        "\n",
        "csv_with_id = \"\"\"ID,Name,Age,City\n",
        "101,John,28,New York\n",
        "102,Alice,34,Los Angeles\n",
        "103,Bob,45,Chicago\n",
        "104,Emma,29,Houston\"\"\"\n",
        "\n",
        "# Set 'ID' as index\n",
        "df_indexed = pd.read_csv(io.StringIO(csv_with_id), index_col='ID')\n",
        "\n",
        "print(\"DataFrame with custom index:\")\n",
        "print(df_indexed)\n",
        "print(f\"\\nIndex: {df_indexed.index.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 7: Specifying data types\n",
        "\n",
        "csv_mixed = \"\"\"Name,Age,Score,IsActive\n",
        "John,28,85.5,True\n",
        "Alice,34,92.3,False\n",
        "Bob,45,78.9,True\"\"\"\n",
        "\n",
        "# Specify dtypes for better performance and accuracy\n",
        "df_typed = pd.read_csv(\n",
        "    io.StringIO(csv_mixed),\n",
        "    dtype={\n",
        "        'Name': str,\n",
        "        'Age': int,\n",
        "        'Score': float,\n",
        "        'IsActive': bool\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"DataFrame with specified dtypes:\")\n",
        "print(df_typed)\n",
        "print(f\"\\nData types:\\n{df_typed.dtypes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 8: Parsing dates\n",
        "\n",
        "csv_dates = \"\"\"Date,Product,Sales\n",
        "2024-01-01,Widget A,150\n",
        "2024-01-02,Widget B,200\n",
        "2024-01-03,Widget A,175\n",
        "2024-01-04,Widget C,300\"\"\"\n",
        "\n",
        "# Parse Date column as datetime\n",
        "df_dates = pd.read_csv(\n",
        "    io.StringIO(csv_dates),\n",
        "    parse_dates=['Date']  # Automatically convert to datetime\n",
        ")\n",
        "\n",
        "print(\"DataFrame with parsed dates:\")\n",
        "print(df_dates)\n",
        "print(f\"\\nDate column dtype: {df_dates['Date'].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 9: Handling missing values\n",
        "\n",
        "csv_missing = \"\"\"Name,Age,City,Salary\n",
        "John,28,New York,75000\n",
        "Alice,NA,Los Angeles,85000\n",
        "Bob,45,missing,65000\n",
        "Emma,29,Houston,N/A\"\"\"\n",
        "\n",
        "# Specify additional NA values\n",
        "df_na = pd.read_csv(\n",
        "    io.StringIO(csv_missing),\n",
        "    na_values=['NA', 'missing', 'N/A', 'null']  # Treat these as NaN\n",
        ")\n",
        "\n",
        "print(\"DataFrame with handled missing values:\")\n",
        "print(df_na)\n",
        "print(f\"\\nMissing values per column:\\n{df_na.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 10: Reading large files in chunks\n",
        "\n",
        "# Create sample large CSV\n",
        "large_csv_data = \"Name,Age,Score\\n\" + \"\\n\".join(\n",
        "    [f\"Person{i},{20+i%30},{50+i%50}\" for i in range(20)]\n",
        ")\n",
        "\n",
        "# Read in chunks (useful for very large files)\n",
        "chunk_size = 5\n",
        "chunks = []\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(io.StringIO(large_csv_data), chunksize=chunk_size)):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(chunk)\n",
        "    print()\n",
        "    chunks.append(chunk)\n",
        "    if i == 2:  # Show only first 3 chunks\n",
        "        print(\"... (more chunks)\")\n",
        "        break\n",
        "\n",
        "# Combine chunks if needed\n",
        "# df_full = pd.concat(chunks, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Reading Excel Files <a id='excel'></a>\n",
        "\n",
        "**Excel files** (.xlsx, .xls) are widely used in business environments. Pandas can read and write Excel files using the `openpyxl` or `xlrd` engines.\n",
        "\n",
        "### Installation:\n",
        "```bash\n",
        "pip install openpyxl  # For .xlsx files\n",
        "pip install xlrd      # For .xls files (older format)\n",
        "```\n",
        "\n",
        "### Basic Syntax:\n",
        "```python\n",
        "df = pd.read_excel('filename.xlsx')\n",
        "```\n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "| Parameter | Description | Example |\n",
        "|-----------|-------------|----------|\n",
        "| `sheet_name` | Sheet to read | `sheet_name=0` or `sheet_name='Sheet1'` |\n",
        "| `header` | Row for column names | `header=0` (default) |\n",
        "| `usecols` | Columns to read | `usecols='A:C'` or `usecols=[0, 1, 2]` |\n",
        "| `skiprows` | Rows to skip | `skiprows=2` |\n",
        "| `nrows` | Number of rows | `nrows=100` |\n",
        "| `engine` | Excel engine | `engine='openpyxl'` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Creating and reading Excel file\n",
        "\n",
        "# Create sample data\n",
        "data = {\n",
        "    'Name': ['John', 'Alice', 'Bob', 'Emma', 'Michael'],\n",
        "    'Age': [28, 34, 45, 29, 52],\n",
        "    'Department': ['IT', 'HR', 'Finance', 'IT', 'Marketing'],\n",
        "    'Salary': [75000, 85000, 65000, 70000, 90000]\n",
        "}\n",
        "\n",
        "df_sample = pd.DataFrame(data)\n",
        "\n",
        "# Write to Excel (we'll use this for demonstration)\n",
        "# df_sample.to_excel('employees.xlsx', index=False)\n",
        "\n",
        "print(\"Sample data created:\")\n",
        "print(df_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Reading specific sheet by name or index\n",
        "\n",
        "# If you have a real Excel file:\n",
        "# df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # By name\n",
        "# df = pd.read_excel('data.xlsx', sheet_name=0)         # By index (first sheet)\n",
        "\n",
        "# Demonstration with in-memory data\n",
        "print(\"Reading Excel file:\")\n",
        "print(\"- sheet_name='Sheet1' reads the sheet named 'Sheet1'\")\n",
        "print(\"- sheet_name=0 reads the first sheet\")\n",
        "print(\"- sheet_name=None reads all sheets (returns dict of DataFrames)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Reading multiple sheets\n",
        "\n",
        "# Create multiple sheets for demonstration\n",
        "data_sheet1 = {'Product': ['A', 'B', 'C'], 'Price': [100, 200, 150]}\n",
        "data_sheet2 = {'Product': ['D', 'E', 'F'], 'Price': [300, 250, 180]}\n",
        "\n",
        "# Write to Excel with multiple sheets\n",
        "# with pd.ExcelWriter('multi_sheet.xlsx') as writer:\n",
        "#     pd.DataFrame(data_sheet1).to_excel(writer, sheet_name='Q1', index=False)\n",
        "#     pd.DataFrame(data_sheet2).to_excel(writer, sheet_name='Q2', index=False)\n",
        "\n",
        "# Read all sheets\n",
        "# excel_file = pd.read_excel('multi_sheet.xlsx', sheet_name=None)\n",
        "# print(\"Available sheets:\", excel_file.keys())\n",
        "# print(\"Q1 data:\\n\", excel_file['Q1'])\n",
        "# print(\"Q2 data:\\n\", excel_file['Q2'])\n",
        "\n",
        "print(\"To read all sheets: sheet_name=None\")\n",
        "print(\"Returns a dictionary: {'Sheet1': df1, 'Sheet2': df2, ...}\")\n",
        "print(\"\\nExample:\")\n",
        "print(pd.DataFrame(data_sheet1))\n",
        "print(\"\\n(This would be one of the sheets)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Reading specific columns from Excel\n",
        "\n",
        "# Method 1: Using Excel column letters\n",
        "# df = pd.read_excel('data.xlsx', usecols='A:C')  # Columns A, B, C\n",
        "\n",
        "# Method 2: Using column indices\n",
        "# df = pd.read_excel('data.xlsx', usecols=[0, 1, 2])  # First 3 columns\n",
        "\n",
        "# Method 3: Using column names\n",
        "# df = pd.read_excel('data.xlsx', usecols=['Name', 'Age', 'Salary'])\n",
        "\n",
        "print(\"Reading specific columns from Excel:\")\n",
        "print(\"\\nMethod 1 - Column letters: usecols='A:D'\")\n",
        "print(\"Method 2 - Column indices: usecols=[0, 1, 2, 3]\")\n",
        "print(\"Method 3 - Column names: usecols=['Name', 'Age', 'Salary']\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Writing to Excel with formatting\n",
        "\n",
        "# Create sample data\n",
        "sales_data = {\n",
        "    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n",
        "    'Q1_Sales': [15000, 23000, 18000, 31000],\n",
        "    'Q2_Sales': [17000, 25000, 19000, 33000],\n",
        "    'Q3_Sales': [16000, 24000, 21000, 35000]\n",
        "}\n",
        "\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "df_sales['Total'] = df_sales[['Q1_Sales', 'Q2_Sales', 'Q3_Sales']].sum(axis=1)\n",
        "\n",
        "print(\"Sales data to export:\")\n",
        "print(df_sales)\n",
        "\n",
        "# Writing to Excel (demonstration)\n",
        "# df_sales.to_excel('sales_report.xlsx', sheet_name='Sales', index=False)\n",
        "\n",
        "print(\"\\nExported to Excel with to_excel() method\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Writing multiple DataFrames to different sheets\n",
        "\n",
        "# Create multiple DataFrames\n",
        "df_employees = pd.DataFrame({\n",
        "    'Name': ['John', 'Alice', 'Bob'],\n",
        "    'Department': ['IT', 'HR', 'Finance']\n",
        "})\n",
        "\n",
        "df_departments = pd.DataFrame({\n",
        "    'Department': ['IT', 'HR', 'Finance'],\n",
        "    'Budget': [500000, 300000, 400000]\n",
        "})\n",
        "\n",
        "# Write to multiple sheets\n",
        "# with pd.ExcelWriter('company_data.xlsx', engine='openpyxl') as writer:\n",
        "#     df_employees.to_excel(writer, sheet_name='Employees', index=False)\n",
        "#     df_departments.to_excel(writer, sheet_name='Departments', index=False)\n",
        "\n",
        "print(\"Employees sheet:\")\n",
        "print(df_employees)\n",
        "print(\"\\nDepartments sheet:\")\n",
        "print(df_departments)\n",
        "print(\"\\nBoth written to different sheets in same Excel file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Reading JSON Files <a id='json'></a>\n",
        "\n",
        "**JSON (JavaScript Object Notation)** is a lightweight data format commonly used for web APIs and configuration files. It supports nested structures.\n",
        "\n",
        "### Basic Syntax:\n",
        "```python\n",
        "df = pd.read_json('data.json')\n",
        "```\n",
        "\n",
        "### JSON Orientations:\n",
        "\n",
        "| Orientation | Description | Structure |\n",
        "|-------------|-------------|----------|\n",
        "| `'split'` | Dict with index, columns, data | `{\"index\": [...], \"columns\": [...], \"data\": [...]}` |\n",
        "| `'records'` | List of records (most common) | `[{\"col1\": val1, \"col2\": val2}, ...]` |\n",
        "| `'index'` | Dict of dicts indexed by row | `{\"row1\": {\"col1\": val1}, ...}` |\n",
        "| `'columns'` | Dict of dicts indexed by column | `{\"col1\": {\"row1\": val1}, ...}` |\n",
        "| `'values'` | Just the values array | `[[val1, val2], [val3, val4], ...]` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Reading JSON from string (records orientation)\n",
        "\n",
        "import json\n",
        "\n",
        "# JSON in 'records' format (most common)\n",
        "json_records = '''\n",
        "[\n",
        "    {\"Name\": \"John\", \"Age\": 28, \"City\": \"New York\"},\n",
        "    {\"Name\": \"Alice\", \"Age\": 34, \"City\": \"Los Angeles\"},\n",
        "    {\"Name\": \"Bob\", \"Age\": 45, \"City\": \"Chicago\"}\n",
        "]\n",
        "'''\n",
        "\n",
        "df_json = pd.read_json(json_records, orient='records')\n",
        "\n",
        "print(\"JSON (records orientation):\")\n",
        "print(df_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Different JSON orientations\n",
        "\n",
        "# Create sample DataFrame\n",
        "df_sample = pd.DataFrame({\n",
        "    'Name': ['John', 'Alice'],\n",
        "    'Age': [28, 34],\n",
        "    'City': ['New York', 'LA']\n",
        "})\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "print(df_sample)\n",
        "print()\n",
        "\n",
        "# Convert to different JSON formats\n",
        "print(\"\\n1. RECORDS orientation (list of dicts):\")\n",
        "print(df_sample.to_json(orient='records', indent=2))\n",
        "\n",
        "print(\"\\n2. COLUMNS orientation (dict of columns):\")\n",
        "print(df_sample.to_json(orient='columns', indent=2))\n",
        "\n",
        "print(\"\\n3. INDEX orientation (dict of rows):\")\n",
        "print(df_sample.to_json(orient='index', indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Handling nested JSON\n",
        "\n",
        "# Nested JSON structure\n",
        "nested_json = '''\n",
        "[\n",
        "    {\n",
        "        \"name\": \"John\",\n",
        "        \"age\": 28,\n",
        "        \"address\": {\n",
        "            \"city\": \"New York\",\n",
        "            \"zipcode\": \"10001\"\n",
        "        },\n",
        "        \"skills\": [\"Python\", \"SQL\"]\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Alice\",\n",
        "        \"age\": 34,\n",
        "        \"address\": {\n",
        "            \"city\": \"Los Angeles\",\n",
        "            \"zipcode\": \"90001\"\n",
        "        },\n",
        "        \"skills\": [\"Java\", \"JavaScript\"]\n",
        "    }\n",
        "]\n",
        "'''\n",
        "\n",
        "# Read JSON\n",
        "df_nested = pd.read_json(nested_json)\n",
        "\n",
        "print(\"Nested JSON DataFrame:\")\n",
        "print(df_nested)\n",
        "print(f\"\\nData types:\\n{df_nested.dtypes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Normalizing nested JSON (flattening)\n",
        "\n",
        "from pandas import json_normalize\n",
        "\n",
        "# Parse JSON string\n",
        "data = json.loads(nested_json)\n",
        "\n",
        "# Normalize (flatten) nested structure\n",
        "df_normalized = json_normalize(data)\n",
        "\n",
        "print(\"Normalized (flattened) JSON:\")\n",
        "print(df_normalized)\n",
        "print(f\"\\nColumns: {df_normalized.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Reading JSON from API-like structure\n",
        "\n",
        "# JSON with metadata (common in APIs)\n",
        "api_json = '''\n",
        "{\n",
        "    \"status\": \"success\",\n",
        "    \"count\": 3,\n",
        "    \"data\": [\n",
        "        {\"id\": 1, \"product\": \"Widget A\", \"price\": 100},\n",
        "        {\"id\": 2, \"product\": \"Widget B\", \"price\": 200},\n",
        "        {\"id\": 3, \"product\": \"Widget C\", \"price\": 150}\n",
        "    ]\n",
        "}\n",
        "'''\n",
        "\n",
        "# Parse JSON and extract data\n",
        "json_data = json.loads(api_json)\n",
        "df_api = pd.DataFrame(json_data['data'])\n",
        "\n",
        "print(f\"API Status: {json_data['status']}\")\n",
        "print(f\"Record Count: {json_data['count']}\")\n",
        "print(\"\\nData:\")\n",
        "print(df_api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Writing DataFrame to JSON\n",
        "\n",
        "data = {\n",
        "    'Product': ['A', 'B', 'C'],\n",
        "    'Price': [100, 200, 150],\n",
        "    'Stock': [50, 30, 45]\n",
        "}\n",
        "\n",
        "df_products = pd.DataFrame(data)\n",
        "\n",
        "# Write to JSON file (demonstration)\n",
        "# df_products.to_json('products.json', orient='records', indent=2)\n",
        "\n",
        "print(\"DataFrame:\")\n",
        "print(df_products)\n",
        "print(\"\\nJSON output (records):\")\n",
        "print(df_products.to_json(orient='records', indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Reading from Databases <a id='databases'></a>\n",
        "\n",
        "**Databases** are the primary storage for structured data in production systems. Pandas can read data directly from SQL databases.\n",
        "\n",
        "### Database Types:\n",
        "- **SQLite**: Lightweight, serverless (built-in with Python)\n",
        "- **MySQL/PostgreSQL**: Full-featured relational databases\n",
        "- **SQL Server, Oracle**: Enterprise databases\n",
        "\n",
        "### Required Libraries:\n",
        "```bash\n",
        "pip install sqlalchemy  # Database toolkit\n",
        "pip install sqlite3     # Built-in for SQLite\n",
        "```\n",
        "\n",
        "### Basic Syntax:\n",
        "```python\n",
        "df = pd.read_sql('SELECT * FROM table_name', connection)\n",
        "df = pd.read_sql_query('SELECT ...', connection)\n",
        "df = pd.read_sql_table('table_name', connection)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Creating SQLite database and table\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "# Create in-memory SQLite database\n",
        "conn = sqlite3.connect(':memory:')  # In-memory database for demo\n",
        "\n",
        "# Create sample table\n",
        "cursor = conn.cursor()\n",
        "cursor.execute('''\n",
        "    CREATE TABLE employees (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        name TEXT,\n",
        "        age INTEGER,\n",
        "        department TEXT,\n",
        "        salary REAL\n",
        "    )\n",
        "''')\n",
        "\n",
        "# Insert sample data\n",
        "employees_data = [\n",
        "    (1, 'John', 28, 'IT', 75000),\n",
        "    (2, 'Alice', 34, 'HR', 85000),\n",
        "    (3, 'Bob', 45, 'Finance', 65000),\n",
        "    (4, 'Emma', 29, 'IT', 70000),\n",
        "    (5, 'Michael', 52, 'Marketing', 90000)\n",
        "]\n",
        "\n",
        "cursor.executemany('INSERT INTO employees VALUES (?, ?, ?, ?, ?)', employees_data)\n",
        "conn.commit()\n",
        "\n",
        "print(\"SQLite database created with 'employees' table\")\n",
        "print(f\"Inserted {len(employees_data)} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Reading entire table\n",
        "\n",
        "# Read entire table\n",
        "df_employees = pd.read_sql('SELECT * FROM employees', conn)\n",
        "\n",
        "print(\"All employees:\")\n",
        "print(df_employees)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Reading with SQL queries\n",
        "\n",
        "# Query 1: Filter by department\n",
        "query1 = \"SELECT * FROM employees WHERE department = 'IT'\"\n",
        "df_it = pd.read_sql_query(query1, conn)\n",
        "\n",
        "print(\"IT Department employees:\")\n",
        "print(df_it)\n",
        "print()\n",
        "\n",
        "# Query 2: Aggregate query\n",
        "query2 = '''\n",
        "    SELECT department, \n",
        "           COUNT(*) as employee_count, \n",
        "           AVG(salary) as avg_salary\n",
        "    FROM employees\n",
        "    GROUP BY department\n",
        "'''\n",
        "df_dept_stats = pd.read_sql_query(query2, conn)\n",
        "\n",
        "print(\"Department statistics:\")\n",
        "print(df_dept_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Reading with parameterized queries (safe from SQL injection)\n",
        "\n",
        "# Parameterized query (safer)\n",
        "min_salary = 70000\n",
        "query = \"SELECT * FROM employees WHERE salary >= ?\"\n",
        "\n",
        "df_high_salary = pd.read_sql_query(query, conn, params=(min_salary,))\n",
        "\n",
        "print(f\"Employees with salary >= {min_salary}:\")\n",
        "print(df_high_salary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Writing DataFrame to SQL database\n",
        "\n",
        "# Create new DataFrame\n",
        "new_employees = pd.DataFrame({\n",
        "    'id': [6, 7, 8],\n",
        "    'name': ['Sarah', 'Tom', 'Lisa'],\n",
        "    'age': [31, 38, 27],\n",
        "    'department': ['IT', 'HR', 'Finance'],\n",
        "    'salary': [78000, 82000, 68000]\n",
        "})\n",
        "\n",
        "print(\"New employees to add:\")\n",
        "print(new_employees)\n",
        "\n",
        "# Write to database\n",
        "new_employees.to_sql(\n",
        "    'employees',           # Table name\n",
        "    conn,                  # Connection\n",
        "    if_exists='append',    # Append to existing table\n",
        "    index=False            # Don't write index\n",
        ")\n",
        "\n",
        "# Verify\n",
        "df_all = pd.read_sql('SELECT * FROM employees', conn)\n",
        "print(f\"\\nTotal employees after insert: {len(df_all)}\")\n",
        "print(df_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Using SQLAlchemy (recommended for production)\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create engine (for SQLite)\n",
        "engine = create_engine('sqlite:///:memory:')\n",
        "\n",
        "# Write DataFrame to database\n",
        "df_employees.to_sql('employees_new', engine, index=False, if_exists='replace')\n",
        "\n",
        "# Read back\n",
        "df_read = pd.read_sql('SELECT * FROM employees_new', engine)\n",
        "\n",
        "print(\"Data read using SQLAlchemy:\")\n",
        "print(df_read)\n",
        "\n",
        "# For other databases:\n",
        "# MySQL: engine = create_engine('mysql+pymysql://user:password@localhost/dbname')\n",
        "# PostgreSQL: engine = create_engine('postgresql://user:password@localhost/dbname')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up: Close connection\n",
        "conn.close()\n",
        "print(\"Database connection closed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Reading from Web <a id='web'></a>\n",
        "\n",
        "**Web data** can come from HTML tables, APIs, or web scraping. Pandas provides built-in support for reading HTML tables.\n",
        "\n",
        "### Methods:\n",
        "1. **pd.read_html()**: Extract tables from HTML\n",
        "2. **requests + JSON**: Fetch data from APIs\n",
        "3. **BeautifulSoup**: Web scraping (advanced)\n",
        "\n",
        "### Installation:\n",
        "```bash\n",
        "pip install lxml html5lib beautifulsoup4 requests\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Reading HTML tables with pd.read_html()\n",
        "\n",
        "# Sample HTML with table\n",
        "html_data = '''\n",
        "<html>\n",
        "<body>\n",
        "    <h1>Employee Data</h1>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Name</th>\n",
        "            <th>Age</th>\n",
        "            <th>Department</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>John</td>\n",
        "            <td>28</td>\n",
        "            <td>IT</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Alice</td>\n",
        "            <td>34</td>\n",
        "            <td>HR</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Bob</td>\n",
        "            <td>45</td>\n",
        "            <td>Finance</td>\n",
        "        </tr>\n",
        "    </table>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "# Read HTML tables (returns list of DataFrames)\n",
        "tables = pd.read_html(io.StringIO(html_data))\n",
        "\n",
        "print(f\"Number of tables found: {len(tables)}\")\n",
        "print(\"\\nFirst table:\")\n",
        "print(tables[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Reading from URL (demonstration)\n",
        "\n",
        "# Read tables from Wikipedia (example)\n",
        "# url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
        "# tables = pd.read_html(url)\n",
        "# df_gdp = tables[0]  # First table\n",
        "\n",
        "print(\"Reading from URL:\")\n",
        "print(\"tables = pd.read_html('https://example.com/page.html')\")\n",
        "print(\"df = tables[0]  # Get first table\")\n",
        "print(\"\\nNote: Requires internet connection and valid URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Fetching JSON data from API\n",
        "\n",
        "# Simulating API response\n",
        "api_response = {\n",
        "    'status': 'success',\n",
        "    'data': [\n",
        "        {'id': 1, 'name': 'John', 'email': 'john@example.com'},\n",
        "        {'id': 2, 'name': 'Alice', 'email': 'alice@example.com'},\n",
        "        {'id': 3, 'name': 'Bob', 'email': 'bob@example.com'}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert API response to DataFrame\n",
        "df_api = pd.DataFrame(api_response['data'])\n",
        "\n",
        "print(\"API data converted to DataFrame:\")\n",
        "print(df_api)\n",
        "\n",
        "# Real API example (requires requests library):\n",
        "# import requests\n",
        "# response = requests.get('https://api.example.com/data')\n",
        "# data = response.json()\n",
        "# df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Reading CSV from URL\n",
        "\n",
        "# Pandas can read CSV directly from URL\n",
        "# url = 'https://raw.githubusercontent.com/example/data.csv'\n",
        "# df = pd.read_csv(url)\n",
        "\n",
        "print(\"Reading CSV from URL:\")\n",
        "print(\"df = pd.read_csv('https://example.com/data.csv')\")\n",
        "print(\"\\nWorks with any publicly accessible CSV file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Web scraping with BeautifulSoup (basic)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sample HTML\n",
        "html = '''\n",
        "<div class=\"products\">\n",
        "    <div class=\"product\">\n",
        "        <span class=\"name\">Widget A</span>\n",
        "        <span class=\"price\">$100</span>\n",
        "    </div>\n",
        "    <div class=\"product\">\n",
        "        <span class=\"name\">Widget B</span>\n",
        "        <span class=\"price\">$200</span>\n",
        "    </div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "# Parse HTML\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Extract data\n",
        "products = []\n",
        "for product in soup.find_all('div', class_='product'):\n",
        "    name = product.find('span', class_='name').text\n",
        "    price = product.find('span', class_='price').text\n",
        "    products.append({'Product': name, 'Price': price})\n",
        "\n",
        "# Create DataFrame\n",
        "df_scraped = pd.DataFrame(products)\n",
        "\n",
        "print(\"Scraped data:\")\n",
        "print(df_scraped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Reading Other Formats <a id='other-formats'></a>\n",
        "\n",
        "Pandas supports many specialized data formats for different use cases.\n",
        "\n",
        "### Format Comparison:\n",
        "\n",
        "| Format | Use Case | Pros | Cons |\n",
        "|--------|----------|------|------|\n",
        "| **Parquet** | Big data, analytics | Fast, compressed, typed | Binary format |\n",
        "| **HDF5** | Scientific data | Hierarchical, fast | Complex |\n",
        "| **Pickle** | Python objects | Preserves types | Python-only, security risk |\n",
        "| **Feather** | Data exchange | Very fast | Limited compression |\n",
        "| **Text** | Simple data | Human-readable | Limited structure |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Parquet files (efficient columnar storage)\n",
        "\n",
        "# Create sample data\n",
        "df_sample = pd.DataFrame({\n",
        "    'id': range(1, 6),\n",
        "    'name': ['John', 'Alice', 'Bob', 'Emma', 'Michael'],\n",
        "    'salary': [75000, 85000, 65000, 70000, 90000],\n",
        "    'hire_date': pd.date_range('2020-01-01', periods=5, freq='M')\n",
        "})\n",
        "\n",
        "print(\"Sample DataFrame:\")\n",
        "print(df_sample)\n",
        "\n",
        "# Writing to Parquet (requires pyarrow or fastparquet)\n",
        "# df_sample.to_parquet('data.parquet', engine='pyarrow')\n",
        "\n",
        "# Reading from Parquet\n",
        "# df_read = pd.read_parquet('data.parquet')\n",
        "\n",
        "print(\"\\nParquet format:\")\n",
        "print(\"- Highly compressed\")\n",
        "print(\"- Preserves data types\")\n",
        "print(\"- Fast for big data\")\n",
        "print(\"\\nRequires: pip install pyarrow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Pickle files (Python serialization)\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Create DataFrame with complex types\n",
        "df_complex = pd.DataFrame({\n",
        "    'name': ['John', 'Alice', 'Bob'],\n",
        "    'scores': [[85, 90, 88], [92, 87, 95], [78, 82, 80]],  # Lists\n",
        "    'metadata': [{'age': 28}, {'age': 34}, {'age': 45}]     # Dicts\n",
        "})\n",
        "\n",
        "print(\"DataFrame with complex types:\")\n",
        "print(df_complex)\n",
        "\n",
        "# Save to pickle\n",
        "# df_complex.to_pickle('data.pkl')\n",
        "\n",
        "# Load from pickle\n",
        "# df_loaded = pd.read_pickle('data.pkl')\n",
        "\n",
        "print(\"\\nPickle format:\")\n",
        "print(\"- Preserves all Python objects\")\n",
        "print(\"- Fast read/write\")\n",
        "print(\"- WARNING: Only load from trusted sources!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: HDF5 files (hierarchical data)\n",
        "\n",
        "# Create sample data\n",
        "df_sales = pd.DataFrame({\n",
        "    'date': pd.date_range('2024-01-01', periods=5),\n",
        "    'product': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'sales': [100, 200, 150, 300, 250]\n",
        "})\n",
        "\n",
        "print(\"Sales data:\")\n",
        "print(df_sales)\n",
        "\n",
        "# Writing to HDF5 (requires tables/pytables)\n",
        "# df_sales.to_hdf('data.h5', key='sales', mode='w')\n",
        "\n",
        "# Reading from HDF5\n",
        "# df_read = pd.read_hdf('data.h5', key='sales')\n",
        "\n",
        "print(\"\\nHDF5 format:\")\n",
        "print(\"- Hierarchical structure (multiple datasets)\")\n",
        "print(\"- Fast I/O for large datasets\")\n",
        "print(\"- Supports compression\")\n",
        "print(\"\\nRequires: pip install tables\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Reading plain text files\n",
        "\n",
        "# Text file with structured data\n",
        "text_data = \"\"\"Name: John, Age: 28, City: New York\n",
        "Name: Alice, Age: 34, City: Los Angeles\n",
        "Name: Bob, Age: 45, City: Chicago\"\"\"\n",
        "\n",
        "# Parse text manually\n",
        "lines = text_data.strip().split('\\n')\n",
        "records = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split(', ')\n",
        "    record = {}\n",
        "    for part in parts:\n",
        "        key, value = part.split(': ')\n",
        "        record[key] = value\n",
        "    records.append(record)\n",
        "\n",
        "df_text = pd.DataFrame(records)\n",
        "\n",
        "print(\"Parsed text data:\")\n",
        "print(df_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Reading clipboard data\n",
        "\n",
        "# Copy data to clipboard first, then:\n",
        "# df = pd.read_clipboard()\n",
        "\n",
        "print(\"Reading from clipboard:\")\n",
        "print(\"1. Copy tabular data (from Excel, web table, etc.)\")\n",
        "print(\"2. Run: df = pd.read_clipboard()\")\n",
        "print(\"3. Data is automatically parsed into DataFrame\")\n",
        "print(\"\\nVery useful for quick data imports!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Feather format (fast data exchange)\n",
        "\n",
        "# Create sample data\n",
        "df_sample = pd.DataFrame({\n",
        "    'A': range(100),\n",
        "    'B': np.random.randn(100),\n",
        "    'C': ['text'] * 100\n",
        "})\n",
        "\n",
        "# Write to Feather\n",
        "# df_sample.to_feather('data.feather')\n",
        "\n",
        "# Read from Feather\n",
        "# df_read = pd.read_feather('data.feather')\n",
        "\n",
        "print(\"Feather format:\")\n",
        "print(\"- Very fast read/write\")\n",
        "print(\"- Language-agnostic (works with R, Python, etc.)\")\n",
        "print(\"- Good for temporary storage\")\n",
        "print(f\"\\nSample data shape: {df_sample.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Writing Data <a id='writing'></a>\n",
        "\n",
        "After processing data, you often need to export it to various formats. Pandas provides comprehensive writing functions.\n",
        "\n",
        "### Common Export Methods:\n",
        "\n",
        "| Method | Description | Usage |\n",
        "|--------|-------------|-------|\n",
        "| `to_csv()` | Export to CSV | `df.to_csv('file.csv')` |\n",
        "| `to_excel()` | Export to Excel | `df.to_excel('file.xlsx')` |\n",
        "| `to_json()` | Export to JSON | `df.to_json('file.json')` |\n",
        "| `to_sql()` | Export to database | `df.to_sql('table', conn)` |\n",
        "| `to_parquet()` | Export to Parquet | `df.to_parquet('file.parquet')` |\n",
        "| `to_pickle()` | Export to Pickle | `df.to_pickle('file.pkl')` |\n",
        "| `to_html()` | Export to HTML | `df.to_html('file.html')` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample DataFrame for export examples\n",
        "\n",
        "df_export = pd.DataFrame({\n",
        "    'Product': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n",
        "    'Category': ['Electronics', 'Home', 'Electronics', 'Sports'],\n",
        "    'Price': [299.99, 49.99, 199.99, 89.99],\n",
        "    'Stock': [50, 120, 35, 80],\n",
        "    'Rating': [4.5, 4.2, 4.8, 4.1]\n",
        "})\n",
        "\n",
        "print(\"Sample data for export:\")\n",
        "print(df_export)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Exporting to CSV with options\n",
        "\n",
        "# Basic export\n",
        "# df_export.to_csv('products.csv', index=False)\n",
        "\n",
        "# Export with custom settings\n",
        "# df_export.to_csv(\n",
        "#     'products_custom.csv',\n",
        "#     index=False,           # Don't write row numbers\n",
        "#     sep=';',               # Use semicolon separator\n",
        "#     encoding='utf-8',      # UTF-8 encoding\n",
        "#     float_format='%.2f',   # 2 decimal places for floats\n",
        "#     columns=['Product', 'Price', 'Stock']  # Select columns\n",
        "# )\n",
        "\n",
        "print(\"CSV export options:\")\n",
        "print(\"- index=False: Don't write index\")\n",
        "print(\"- sep=';': Custom delimiter\")\n",
        "print(\"- encoding='utf-8': Character encoding\")\n",
        "print(\"- float_format='%.2f': Number formatting\")\n",
        "print(\"- columns=[...]: Select specific columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Exporting to Excel with formatting\n",
        "\n",
        "# Single sheet\n",
        "# df_export.to_excel('products.xlsx', sheet_name='Products', index=False)\n",
        "\n",
        "# Multiple sheets with formatting\n",
        "# with pd.ExcelWriter('sales_report.xlsx', engine='openpyxl') as writer:\n",
        "#     df_export.to_excel(writer, sheet_name='Products', index=False)\n",
        "#     df_export[df_export['Category'] == 'Electronics'].to_excel(\n",
        "#         writer, sheet_name='Electronics', index=False\n",
        "#     )\n",
        "\n",
        "print(\"Excel export features:\")\n",
        "print(\"- Multiple sheets\")\n",
        "print(\"- Custom sheet names\")\n",
        "print(\"- Formatting options\")\n",
        "print(\"- Formulas (with xlsxwriter)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Exporting to JSON with different orientations\n",
        "\n",
        "# Records format (most common for APIs)\n",
        "json_records = df_export.to_json(orient='records', indent=2)\n",
        "print(\"JSON (records format):\")\n",
        "print(json_records)\n",
        "print()\n",
        "\n",
        "# Columns format\n",
        "json_columns = df_export.to_json(orient='columns', indent=2)\n",
        "print(\"JSON (columns format):\")\n",
        "print(json_columns[:200], \"...\")  # Show first 200 chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Exporting to SQL database\n",
        "\n",
        "# Create database connection\n",
        "conn = sqlite3.connect(':memory:')\n",
        "\n",
        "# Write to SQL\n",
        "df_export.to_sql(\n",
        "    'products',           # Table name\n",
        "    conn,                 # Connection\n",
        "    if_exists='replace',  # Replace if exists\n",
        "    index=False           # Don't write index\n",
        ")\n",
        "\n",
        "# Verify\n",
        "df_verify = pd.read_sql('SELECT * FROM products', conn)\n",
        "print(\"Data written to SQL and read back:\")\n",
        "print(df_verify)\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Exporting to HTML\n",
        "\n",
        "# Convert DataFrame to HTML table\n",
        "html_table = df_export.to_html(\n",
        "    index=False,\n",
        "    classes='table table-striped',  # CSS classes\n",
        "    border=0\n",
        ")\n",
        "\n",
        "print(\"HTML table:\")\n",
        "print(html_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 6: Exporting to clipboard\n",
        "\n",
        "# Copy to clipboard\n",
        "# df_export.to_clipboard(index=False)\n",
        "\n",
        "print(\"Clipboard export:\")\n",
        "print(\"df.to_clipboard(index=False)\")\n",
        "print(\"\\nData is copied and can be pasted into Excel, Google Sheets, etc.\")\n",
        "print(\"Very convenient for quick data sharing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Data Import Best Practices <a id='best-practices'></a>\n",
        "\n",
        "Following best practices ensures reliable and efficient data imports.\n",
        "\n",
        "### Key Principles:\n",
        "\n",
        "1. **Always validate data after import**\n",
        "2. **Handle encoding issues early**\n",
        "3. **Use appropriate data types**\n",
        "4. **Handle large files efficiently**\n",
        "5. **Check for missing values**\n",
        "6. **Verify data integrity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 1: Data validation after import\n",
        "\n",
        "# Create sample data with issues\n",
        "csv_with_issues = \"\"\"Name,Age,Salary,Department\n",
        "John,28,75000,IT\n",
        "Alice,34,85000,HR\n",
        "Bob,invalid,65000,Finance\n",
        "Emma,29,,IT\n",
        "Michael,52,90000,Marketing\"\"\"\n",
        "\n",
        "df_validate = pd.read_csv(io.StringIO(csv_with_issues))\n",
        "\n",
        "print(\"Imported data:\")\n",
        "print(df_validate)\n",
        "print()\n",
        "\n",
        "# Validation steps\n",
        "print(\"Data Validation:\")\n",
        "print(f\"1. Shape: {df_validate.shape}\")\n",
        "print(f\"2. Data types:\\n{df_validate.dtypes}\")\n",
        "print(f\"\\n3. Missing values:\\n{df_validate.isnull().sum()}\")\n",
        "print(f\"\\n4. Duplicates: {df_validate.duplicated().sum()}\")\n",
        "print(f\"\\n5. Basic statistics:\\n{df_validate.describe()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 2: Handling encoding issues\n",
        "\n",
        "# Common encodings to try\n",
        "encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
        "\n",
        "def read_with_encoding(filepath):\n",
        "    \"\"\"Try different encodings until one works\"\"\"\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, encoding=encoding)\n",
        "            print(f\"Successfully read with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    raise ValueError(\"Could not read file with any encoding\")\n",
        "\n",
        "print(\"Encoding handling:\")\n",
        "print(\"1. Try UTF-8 first (most common)\")\n",
        "print(\"2. Fall back to latin1 or cp1252 for Windows files\")\n",
        "print(\"3. Use errors='ignore' or errors='replace' as last resort\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 3: Efficient handling of large files\n",
        "\n",
        "def process_large_file(filepath, chunksize=10000):\n",
        "    \"\"\"Process large file in chunks to avoid memory issues\"\"\"\n",
        "    # Example: Calculate statistics without loading entire file\n",
        "    total_rows = 0\n",
        "    sum_values = 0\n",
        "    \n",
        "    for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
        "        total_rows += len(chunk)\n",
        "        # Process chunk here\n",
        "        # sum_values += chunk['column_name'].sum()\n",
        "    \n",
        "    return total_rows\n",
        "\n",
        "print(\"Large file handling:\")\n",
        "print(\"1. Use chunksize parameter\")\n",
        "print(\"2. Use usecols to read only needed columns\")\n",
        "print(\"3. Use dtype to reduce memory (int8 vs int64)\")\n",
        "print(\"4. Consider Parquet format for big data\")\n",
        "print(\"5. Use Dask for very large datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 4: Type conversion and validation\n",
        "\n",
        "csv_data = \"\"\"Name,Age,Salary,StartDate\n",
        "John,28,75000,2020-01-15\n",
        "Alice,34,85000,2018-06-20\n",
        "Bob,45,65000,2015-03-10\"\"\"\n",
        "\n",
        "# Read with proper types\n",
        "df_typed = pd.read_csv(\n",
        "    io.StringIO(csv_data),\n",
        "    dtype={'Name': str, 'Age': 'Int64', 'Salary': 'Int64'},\n",
        "    parse_dates=['StartDate']\n",
        ")\n",
        "\n",
        "print(\"Properly typed DataFrame:\")\n",
        "print(df_typed)\n",
        "print(f\"\\nData types:\\n{df_typed.dtypes}\")\n",
        "print(f\"\\nMemory usage: {df_typed.memory_usage(deep=True).sum()} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 5: Comprehensive data quality check\n",
        "\n",
        "def data_quality_report(df):\n",
        "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"DATA QUALITY REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\n1. BASIC INFO:\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "    \n",
        "    print(f\"\\n2. DATA TYPES:\")\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    print(f\"\\n3. MISSING VALUES:\")\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df) * 100).round(2)\n",
        "    print(pd.DataFrame({'Count': missing, 'Percentage': missing_pct}))\n",
        "    \n",
        "    print(f\"\\n4. DUPLICATES:\")\n",
        "    print(f\"   Duplicate rows: {df.duplicated().sum()}\")\n",
        "    \n",
        "    print(f\"\\n5. NUMERIC SUMMARY:\")\n",
        "    print(df.describe())\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Example usage\n",
        "df_sample = pd.DataFrame({\n",
        "    'A': [1, 2, 3, None, 5],\n",
        "    'B': [10, 20, 30, 40, 50],\n",
        "    'C': ['x', 'y', 'z', 'x', 'y']\n",
        "})\n",
        "\n",
        "data_quality_report(df_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Practice 6: Safe file operations with context managers\n",
        "\n",
        "def safe_read_csv(filepath, **kwargs):\n",
        "    \"\"\"Safely read CSV with error handling\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, **kwargs)\n",
        "        print(f\"Successfully read {len(df)} rows from {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {filepath} not found\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: File {filepath} is empty\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"Safe file reading:\")\n",
        "print(\"- Always use try-except blocks\")\n",
        "print(\"- Validate file exists before reading\")\n",
        "print(\"- Handle specific exceptions\")\n",
        "print(\"- Log errors for debugging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Practical Examples <a id='examples'></a>\n",
        "\n",
        "Real-world scenarios combining multiple data sources and techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Combining data from multiple CSV files\n",
        "\n",
        "# Simulate multiple CSV files (monthly sales data)\n",
        "jan_data = \"\"\"Date,Product,Sales\n",
        "2024-01-01,Widget A,100\n",
        "2024-01-02,Widget B,150\n",
        "2024-01-03,Widget A,120\"\"\"\n",
        "\n",
        "feb_data = \"\"\"Date,Product,Sales\n",
        "2024-02-01,Widget A,130\n",
        "2024-02-02,Widget B,170\n",
        "2024-02-03,Widget A,140\"\"\"\n",
        "\n",
        "mar_data = \"\"\"Date,Product,Sales\n",
        "2024-03-01,Widget A,160\n",
        "2024-03-02,Widget B,190\n",
        "2024-03-03,Widget A,150\"\"\"\n",
        "\n",
        "# Read and combine\n",
        "df_jan = pd.read_csv(io.StringIO(jan_data), parse_dates=['Date'])\n",
        "df_feb = pd.read_csv(io.StringIO(feb_data), parse_dates=['Date'])\n",
        "df_mar = pd.read_csv(io.StringIO(mar_data), parse_dates=['Date'])\n",
        "\n",
        "# Concatenate\n",
        "df_q1 = pd.concat([df_jan, df_feb, df_mar], ignore_index=True)\n",
        "\n",
        "print(\"Q1 Sales Data (combined):\")\n",
        "print(df_q1)\n",
        "print(f\"\\nTotal records: {len(df_q1)}\")\n",
        "print(f\"\\nTotal sales by product:\")\n",
        "print(df_q1.groupby('Product')['Sales'].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Merging data from different sources\n",
        "\n",
        "# Employee data from CSV\n",
        "employees_csv = \"\"\"EmployeeID,Name,DepartmentID\n",
        "1,John,101\n",
        "2,Alice,102\n",
        "3,Bob,101\n",
        "4,Emma,103\"\"\"\n",
        "\n",
        "# Department data from JSON\n",
        "departments_json = '''\n",
        "[\n",
        "    {\"DepartmentID\": 101, \"DepartmentName\": \"IT\", \"Budget\": 500000},\n",
        "    {\"DepartmentID\": 102, \"DepartmentName\": \"HR\", \"Budget\": 300000},\n",
        "    {\"DepartmentID\": 103, \"DepartmentName\": \"Finance\", \"Budget\": 400000}\n",
        "]\n",
        "'''\n",
        "\n",
        "# Read both sources\n",
        "df_employees = pd.read_csv(io.StringIO(employees_csv))\n",
        "df_departments = pd.read_json(departments_json)\n",
        "\n",
        "print(\"Employees:\")\n",
        "print(df_employees)\n",
        "print(\"\\nDepartments:\")\n",
        "print(df_departments)\n",
        "\n",
        "# Merge on DepartmentID\n",
        "df_merged = pd.merge(df_employees, df_departments, on='DepartmentID', how='left')\n",
        "\n",
        "print(\"\\nMerged data:\")\n",
        "print(df_merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: ETL Pipeline (Extract, Transform, Load)\n",
        "\n",
        "def etl_pipeline():\n",
        "    \"\"\"Complete ETL pipeline example\"\"\"\n",
        "    \n",
        "    # EXTRACT: Read from multiple sources\n",
        "    print(\"EXTRACT PHASE:\")\n",
        "    \n",
        "    # Source 1: CSV\n",
        "    sales_csv = \"\"\"OrderID,ProductID,Quantity,Price\n",
        "1001,P1,5,100\n",
        "1002,P2,3,200\n",
        "1003,P1,2,100\"\"\"\n",
        "    df_sales = pd.read_csv(io.StringIO(sales_csv))\n",
        "    print(\"Sales data extracted\")\n",
        "    \n",
        "    # Source 2: JSON (product info)\n",
        "    products_json = '[{\"ProductID\":\"P1\",\"Name\":\"Widget A\"},{\"ProductID\":\"P2\",\"Name\":\"Widget B\"}]'\n",
        "    df_products = pd.read_json(products_json)\n",
        "    print(\"Product data extracted\")\n",
        "    \n",
        "    # TRANSFORM: Clean and combine\n",
        "    print(\"\\nTRANSFORM PHASE:\")\n",
        "    \n",
        "    # Calculate total\n",
        "    df_sales['Total'] = df_sales['Quantity'] * df_sales['Price']\n",
        "    \n",
        "    # Merge with product info\n",
        "    df_final = pd.merge(df_sales, df_products, on='ProductID')\n",
        "    \n",
        "    # Reorder columns\n",
        "    df_final = df_final[['OrderID', 'ProductID', 'Name', 'Quantity', 'Price', 'Total']]\n",
        "    \n",
        "    print(\"Data transformed\")\n",
        "    \n",
        "    # LOAD: Save to database\n",
        "    print(\"\\nLOAD PHASE:\")\n",
        "    conn = sqlite3.connect(':memory:')\n",
        "    df_final.to_sql('order_details', conn, index=False, if_exists='replace')\n",
        "    print(\"Data loaded to database\")\n",
        "    \n",
        "    # Verify\n",
        "    df_verify = pd.read_sql('SELECT * FROM order_details', conn)\n",
        "    print(\"\\nFinal result:\")\n",
        "    print(df_verify)\n",
        "    \n",
        "    conn.close()\n",
        "    return df_final\n",
        "\n",
        "result = etl_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Reading and analyzing time series data\n",
        "\n",
        "# Stock price data\n",
        "stock_csv = \"\"\"Date,Symbol,Open,High,Low,Close,Volume\n",
        "2024-01-01,AAPL,180.50,185.20,179.80,184.00,50000000\n",
        "2024-01-02,AAPL,184.50,186.30,183.20,185.50,52000000\n",
        "2024-01-03,AAPL,185.00,187.50,184.00,186.80,48000000\n",
        "2024-01-04,AAPL,187.00,188.20,185.50,187.30,51000000\n",
        "2024-01-05,AAPL,187.50,189.00,186.80,188.50,53000000\"\"\"\n",
        "\n",
        "# Read with date parsing and indexing\n",
        "df_stock = pd.read_csv(\n",
        "    io.StringIO(stock_csv),\n",
        "    parse_dates=['Date'],\n",
        "    index_col='Date'\n",
        ")\n",
        "\n",
        "print(\"Stock data:\")\n",
        "print(df_stock)\n",
        "\n",
        "# Calculate daily returns\n",
        "df_stock['Daily_Return'] = df_stock['Close'].pct_change() * 100\n",
        "\n",
        "# Calculate moving average\n",
        "df_stock['MA_3'] = df_stock['Close'].rolling(window=3).mean()\n",
        "\n",
        "print(\"\\nWith calculated metrics:\")\n",
        "print(df_stock[['Close', 'Daily_Return', 'MA_3']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 5: Data quality monitoring\n",
        "\n",
        "def monitor_data_quality(df, report_name=\"Data Quality Report\"):\n",
        "    \"\"\"Generate automated data quality monitoring report\"\"\"\n",
        "    \n",
        "    issues = []\n",
        "    \n",
        "    # Check 1: Missing values\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.any():\n",
        "        issues.append(f\"Missing values detected in: {missing[missing > 0].to_dict()}\")\n",
        "    \n",
        "    # Check 2: Duplicates\n",
        "    dup_count = df.duplicated().sum()\n",
        "    if dup_count > 0:\n",
        "        issues.append(f\"Found {dup_count} duplicate rows\")\n",
        "    \n",
        "    # Check 3: Data types\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            # Check if numeric column stored as string\n",
        "            try:\n",
        "                pd.to_numeric(df[col])\n",
        "                issues.append(f\"Column '{col}' appears numeric but stored as text\")\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # Generate report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{report_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    if issues:\n",
        "        print(\"\\nISSUES FOUND:\")\n",
        "        for i, issue in enumerate(issues, 1):\n",
        "            print(f\"{i}. {issue}\")\n",
        "    else:\n",
        "        print(\"\\nNo issues found. Data quality is good!\")\n",
        "    \n",
        "    print(f\"\\nSummary: {len(df)} rows, {len(df.columns)} columns\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "# Test with problematic data\n",
        "test_data = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 3, None],\n",
        "    'B': ['100', '200', '300', '400', '500'],  # Should be numeric\n",
        "    'C': ['x', 'y', 'z', 'x', 'w']\n",
        "})\n",
        "\n",
        "monitor_data_quality(test_data, \"Test Data Quality Check\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 11. Summary <a id='summary'></a>\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "#### 1. **CSV Files** (Most Common):\n",
        "- Use `pd.read_csv()` for reading\n",
        "- Key parameters: `sep`, `header`, `usecols`, `dtype`, `parse_dates`\n",
        "- Handle large files with `chunksize`\n",
        "- Export with `to_csv()`\n",
        "\n",
        "#### 2. **Excel Files** (Business Standard):\n",
        "- Use `pd.read_excel()` for reading\n",
        "- Can read multiple sheets with `sheet_name=None`\n",
        "- Write with `ExcelWriter` for multiple sheets\n",
        "- Requires `openpyxl` library\n",
        "\n",
        "#### 3. **JSON Files** (Web/APIs):\n",
        "- Use `pd.read_json()` with appropriate `orient`\n",
        "- Flatten nested JSON with `json_normalize()`\n",
        "- Common for API responses\n",
        "- Export with `to_json()`\n",
        "\n",
        "#### 4. **Databases** (Production Data):\n",
        "- Use `pd.read_sql()` or `pd.read_sql_query()`\n",
        "- SQLite built-in with Python\n",
        "- Use SQLAlchemy for other databases\n",
        "- Write with `to_sql()`\n",
        "\n",
        "#### 5. **Web Sources**:\n",
        "- `pd.read_html()` for HTML tables\n",
        "- `requests` library for APIs\n",
        "- BeautifulSoup for web scraping\n",
        "- Can read CSV directly from URLs\n",
        "\n",
        "#### 6. **Other Formats**:\n",
        "- **Parquet**: Fast, compressed (big data)\n",
        "- **HDF5**: Hierarchical data\n",
        "- **Pickle**: Python objects (not portable)\n",
        "- **Feather**: Fast data exchange\n",
        "\n",
        "### Reading Functions Quick Reference:\n",
        "\n",
        "| Format | Read Function | Write Function |\n",
        "|--------|---------------|----------------|\n",
        "| CSV | `pd.read_csv()` | `df.to_csv()` |\n",
        "| Excel | `pd.read_excel()` | `df.to_excel()` |\n",
        "| JSON | `pd.read_json()` | `df.to_json()` |\n",
        "| SQL | `pd.read_sql()` | `df.to_sql()` |\n",
        "| HTML | `pd.read_html()` | `df.to_html()` |\n",
        "| Parquet | `pd.read_parquet()` | `df.to_parquet()` |\n",
        "| Pickle | `pd.read_pickle()` | `df.to_pickle()` |\n",
        "| Clipboard | `pd.read_clipboard()` | `df.to_clipboard()` |\n",
        "\n",
        "### Best Practices Summary:\n",
        "\n",
        "1. **Always Validate**:\n",
        "   - Check shape, dtypes, missing values\n",
        "   - Use `df.info()` and `df.describe()`\n",
        "   - Verify data integrity\n",
        "\n",
        "2. **Handle Encoding**:\n",
        "   - Try UTF-8 first\n",
        "   - Fall back to latin1/cp1252\n",
        "   - Specify encoding explicitly\n",
        "\n",
        "3. **Optimize Performance**:\n",
        "   - Specify dtypes upfront\n",
        "   - Use `usecols` for large files\n",
        "   - Consider chunksize for very large datasets\n",
        "\n",
        "4. **Data Quality**:\n",
        "   - Check for missing values\n",
        "   - Remove duplicates\n",
        "   - Validate data ranges\n",
        "   - Monitor data quality regularly\n",
        "\n",
        "5. **Error Handling**:\n",
        "   - Use try-except blocks\n",
        "   - Validate file existence\n",
        "   - Handle specific exceptions\n",
        "   - Log errors for debugging\n",
        "\n",
        "6. **Security**:\n",
        "   - Use parameterized SQL queries\n",
        "   - Don't use pickle from untrusted sources\n",
        "   - Validate data before processing\n",
        "   - Sanitize user inputs\n",
        "\n",
        "### Common Patterns:\n",
        "\n",
        "```python\n",
        "# Pattern 1: Read  Clean  Analyze  Export\n",
        "df = pd.read_csv('data.csv')\n",
        "df = df.dropna()  # Clean\n",
        "result = df.groupby('category').sum()  # Analyze\n",
        "result.to_excel('output.xlsx')  # Export\n",
        "\n",
        "# Pattern 2: Multiple sources  Merge  Save\n",
        "df1 = pd.read_csv('sales.csv')\n",
        "df2 = pd.read_json('products.json')\n",
        "merged = pd.merge(df1, df2, on='product_id')\n",
        "merged.to_sql('sales_details', conn)\n",
        "\n",
        "# Pattern 3: Large file  Process in chunks\n",
        "for chunk in pd.read_csv('large.csv', chunksize=10000):\n",
        "    process(chunk)  # Your processing logic\n",
        "    save(chunk)     # Save results\n",
        "```\n",
        "\n",
        "### When to Use Which Format:\n",
        "\n",
        "- **CSV**: Simple data, sharing with non-Python tools\n",
        "- **Excel**: Business reports, formatted data\n",
        "- **JSON**: Web APIs, nested structures\n",
        "- **SQL**: Large datasets, multi-user access\n",
        "- **Parquet**: Big data, data warehouses\n",
        "- **Pickle**: Temporary Python storage (avoid for long-term)\n",
        "\n",
        "### Remember:\n",
        "\n",
        "Reading data is just the first step! Always:\n",
        "1. Validate the data after import\n",
        "2. Check for quality issues\n",
        "3. Document your data sources\n",
        "4. Version your data when possible\n",
        "5. Consider data privacy and security\n",
        "\n",
        "**Mastering data import is essential for any data science workflow!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
