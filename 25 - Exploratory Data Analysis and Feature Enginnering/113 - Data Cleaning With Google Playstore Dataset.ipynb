{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "930e2780",
      "metadata": {},
      "source": [
        "# Data Cleaning - Google Play Store Dataset\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Data Cleaning** is one of the most important and time-consuming steps in any data science project. Real-world data is messy, incomplete, inconsistent, and often contains errors. Before we can perform meaningful analysis or build models, we must clean and prepare our data.\n",
        "\n",
        "### What is Data Cleaning?\n",
        "\n",
        "Data Cleaning (also called Data Cleansing or Data Scrubbing) is the process of detecting and correcting (or removing) corrupt, inaccurate, or irrelevant records from a dataset. It includes:\n",
        "- **Handling Missing Values**\n",
        "- **Removing Duplicates**\n",
        "- **Correcting Data Types**\n",
        "- **Handling Outliers**\n",
        "- **Standardizing Formats**\n",
        "- **Fixing Inconsistencies**\n",
        "- **Removing Irrelevant Data**\n",
        "\n",
        "### Why Data Cleaning Matters\n",
        "\n",
        "**\"Garbage In, Garbage Out\"** - Poor quality data leads to poor results!\n",
        "\n",
        "- **Improves Accuracy**: Clean data = better predictions\n",
        "- **Saves Time**: Prevents errors during analysis\n",
        "- **Enables Analysis**: Many algorithms can't handle missing/inconsistent data\n",
        "- **Better Decisions**: Reliable data leads to reliable insights\n",
        "- **Increases Efficiency**: Reduces computational overhead\n",
        "\n",
        "### Real-World Data Problems\n",
        "\n",
        "1. **Missing Values**: 20-30% of real datasets have missing data\n",
        "2. **Duplicates**: Can skew statistics and model performance\n",
        "3. **Inconsistent Formats**: \"5.0\", \"5\", \"5.0k\" for the same value\n",
        "4. **Type Errors**: Numbers stored as strings\n",
        "5. **Special Characters**: \"$10.99\" instead of 10.99\n",
        "6. **Outliers**: Extreme values that may be errors\n",
        "7. **Inconsistent Categories**: \"Free\", \"free\", \"FREE\", \"0\"\n",
        "\n",
        "### Google Play Store Dataset\n",
        "\n",
        "This dataset contains information about Android apps on the Google Play Store:\n",
        "- **App**: Application name\n",
        "- **Category**: App category\n",
        "- **Rating**: User rating (1-5 stars)\n",
        "- **Reviews**: Number of user reviews\n",
        "- **Size**: App size\n",
        "- **Installs**: Number of installs\n",
        "- **Type**: Free or Paid\n",
        "- **Price**: App price\n",
        "- **Content Rating**: Target audience\n",
        "- **Genres**: App genres\n",
        "- **Last Updated**: Last update date\n",
        "- **Current Ver**: Current version\n",
        "- **Android Ver**: Required Android version\n",
        "\n",
        "This dataset is notoriously messy and perfect for learning data cleaning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9fc2cec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA CLEANING TOOLKIT LOADED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ pandas - Data manipulation\")\n",
        "print(\"✓ numpy - Numerical operations\")\n",
        "print(\"✓ matplotlib & seaborn - Visualization\")\n",
        "print(\"✓ re - Regular expressions for text processing\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93114aaf",
      "metadata": {},
      "source": [
        "## 1. Load Data and Initial Inspection\n",
        "\n",
        "The first step in data cleaning is understanding what we're working with - the \"before\" picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b882273",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a messy Google Play Store dataset for demonstration\n",
        "np.random.seed(42)\n",
        "n_samples = 300\n",
        "\n",
        "categories = ['GAME', 'FAMILY', 'TOOLS', 'PRODUCTIVITY', 'MEDICAL', 'FINANCE']\n",
        "types = ['Free', 'Paid', 'free', 'FREE', 'paid']\n",
        "ratings_messy = ['4.5', '4.2', 'NaN', '3.8', 'null', '4.7', '', '19']  # Some errors\n",
        "content_ratings = ['Everyone', 'Teen', 'Mature 17+', 'Everyone 10+']\n",
        "\n",
        "# Create messy data\n",
        "data = {\n",
        "    'App': [f'App_{i}' for i in range(n_samples)],\n",
        "    'Category': np.random.choice(categories, n_samples),\n",
        "    'Rating': np.random.choice(ratings_messy + [str(round(np.random.uniform(1, 5), 1)) for _ in range(20)], n_samples),\n",
        "    'Reviews': [str(np.random.randint(10, 1000000)) if np.random.random() > 0.05 else 'NaN' for _ in range(n_samples)],\n",
        "    'Size': [f'{np.random.choice([str(np.random.randint(1, 100)) + \"M\", str(np.random.randint(1, 10)) + \"k\", \"Varies with device\"])}' \n",
        "             for _ in range(n_samples)],\n",
        "    'Installs': [f'{np.random.choice([\"1,000+\", \"10,000+\", \"100,000+\", \"1,000,000+\", \"10,000,000+\", \"Free\"])}' \n",
        "                 for _ in range(n_samples)],\n",
        "    'Type': np.random.choice(types, n_samples),\n",
        "    'Price': [f'${np.random.choice([0, 0.99, 2.99, 4.99, 9.99])}' if t in ['Paid', 'paid'] else '$0' \n",
        "              for t in np.random.choice(types, n_samples)],\n",
        "    'Content Rating': np.random.choice(content_ratings, n_samples),\n",
        "}\n",
        "\n",
        "# Introduce some missing values\n",
        "for col in ['Rating', 'Reviews', 'Size']:\n",
        "    indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
        "    for idx in indices:\n",
        "        data[col][idx] = np.nan if col == 'Rating' else 'NaN'\n",
        "\n",
        "# Add some duplicates\n",
        "for i in range(10):\n",
        "    dup_idx = np.random.randint(0, n_samples-50)\n",
        "    for col in data.keys():\n",
        "        data[col].append(data[col][dup_idx])\n",
        "\n",
        "df_dirty = pd.DataFrame(data)\n",
        "\n",
        "# Display initial state\n",
        "print(\"=\" * 70)\n",
        "print(\"INITIAL DATA INSPECTION - DIRTY DATASET\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nDataset Shape: {df_dirty.shape}\")\n",
        "print(f\"Rows: {df_dirty.shape[0]:,} | Columns: {df_dirty.shape[1]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FIRST 10 ROWS:\")\n",
        "print(\"=\" * 70)\n",
        "display(df_dirty.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATASET INFO:\")\n",
        "print(\"=\" * 70)\n",
        "df_dirty.info()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAMPLE VALUES FROM EACH COLUMN:\")\n",
        "print(\"=\" * 70)\n",
        "for col in df_dirty.columns:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Sample values: {df_dirty[col].head(5).tolist()}\")\n",
        "    print(f\"  Unique count: {df_dirty[col].nunique()}\")\n",
        "    print(f\"  Data type: {df_dirty[col].dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e03e44a",
      "metadata": {},
      "source": [
        "## 2. Identify and Document Data Quality Issues\n",
        "\n",
        "Before cleaning, let's systematically identify all problems in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afce3178",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data quality assessment\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA QUALITY ASSESSMENT REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. Missing Values\n",
        "print(\"\\n1. MISSING VALUES ANALYSIS:\")\n",
        "print(\"-\" * 70)\n",
        "missing_stats = pd.DataFrame({\n",
        "    'Column': df_dirty.columns,\n",
        "    'Missing_Count': df_dirty.isnull().sum(),\n",
        "    'Missing_Percentage': (df_dirty.isnull().sum() / len(df_dirty) * 100).round(2),\n",
        "    'Data_Type': df_dirty.dtypes\n",
        "})\n",
        "display(missing_stats[missing_stats['Missing_Count'] > 0])\n",
        "\n",
        "# 2. Duplicates\n",
        "print(\"\\n2. DUPLICATE ROWS:\")\n",
        "print(\"-\" * 70)\n",
        "n_duplicates = df_dirty.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {n_duplicates}\")\n",
        "print(f\"Percentage: {(n_duplicates/len(df_dirty)*100):.2f}%\")\n",
        "\n",
        "# 3. Data Type Issues\n",
        "print(\"\\n3. DATA TYPE ISSUES:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"Issues found:\")\n",
        "print(\"  • Rating: Should be float, currently object\")\n",
        "print(\"  • Reviews: Should be int, currently object\")\n",
        "print(\"  • Size: Contains 'M', 'k', and text\")\n",
        "print(\"  • Installs: Contains '+' and ',' characters\")\n",
        "print(\"  • Price: Contains '$' symbol\")\n",
        "print(\"  • Type: Inconsistent case (Free, free, FREE)\")\n",
        "\n",
        "# 4. Invalid/Outlier Values\n",
        "print(\"\\n4. INVALID VALUES DETECTED:\")\n",
        "print(\"-\" * 70)\n",
        "# Check Rating for values outside 1-5 range\n",
        "invalid_ratings = df_dirty[df_dirty['Rating'].notna()]\n",
        "invalid_ratings = invalid_ratings[pd.to_numeric(invalid_ratings['Rating'], errors='coerce').isna() | \n",
        "                                   (pd.to_numeric(invalid_ratings['Rating'], errors='coerce') > 5)]\n",
        "print(f\"  • Invalid ratings: {len(invalid_ratings)} entries\")\n",
        "\n",
        "# 5. Inconsistent Categories\n",
        "print(\"\\n5. INCONSISTENT CATEGORIES:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"  • Type values: {df_dirty['Type'].unique()}\")\n",
        "print(\"    Issue: Inconsistent case (Free, free, FREE, Paid, paid)\")\n",
        "\n",
        "# Visualization of data quality\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Missing values\n",
        "missing_stats[missing_stats['Missing_Count'] > 0].plot(\n",
        "    x='Column', y='Missing_Percentage', kind='bar', ax=axes[0, 0], color='red', alpha=0.7\n",
        ")\n",
        "axes[0, 0].set_title('Missing Values by Column (%)', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('')\n",
        "axes[0, 0].set_ylabel('Missing %')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Duplicates\n",
        "axes[0, 1].pie([len(df_dirty) - n_duplicates, n_duplicates], \n",
        "               labels=['Unique', 'Duplicates'], \n",
        "               autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
        "axes[0, 1].set_title('Duplicate Rows Distribution', fontweight='bold')\n",
        "\n",
        "# Data types\n",
        "df_dirty.dtypes.value_counts().plot(kind='bar', ax=axes[1, 0], color='skyblue', alpha=0.7)\n",
        "axes[1, 0].set_title('Data Types Distribution', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Data Type')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Completeness score\n",
        "completeness = ((df_dirty.size - df_dirty.isnull().sum().sum()) / df_dirty.size) * 100\n",
        "axes[1, 1].barh(['Dataset Completeness'], [completeness], color='green', alpha=0.7)\n",
        "axes[1, 1].set_xlim(0, 100)\n",
        "axes[1, 1].set_title('Overall Data Completeness', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Completeness %')\n",
        "axes[1, 1].text(completeness/2, 0, f'{completeness:.1f}%', \n",
        "                ha='center', va='center', fontsize=20, fontweight='bold', color='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY OF ISSUES TO FIX:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✗ Missing values in multiple columns\")\n",
        "print(\"✗ Duplicate rows present\")\n",
        "print(\"✗ Incorrect data types (objects instead of numeric)\")\n",
        "print(\"✗ Special characters in numeric columns ($, +, ,, M, k)\")\n",
        "print(\"✗ Inconsistent text case in categorical variables\")\n",
        "print(\"✗ Invalid values (ratings > 5, 'NaN' strings)\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d729fab1",
      "metadata": {},
      "source": [
        "## 3. Step-by-Step Data Cleaning Process\n",
        "\n",
        "Now we'll systematically clean each issue we identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8d3d68",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_clean = df_dirty.copy()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA CLEANING IN PROGRESS...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Remove Duplicates\n",
        "print(\"\\nStep 1: Removing Duplicates\")\n",
        "print(\"-\" * 70)\n",
        "before_dup = len(df_clean)\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "after_dup = len(df_clean)\n",
        "print(f\"✓ Removed {before_dup - after_dup} duplicate rows\")\n",
        "print(f\"  Before: {before_dup:,} rows | After: {after_dup:,} rows\")\n",
        "\n",
        "# Step 2: Clean Rating column\n",
        "print(\"\\nStep 2: Cleaning Rating Column\")\n",
        "print(\"-\" * 70)\n",
        "df_clean['Rating'] = pd.to_numeric(df_clean['Rating'], errors='coerce')\n",
        "df_clean['Rating'] = df_clean['Rating'].clip(1, 5)  # Valid ratings are 1-5\n",
        "invalid_before = df_clean['Rating'].isna().sum()\n",
        "df_clean['Rating'] = df_clean['Rating'].fillna(df_clean['Rating'].median())\n",
        "print(f\"✓ Converted to numeric\")\n",
        "print(f\"✓ Clipped values to 1-5 range\")\n",
        "print(f\"✓ Filled {invalid_before} missing values with median: {df_clean['Rating'].median():.2f}\")\n",
        "\n",
        "# Step 3: Clean Reviews column\n",
        "print(\"\\nStep 3: Cleaning Reviews Column\")\n",
        "print(\"-\" * 70)\n",
        "df_clean['Reviews'] = df_clean['Reviews'].replace(['NaN', 'null', ''], np.nan)\n",
        "df_clean['Reviews'] = pd.to_numeric(df_clean['Reviews'], errors='coerce')\n",
        "reviews_missing = df_clean['Reviews'].isna().sum()\n",
        "df_clean['Reviews'] = df_clean['Reviews'].fillna(0).astype(int)\n",
        "print(f\"✓ Converted to numeric\")\n",
        "print(f\"✓ Filled {reviews_missing} missing values with 0\")\n",
        "\n",
        "# Step 4: Clean Size column\n",
        "print(\"\\nStep 4: Cleaning Size Column\")\n",
        "print(\"-\" * 70)\n",
        "def clean_size(size):\n",
        "    if pd.isna(size) or 'Varies' in str(size):\n",
        "        return np.nan\n",
        "    size_str = str(size).strip()\n",
        "    if 'M' in size_str:\n",
        "        return float(size_str.replace('M', ''))\n",
        "    elif 'k' in size_str:\n",
        "        return float(size_str.replace('k', '')) / 1024\n",
        "    return np.nan\n",
        "\n",
        "df_clean['Size_MB'] = df_clean['Size'].apply(clean_size)\n",
        "size_missing = df_clean['Size_MB'].isna().sum()\n",
        "df_clean['Size_MB'] = df_clean['Size_MB'].fillna(df_clean['Size_MB'].median())\n",
        "print(f\"✓ Converted all sizes to MB\")\n",
        "print(f\"✓ Handled 'Varies with device' entries\")\n",
        "print(f\"✓ Filled {size_missing} missing values with median\")\n",
        "\n",
        "# Step 5: Clean Installs column\n",
        "print(\"\\nStep 5: Cleaning Installs Column\")\n",
        "print(\"-\" * 70)\n",
        "def clean_installs(install):\n",
        "    if pd.isna(install) or 'Free' in str(install):\n",
        "        return np.nan\n",
        "    # Remove + and , characters\n",
        "    return int(str(install).replace('+', '').replace(',', ''))\n",
        "\n",
        "df_clean['Installs_Numeric'] = df_clean['Installs'].apply(clean_installs)\n",
        "installs_missing = df_clean['Installs_Numeric'].isna().sum()\n",
        "df_clean['Installs_Numeric'] = df_clean['Installs_Numeric'].fillna(0).astype(int)\n",
        "print(f\"✓ Converted to numeric\")\n",
        "print(f\"✓ Removed special characters (+, ,)\")\n",
        "print(f\"✓ Filled {installs_missing} missing values with 0\")\n",
        "\n",
        "# Step 6: Clean Price column\n",
        "print(\"\\nStep 6: Cleaning Price Column\")\n",
        "print(\"-\" * 70)\n",
        "df_clean['Price_Numeric'] = df_clean['Price'].str.replace('$', '').astype(float)\n",
        "print(f\"✓ Removed $ symbol\")\n",
        "print(f\"✓ Converted to numeric\")\n",
        "\n",
        "# Step 7: Standardize Type column\n",
        "print(\"\\nStep 7: Standardizing Type Column\")\n",
        "print(\"-\" * 70)\n",
        "df_clean['Type'] = df_clean['Type'].str.capitalize()\n",
        "print(f\"✓ Standardized case: {df_clean['Type'].unique()}\")\n",
        "\n",
        "# Final cleaned dataset\n",
        "df_clean = df_clean[['App', 'Category', 'Rating', 'Reviews', 'Size_MB', \n",
        "                     'Installs_Numeric', 'Type', 'Price_Numeric', 'Content Rating']]\n",
        "\n",
        "df_clean.columns = ['App', 'Category', 'Rating', 'Reviews', 'Size_MB', \n",
        "                    'Installs', 'Type', 'Price', 'Content_Rating']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CLEANING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Final dataset shape: {df_clean.shape}\")\n",
        "print(f\"\\nCleaned columns:\")\n",
        "for col in df_clean.columns:\n",
        "    print(f\"  • {col:20s}: {df_clean[col].dtype}\")\n",
        "\n",
        "# Compare before and after\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Before\n",
        "axes[0].bar(['Complete', 'With Issues'], \n",
        "            [100-((df_dirty.isnull().sum().sum()/df_dirty.size)*100),\n",
        "             (df_dirty.isnull().sum().sum()/df_dirty.size)*100],\n",
        "            color=['lightcoral', 'red'], alpha=0.7)\n",
        "axes[0].set_title('Data Quality: BEFORE Cleaning', fontweight='bold', fontsize=14)\n",
        "axes[0].set_ylabel('Percentage')\n",
        "axes[0].set_ylim(0, 100)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# After\n",
        "axes[1].bar(['Complete', 'With Issues'], \n",
        "            [100-((df_clean.isnull().sum().sum()/df_clean.size)*100),\n",
        "             (df_clean.isnull().sum().sum()/df_clean.size)*100],\n",
        "            color=['lightgreen', 'orange'], alpha=0.7)\n",
        "axes[1].set_title('Data Quality: AFTER Cleaning', fontweight='bold', fontsize=14)\n",
        "axes[1].set_ylabel('Percentage')\n",
        "axes[1].set_ylim(0, 100)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Dataset is now clean and ready for analysis!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877979e3",
      "metadata": {},
      "source": [
        "## Summary: Data Cleaning Best Practices\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "**1. Data Cleaning Steps:**\n",
        "1. **Inspect Data**: Understand structure and identify issues\n",
        "2. **Document Problems**: List all quality issues\n",
        "3. **Create Cleaning Plan**: Prioritize fixes\n",
        "4. **Clean Systematically**: Handle one issue at a time\n",
        "5. **Validate Results**: Verify cleaning worked\n",
        "6. **Document Changes**: Keep track of transformations\n",
        "\n",
        "**2. Common Cleaning Operations:**\n",
        "\n",
        "| Problem | Solution | Example |\n",
        "|---------|----------|---------|\n",
        "| **Missing Values** | Fill with mean/median/mode or drop | `fillna(median)` |\n",
        "| **Duplicates** | Remove duplicate rows | `drop_duplicates()` |\n",
        "| **Wrong Data Types** | Convert to correct type | `astype(float)` |\n",
        "| **Special Characters** | Remove/replace characters | `str.replace('$', '')` |\n",
        "| **Inconsistent Case** | Standardize case | `str.lower()` or `.capitalize()` |\n",
        "| **Outliers** | Clip, cap, or remove | `.clip(min, max)` |\n",
        "| **Invalid Values** | Replace or remove | `pd.to_numeric(errors='coerce')` |\n",
        "\n",
        "**3. Key Principles:**\n",
        "- **Never Modify Original Data**: Always work on a copy\n",
        "- **Document Everything**: Record all transformations\n",
        "- **Validate Changes**: Check results after each step\n",
        "- **Consider Domain Knowledge**: Understand what values make sense\n",
        "- **Be Consistent**: Apply same rules across dataset\n",
        "- **Handle Missing Data Thoughtfully**: Don't blindly drop or fill\n",
        "\n",
        "**4. When to Remove vs. Fix:**\n",
        "\n",
        "**Remove When:**\n",
        "- Rows are completely corrupted\n",
        "- Duplicates exist\n",
        "- > 50% of row data is missing\n",
        "- Data is clearly erroneous and can't be corrected\n",
        "\n",
        "**Fix When:**\n",
        "- Issue is systematic (e.g., all prices have $)\n",
        "- Valid data with formatting issues\n",
        "- Missing values can be reasonably imputed\n",
        "- Outliers are legitimate but need handling\n",
        "\n",
        "**5. Tools and Techniques:**\n",
        "\n",
        "```python\n",
        "# Remove duplicates\n",
        "df.drop_duplicates()\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(value)  # Fill with value\n",
        "df.dropna()  # Drop rows with missing values\n",
        "df.interpolate()  # Interpolate missing values\n",
        "\n",
        "# Fix data types\n",
        "pd.to_numeric(df['col'], errors='coerce')\n",
        "df['col'].astype(float)\n",
        "\n",
        "# Clean strings\n",
        "df['col'].str.replace('$', '')\n",
        "df['col'].str.strip()  # Remove whitespace\n",
        "df['col'].str.lower()  # Lowercase\n",
        "\n",
        "# Handle outliers\n",
        "df['col'].clip(lower, upper)  # Cap values\n",
        "df[df['col'].between(low, high)]  # Filter\n",
        "```\n",
        "\n",
        "### Impact of Data Cleaning\n",
        "\n",
        "**Before Cleaning:**\n",
        "- Missing values: Multiple columns\n",
        "- Duplicates: Present\n",
        "- Invalid data types: Many\n",
        "- Special characters: Throughout\n",
        "- Inconsistent formats: Multiple issues\n",
        "\n",
        "**After Cleaning:**\n",
        "- ✓ No missing values (handled appropriately)\n",
        "- ✓ No duplicates\n",
        "- ✓ Correct data types for all columns\n",
        "- ✓ Numeric columns are truly numeric\n",
        "- ✓ Consistent formatting\n",
        "- ✓ Ready for analysis and modeling!\n",
        "\n",
        "**Remember:** Data cleaning is iterative - you may need to revisit and refine your cleaning process as you learn more about your data through analysis!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
