{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9112df46",
      "metadata": {},
      "source": [
        "# Handling Imbalanced Datasets Using SMOTE\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**SMOTE** (Synthetic Minority Over-sampling Technique) is an advanced oversampling method that creates synthetic (artificial) samples for the minority class instead of simply duplicating existing samples.\n",
        "\n",
        "### The Problem with Random Oversampling\n",
        "\n",
        "Random oversampling creates **exact duplicates** of minority class samples, which can lead to:\n",
        "- **Overfitting**: Model memorizes specific instances\n",
        "- **No new information**: Just repeating the same data\n",
        "- **Poor generalization**: Model doesn't learn the underlying pattern\n",
        "\n",
        "### What is SMOTE?\n",
        "\n",
        "SMOTE generates **new synthetic samples** by:\n",
        "1. Taking a minority class sample\n",
        "2. Finding its K nearest neighbors (from minority class)\n",
        "3. Creating new samples along the line segments joining the sample and its neighbors\n",
        "4. Adding these synthetic samples to the dataset\n",
        "\n",
        "### Key Advantages of SMOTE\n",
        "\n",
        "‚úÖ **Creates new information** - synthetic samples, not duplicates\n",
        "‚úÖ **Better generalization** - model learns patterns, not specific instances\n",
        "‚úÖ **Reduces overfitting** - more diverse training data\n",
        "‚úÖ **Interpolates** between existing samples rather than extrapolating\n",
        "\n",
        "### How SMOTE Works (Step-by-Step)\n",
        "\n",
        "```\n",
        "For each minority class sample x_i:\n",
        "1. Find K nearest minority neighbors (typically K=5)\n",
        "2. Randomly select one neighbor x_nn\n",
        "3. Generate synthetic sample:\n",
        "   x_new = x_i + Œª √ó (x_nn - x_i)\n",
        "   where Œª is random number between [0, 1]\n",
        "4. Repeat until desired balance is achieved\n",
        "```\n",
        "\n",
        "### Visual Example\n",
        "\n",
        "```\n",
        "Original minority samples: ‚óè ‚óè ‚óè\n",
        "Nearest neighbors connected: ‚óè‚Äî‚óè  ‚óè‚Äî‚óè\n",
        "Synthetic samples created:   ‚óè‚Äî‚óâ‚Äî‚óè\n",
        "New balanced dataset: ‚óè ‚óè ‚óè ‚óâ ‚óâ ‚óâ (originals + synthetics)\n",
        "```\n",
        "\n",
        "### When to Use SMOTE\n",
        "\n",
        "- Imbalanced classification problems\n",
        "- When random oversampling causes overfitting\n",
        "- Small to medium-sized datasets\n",
        "- When minority class has some structure/pattern\n",
        "- Classification tasks (not regression)\n",
        "\n",
        "### Variants of SMOTE\n",
        "\n",
        "1. **SMOTE** (original) - Standard synthetic oversampling\n",
        "2. **Borderline-SMOTE** - Focuses on borderline samples\n",
        "3. **ADASYN** - Adaptive synthetic sampling\n",
        "4. **SMOTE-ENN** - SMOTE + Edited Nearest Neighbors\n",
        "5. **SMOTE-Tomek** - SMOTE + Tomek links removal\n",
        "\n",
        "Let's implement SMOTE and see the difference!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c5f660b",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e507d2af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, \n",
        "                              accuracy_score, precision_score, recall_score, \n",
        "                              f1_score, roc_auc_score, roc_curve)\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, BorderlineSMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=2,  # Using 2 features for easy visualization\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    n_classes=2,\n",
        "    weights=[0.95, 0.05],  # 95% vs 5% imbalance\n",
        "    random_state=42,\n",
        "    flip_y=0.01\n",
        ")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"IMBALANCED DATASET FOR SMOTE DEMONSTRATION\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\nDataset Shape: {X.shape}\")\n",
        "print(f\"\\nClass Distribution:\")\n",
        "for cls, count in Counter(y).items():\n",
        "    print(f\"  Class {cls}: {count:,} samples ({count/len(y)*100:.2f}%)\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Visualization of original data\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], \n",
        "            c='blue', label='Class 0 (Majority)', alpha=0.5, s=30)\n",
        "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], \n",
        "            c='red', label='Class 1 (Minority)', alpha=0.8, s=50, edgecolors='black')\n",
        "plt.xlabel('Feature 1', fontweight='bold')\n",
        "plt.ylabel('Feature 2', fontweight='bold')\n",
        "plt.title('Original Imbalanced Training Data', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "counts = [Counter(y_train)[0], Counter(y_train)[1]]\n",
        "plt.bar(['Majority (0)', 'Minority (1)'], counts, color=['skyblue', 'salmon'], \n",
        "        alpha=0.7, edgecolor='black', linewidth=2)\n",
        "plt.ylabel('Number of Samples', fontweight='bold')\n",
        "plt.title('Training Data Distribution', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count + 20, f'{count}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf46b65",
      "metadata": {},
      "source": [
        "## Step 2: Comparing Random Oversampling vs SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f6ed02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Random Oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"RESAMPLING COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\nOriginal Training Set: {Counter(y_train)}\")\n",
        "print(f\"After Random Oversampling: {Counter(y_train_ros)}\")\n",
        "print(f\"After SMOTE: {Counter(y_train_smote)}\")\n",
        "\n",
        "# Visualize the difference\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Original data\n",
        "axes[0].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], \n",
        "                c='blue', label='Class 0', alpha=0.4, s=20)\n",
        "axes[0].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], \n",
        "                c='red', label='Class 1', alpha=0.8, s=40, edgecolors='black')\n",
        "axes[0].set_title('Original Imbalanced Data', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Random Oversampling\n",
        "axes[1].scatter(X_train_ros[y_train_ros==0, 0], X_train_ros[y_train_ros==0, 1], \n",
        "                c='blue', label='Class 0', alpha=0.4, s=20)\n",
        "axes[1].scatter(X_train_ros[y_train_ros==1, 0], X_train_ros[y_train_ros==1, 1], \n",
        "                c='red', label='Class 1 (Duplicates)', alpha=0.6, s=40, marker='s')\n",
        "axes[1].set_title('Random Oversampling (Duplicates)', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# SMOTE\n",
        "axes[2].scatter(X_train_smote[y_train_smote==0, 0], X_train_smote[y_train_smote==0, 1], \n",
        "                c='blue', label='Class 0', alpha=0.4, s=20)\n",
        "minority_original = X_train[y_train==1]\n",
        "axes[2].scatter(minority_original[:, 0], minority_original[:, 1], \n",
        "                c='darkred', label='Original Minority', alpha=0.9, s=60, \n",
        "                edgecolors='black', linewidth=2, marker='o')\n",
        "synthetic_mask = ~np.isin(X_train_smote[y_train_smote==1], minority_original).all(axis=1)\n",
        "X_synthetic = X_train_smote[y_train_smote==1][synthetic_mask]\n",
        "axes[2].scatter(X_synthetic[:, 0], X_synthetic[:, 1], \n",
        "                c='orangered', label='SMOTE Synthetic', alpha=0.7, s=40, \n",
        "                marker='^', edgecolors='black', linewidth=0.5)\n",
        "axes[2].set_title('SMOTE (Original + Synthetic)', fontsize=12, fontweight='bold')\n",
        "axes[2].legend()\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"KEY OBSERVATION:\")\n",
        "print(\"  ‚Ä¢ Random Oversampling: Creates exact duplicates (overlapping points)\")\n",
        "print(\"  ‚Ä¢ SMOTE: Creates new synthetic samples in the feature space\")\n",
        "print(\"  ‚Ä¢ SMOTE samples are interpolated between existing minority samples\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186007b8",
      "metadata": {},
      "source": [
        "## Step 3: Model Performance Comparison\n",
        "\n",
        "Let's train models and compare performance across different resampling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d652cfba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models on different datasets\n",
        "results = {}\n",
        "\n",
        "# Baseline (No resampling)\n",
        "model_base = LogisticRegression(random_state=42)\n",
        "model_base.fit(X_train, y_train)\n",
        "y_pred_base = model_base.predict(X_test)\n",
        "results['Baseline'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_base),\n",
        "    'precision': precision_score(y_test, y_pred_base),\n",
        "    'recall': recall_score(y_test, y_pred_base),\n",
        "    'f1': f1_score(y_test, y_pred_base),\n",
        "    'roc_auc': roc_auc_score(y_test, model_base.predict_proba(X_test)[:, 1])\n",
        "}\n",
        "\n",
        "# Random Oversampling\n",
        "model_ros = LogisticRegression(random_state=42)\n",
        "model_ros.fit(X_train_ros, y_train_ros)\n",
        "y_pred_ros = model_ros.predict(X_test)\n",
        "results['Random Oversampling'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_ros),\n",
        "    'precision': precision_score(y_test, y_pred_ros),\n",
        "    'recall': recall_score(y_test, y_pred_ros),\n",
        "    'f1': f1_score(y_test, y_pred_ros),\n",
        "    'roc_auc': roc_auc_score(y_test, model_ros.predict_proba(X_test)[:, 1])\n",
        "}\n",
        "\n",
        "# SMOTE\n",
        "model_smote = LogisticRegression(random_state=42)\n",
        "model_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred_smote = model_smote.predict(X_test)\n",
        "results['SMOTE'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_smote),\n",
        "    'precision': precision_score(y_test, y_pred_smote),\n",
        "    'recall': recall_score(y_test, y_pred_smote),\n",
        "    'f1': f1_score(y_test, y_pred_smote),\n",
        "    'roc_auc': roc_auc_score(y_test, model_smote.predict_proba(X_test)[:, 1])\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Performance metrics\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
        "x_pos = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, (method, scores) in enumerate(results.items()):\n",
        "    values = [scores[m] for m in metrics]\n",
        "    axes[0].bar(x_pos + i*width, values, width, label=method, alpha=0.7)\n",
        "\n",
        "axes[0].set_xticks(x_pos + width)\n",
        "axes[0].set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC'], rotation=45)\n",
        "axes[0].set_ylabel('Score', fontweight='bold')\n",
        "axes[0].set_title('Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim([0, 1.1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Focus on key metrics for imbalanced data\n",
        "key_metrics = ['Recall', 'F1', 'ROC-AUC']\n",
        "key_values = {method: [scores['recall'], scores['f1'], scores['roc_auc']] \n",
        "              for method, scores in results.items()}\n",
        "\n",
        "x_pos = np.arange(len(key_metrics))\n",
        "for i, (method, values) in enumerate(key_values.items()):\n",
        "    axes[1].bar(x_pos + i*width, values, width, label=method, alpha=0.7)\n",
        "\n",
        "axes[1].set_xticks(x_pos + width)\n",
        "axes[1].set_xticklabels(key_metrics)\n",
        "axes[1].set_ylabel('Score', fontweight='bold')\n",
        "axes[1].set_title('Key Metrics for Imbalanced Data', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0, 1.1])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üìä KEY FINDINGS:\")\n",
        "print(f\"  ‚Ä¢ SMOTE Recall: {results['SMOTE']['recall']:.4f} vs Random OS: {results['Random Oversampling']['recall']:.4f}\")\n",
        "print(f\"  ‚Ä¢ SMOTE F1: {results['SMOTE']['f1']:.4f} vs Random OS: {results['Random Oversampling']['f1']:.4f}\")\n",
        "print(f\"  ‚Ä¢ SMOTE typically provides better generalization than random oversampling\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e749be87",
      "metadata": {},
      "source": [
        "## Step 4: SMOTE Variants\n",
        "\n",
        "SMOTE has several variants that address specific scenarios. Let's explore some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83ff2b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply different SMOTE variants\n",
        "smote_variants = {\n",
        "    'SMOTE': SMOTE(random_state=42),\n",
        "    'Borderline-SMOTE': BorderlineSMOTE(random_state=42),\n",
        "    'ADASYN': ADASYN(random_state=42),\n",
        "    'SMOTE-Tomek': SMOTETomek(random_state=42),\n",
        "    'SMOTE-ENN': SMOTEENN(random_state=42)\n",
        "}\n",
        "\n",
        "variant_results = {}\n",
        "\n",
        "print(\"SMOTE VARIANTS COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for name, sampler in smote_variants.items():\n",
        "    try:\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Class Distribution: {Counter(y_resampled)}\")\n",
        "        print(f\"  Total Samples: {len(y_resampled)}\")\n",
        "        \n",
        "        # Train and evaluate\n",
        "        model = LogisticRegression(random_state=42)\n",
        "        model.fit(X_resampled, y_resampled)\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        variant_results[name] = {\n",
        "            'samples': len(y_resampled),\n",
        "            'recall': recall_score(y_test, y_pred),\n",
        "            'f1': f1_score(y_test, y_pred),\n",
        "            'roc_auc': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Sample counts\n",
        "methods = list(variant_results.keys())\n",
        "sample_counts = [variant_results[m]['samples'] for m in methods]\n",
        "axes[0].barh(methods, sample_counts, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Total Samples After Resampling', fontweight='bold')\n",
        "axes[0].set_title('Dataset Size: SMOTE Variants', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Performance comparison\n",
        "metrics_v = ['recall', 'f1', 'roc_auc']\n",
        "x_pos = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "for i, metric in enumerate(metrics_v):\n",
        "    values = [variant_results[m][metric] for m in methods]\n",
        "    axes[1].bar(x_pos + i*width, values, width, label=metric.upper(), alpha=0.7)\n",
        "\n",
        "axes[1].set_xticks(x_pos + width)\n",
        "axes[1].set_xticklabels(methods, rotation=45, ha='right')\n",
        "axes[1].set_ylabel('Score', fontweight='bold')\n",
        "axes[1].set_title('Performance: SMOTE Variants', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0, 1.1])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"VARIANT EXPLANATIONS:\")\n",
        "print(\"-\" * 100)\n",
        "print(\"‚Ä¢ SMOTE: Standard synthetic oversampling\")\n",
        "print(\"‚Ä¢ Borderline-SMOTE: Focuses on samples near decision boundary\")\n",
        "print(\"‚Ä¢ ADASYN: Adaptive - generates more synthetics for harder-to-learn samples\")\n",
        "print(\"‚Ä¢ SMOTE-Tomek: SMOTE + removes Tomek links (overlapping samples)\")\n",
        "print(\"‚Ä¢ SMOTE-ENN: SMOTE + Edited Nearest Neighbors (removes noisy samples)\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b02cbf",
      "metadata": {},
      "source": [
        "## Summary: SMOTE Best Practices\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **SMOTE creates synthetic samples** by interpolating between existing minority samples\n",
        "2. **Better than random oversampling** - avoids exact duplication and overfitting\n",
        "3. **Multiple variants available** for different scenarios\n",
        "4. **Works best with moderate imbalance** (not extreme cases)\n",
        "\n",
        "### When to Use SMOTE\n",
        "\n",
        "‚úÖ **Use SMOTE When:**\n",
        "- Imbalance ratio is moderate (1:10 to 1:100)\n",
        "- Dataset is small to medium-sized\n",
        "- Random oversampling causes overfitting\n",
        "- Minority class has clear patterns\n",
        "- Features are continuous/numerical\n",
        "\n",
        "‚ùå **Avoid SMOTE When:**\n",
        "- Extreme imbalance (>1:1000) - consider anomaly detection\n",
        "- Very high dimensional data - curse of dimensionality\n",
        "- Minority class is too sparse - not enough neighbors\n",
        "- Features are categorical - use other techniques\n",
        "- Dataset is very large - undersampling might be better\n",
        "\n",
        "### SMOTE Parameters\n",
        "\n",
        "**Important Parameters:**\n",
        "- `k_neighbors`: Number of nearest neighbors (default=5)\n",
        "  - Higher ‚Üí More diverse synthetics\n",
        "  - Lower ‚Üí Closer to original samples\n",
        "- `sampling_strategy`: Desired ratio after resampling\n",
        "  - 'auto' ‚Üí Balance to 1:1\n",
        "  - float ‚Üí Custom ratio (e.g., 0.5 ‚Üí 1:2)\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "‚úÖ **DO:**\n",
        "- Apply SMOTE only to training data (after train-test split)\n",
        "- Experiment with different k_neighbors values\n",
        "- Try SMOTE variants for better results\n",
        "- Combine with undersampling for extreme imbalance\n",
        "- Use cross-validation for robust evaluation\n",
        "- Check for noise/outliers before applying SMOTE\n",
        "\n",
        "‚ùå **DON'T:**\n",
        "- Apply SMOTE before splitting data (data leakage!)\n",
        "- Use with categorical features directly (encode first)\n",
        "- Expect miracles with extremely imbalanced data\n",
        "- Forget to scale features before SMOTE\n",
        "- Use on test/validation data\n",
        "\n",
        "### Comparison Summary\n",
        "\n",
        "| Aspect | Random Oversampling | SMOTE |\n",
        "|--------|---------------------|-------|\n",
        "| **Method** | Duplicate samples | Create synthetic samples |\n",
        "| **Overfitting Risk** | High | Lower |\n",
        "| **Generalization** | Poor | Better |\n",
        "| **Training Time** | Fast | Slower |\n",
        "| **Diversity** | No new info | New synthetic points |\n",
        "| **Best For** | Very small datasets | Medium datasets |\n",
        "\n",
        "### Real-World Tips\n",
        "\n",
        "1. **Start with standard SMOTE**, then try variants\n",
        "2. **Borderline-SMOTE** often performs best (focuses on hard samples)\n",
        "3. **SMOTE-Tomek/ENN** clean up noise after generation\n",
        "4. **ADASYN** adapts to local difficulty\n",
        "5. **Monitor both precision and recall** - balance matters\n",
        "6. **Use ensemble methods** (Random Forest, XGBoost) with SMOTE for best results\n",
        "\n",
        "### Code Template\n",
        "\n",
        "```python\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Split data FIRST\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# 2. Apply SMOTE only to training data\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# 3. Train model\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# 4. Evaluate on original test data\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You now understand SMOTE and how to handle imbalanced datasets effectively!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
