{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cd21ca8",
      "metadata": {},
      "source": [
        "# Handling Missing Values in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Missing values are one of the most common data quality issues in real-world datasets. They occur when no data value is stored for a particular variable in an observation. Handling missing values is a critical step in the data preprocessing pipeline because most machine learning algorithms cannot work with missing data.\n",
        "\n",
        "### Why Do Missing Values Occur?\n",
        "\n",
        "1. **Data Entry Errors**: Human mistakes during manual data entry\n",
        "2. **System Failures**: Technical glitches during data collection\n",
        "3. **Non-Response**: Survey participants skipping questions\n",
        "4. **Data Merging Issues**: Information lost when combining datasets\n",
        "5. **Sensor Malfunctions**: Equipment failures in IoT or automated systems\n",
        "6. **Privacy Concerns**: Sensitive information intentionally removed\n",
        "7. **Data Not Applicable**: Some fields may not apply to all records\n",
        "\n",
        "### Impact of Missing Values\n",
        "\n",
        "- **Biased Results**: Can lead to inaccurate conclusions\n",
        "- **Reduced Statistical Power**: Smaller effective sample size\n",
        "- **Algorithm Failure**: Many ML models don't accept missing values\n",
        "- **Invalid Insights**: Patterns may be hidden or distorted\n",
        "\n",
        "### Types of Missing Data\n",
        "\n",
        "1. **MCAR (Missing Completely At Random)**: No relationship between missing values and any other data\n",
        "2. **MAR (Missing At Random)**: Missingness depends on other observed variables\n",
        "3. **MNAR (Missing Not At Random)**: Missingness depends on the missing value itself\n",
        "\n",
        "Understanding the type of missingness helps choose the appropriate handling strategy.\n",
        "\n",
        "---\n",
        "\n",
        "Let's explore various techniques to handle missing values with practical examples!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099438bc",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Create Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e3f93d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 100)\n",
        "\n",
        "# Create a sample dataset with missing values\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'Age': [25, 30, np.nan, 35, 40, np.nan, 28, 33, 45, 50, np.nan, 29, 38, np.nan, 42],\n",
        "    'Salary': [50000, 60000, 55000, np.nan, 80000, 70000, np.nan, 65000, 90000, np.nan, 58000, 62000, np.nan, 75000, 85000],\n",
        "    'Experience': [2, 5, 3, np.nan, 10, 8, 4, 6, np.nan, 15, 3, 5, 9, np.nan, 12],\n",
        "    'Department': ['HR', 'IT', np.nan, 'Finance', 'IT', 'HR', 'IT', np.nan, 'Finance', 'IT', 'HR', np.nan, 'Finance', 'IT', 'HR'],\n",
        "    'Performance_Score': [3.5, 4.2, 3.8, np.nan, 4.5, 4.0, np.nan, 4.1, 4.7, 4.8, 3.9, 4.3, np.nan, 4.6, 4.4],\n",
        "    'City': ['New York', 'San Francisco', 'Chicago', 'Boston', np.nan, 'Seattle', 'Austin', np.nan, 'Denver', 'Portland', 'Miami', 'Atlanta', np.nan, 'Dallas', 'Phoenix']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original Dataset:\")\n",
        "print(\"=\" * 100)\n",
        "print(df)\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1c8a29",
      "metadata": {},
      "source": [
        "## Step 2: Detect and Analyze Missing Values\n",
        "\n",
        "Before handling missing values, we need to understand their extent and pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3b7c36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values Analysis:\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Method 1: Count of missing values\n",
        "missing_count = df.isnull().sum()\n",
        "print(\"\\n1. Missing Values Count:\")\n",
        "print(missing_count)\n",
        "\n",
        "# Method 2: Percentage of missing values\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': missing_count.values,\n",
        "    'Missing_Percentage': missing_percentage.values\n",
        "}).sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "print(\"\\n2. Missing Values Summary:\")\n",
        "print(missing_df)\n",
        "\n",
        "# Method 3: Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Bar plot\n",
        "axes[0].bar(missing_df['Column'], missing_df['Missing_Count'], color='coral', alpha=0.7)\n",
        "axes[0].set_xlabel('Columns', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Missing Count', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Missing Values Count by Column', fontsize=14, fontweight='bold')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=True, cmap='viridis', yticklabels=False, ax=axes[1])\n",
        "axes[1].set_title('Missing Values Heatmap (Yellow = Missing)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Columns', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(f\"Total Missing Values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Total Cells: {df.shape[0] * df.shape[1]}\")\n",
        "print(f\"Overall Missing Percentage: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100:.2f}%\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa53e31",
      "metadata": {},
      "source": [
        "## Method 1: Deletion Techniques\n",
        "\n",
        "### 1.1 Listwise Deletion (Complete Case Analysis)\n",
        "\n",
        "Remove entire rows that contain any missing values. This is the simplest approach but can lead to significant data loss.\n",
        "\n",
        "**When to Use:**\n",
        "- Missing data is MCAR (Missing Completely At Random)\n",
        "- Very small percentage of missing values (<5%)\n",
        "- Large dataset where losing some rows won't impact analysis\n",
        "\n",
        "**Pros:** Simple, no bias if MCAR\n",
        "**Cons:** Loss of information, reduced sample size, potential bias if not MCAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7808bdaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Listwise Deletion\n",
        "df_listwise = df.dropna()\n",
        "\n",
        "print(\"LISTWISE DELETION (Complete Case Analysis)\")\n",
        "print(\"=\" * 100)\n",
        "print(\"\\nOriginal Shape:\", df.shape)\n",
        "print(\"After Deletion:\", df_listwise.shape)\n",
        "print(f\"Rows Removed: {df.shape[0] - df_listwise.shape[0]} ({((df.shape[0] - df_listwise.shape[0]) / df.shape[0]) * 100:.1f}%)\")\n",
        "\n",
        "print(\"\\nResulting Dataset:\")\n",
        "print(df_listwise)\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "categories = ['Original', 'After Listwise Deletion']\n",
        "counts = [df.shape[0], df_listwise.shape[0]]\n",
        "colors = ['steelblue', 'coral']\n",
        "bars = ax.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)} rows',\n",
        "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Number of Rows', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Impact of Listwise Deletion', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚ö†Ô∏è WARNING: Listwise deletion removed 93.3% of the data!\")\n",
        "print(\"This is not recommended when you have this much missing data.\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b98b33",
      "metadata": {},
      "source": [
        "### 1.2 Pairwise Deletion (Available Case Analysis)\n",
        "\n",
        "Use all available data for each analysis, only excluding specific missing values.\n",
        "\n",
        "**When to Use:**\n",
        "- Multiple analyses on different variable combinations\n",
        "- Want to maximize available data for each analysis\n",
        "\n",
        "**Pros:** Uses more data than listwise deletion\n",
        "**Cons:** Can lead to inconsistent sample sizes across analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d398c3ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairwise Deletion Example\n",
        "print(\"PAIRWISE DELETION (Available Case Analysis)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Calculate correlations using pairwise deletion (default in pandas)\n",
        "numeric_cols = ['Age', 'Salary', 'Experience', 'Performance_Score']\n",
        "correlation_pairwise = df[numeric_cols].corr()\n",
        "\n",
        "print(\"\\nCorrelation Matrix (using pairwise deletion):\")\n",
        "print(correlation_pairwise.round(3))\n",
        "\n",
        "# Show how many observations were used for each correlation\n",
        "print(\"\\n\\nSample sizes used for each correlation:\")\n",
        "for col1 in numeric_cols:\n",
        "    for col2 in numeric_cols:\n",
        "        valid_pairs = df[[col1, col2]].dropna().shape[0]\n",
        "        print(f\"{col1} vs {col2}: {valid_pairs} observations\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_pairwise, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
        "plt.title('Correlation Matrix (Pairwise Deletion)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üí° NOTE: Different correlations used different numbers of observations\")\n",
        "print(\"This can lead to inconsistent results across analyses.\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b9bac5",
      "metadata": {},
      "source": [
        "### 1.3 Column Deletion\n",
        "\n",
        "Remove entire columns that have too many missing values (typically >60-70% missing).\n",
        "\n",
        "**When to Use:**\n",
        "- Column has excessive missing values\n",
        "- Column is not critical for analysis\n",
        "- Cost of imputation outweighs benefit\n",
        "\n",
        "**Pros:** Simple, removes problematic features\n",
        "**Cons:** Loss of potentially useful information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8762557e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column Deletion\n",
        "threshold = 0.3  # Drop columns with more than 30% missing values\n",
        "\n",
        "print(\"COLUMN DELETION\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\nThreshold: Drop columns with > {threshold*100}% missing values\")\n",
        "\n",
        "# Calculate missing percentage for each column\n",
        "missing_pct = df.isnull().sum() / len(df)\n",
        "columns_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
        "\n",
        "print(f\"\\nColumns to drop: {columns_to_drop}\")\n",
        "print(f\"Columns with missing > {threshold*100}%:\")\n",
        "for col in columns_to_drop:\n",
        "    print(f\"  - {col}: {missing_pct[col]*100:.1f}% missing\")\n",
        "\n",
        "# Drop columns\n",
        "df_column_dropped = df.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"\\nOriginal Columns: {df.shape[1]}\")\n",
        "print(f\"After Column Deletion: {df_column_dropped.shape[1]}\")\n",
        "print(f\"Columns Removed: {len(columns_to_drop)}\")\n",
        "\n",
        "print(\"\\nRemaining Dataset:\")\n",
        "print(df_column_dropped.head(10))\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "all_cols = list(df.columns)\n",
        "colors = ['red' if col in columns_to_drop else 'green' for col in all_cols]\n",
        "missing_percentages = [missing_pct[col] * 100 for col in all_cols]\n",
        "\n",
        "bars = ax.bar(all_cols, missing_percentages, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.axhline(y=threshold*100, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold*100}%)')\n",
        "ax.set_xlabel('Columns', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Missing Percentage (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Column Deletion: Missing Value Threshold', fontsize=14, fontweight='bold')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"‚úÖ Columns with acceptable missing values are retained.\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef8deb3",
      "metadata": {},
      "source": [
        "## Method 2: Imputation Techniques\n",
        "\n",
        "Imputation fills missing values with estimated values based on available data. This preserves sample size and can provide better results than deletion.\n",
        "\n",
        "### 2.1 Mean/Median/Mode Imputation\n",
        "\n",
        "Replace missing values with statistical measures of the available data.\n",
        "\n",
        "**Mean Imputation:** Use average value (for normally distributed numerical data)\n",
        "**Median Imputation:** Use middle value (for skewed numerical data, robust to outliers)\n",
        "**Mode Imputation:** Use most frequent value (for categorical data)\n",
        "\n",
        "**When to Use:**\n",
        "- MCAR (Missing Completely At Random) data\n",
        "- Small to moderate amount of missing values\n",
        "- Quick baseline solution\n",
        "\n",
        "**Pros:** Simple, fast, preserves sample size\n",
        "**Cons:** Reduces variance, ignores relationships between variables, can distort distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e30884a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mean/Median/Mode Imputation\n",
        "print(\"MEAN/MEDIAN/MODE IMPUTATION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Create copies for different imputation strategies\n",
        "df_mean = df.copy()\n",
        "df_median = df.copy()\n",
        "df_mode = df.copy()\n",
        "\n",
        "# Numerical columns for mean/median\n",
        "numerical_cols = ['Age', 'Salary', 'Experience', 'Performance_Score']\n",
        "\n",
        "# Mean Imputation\n",
        "for col in numerical_cols:\n",
        "    mean_value = df[col].mean()\n",
        "    df_mean[col].fillna(mean_value, inplace=True)\n",
        "    print(f\"Mean Imputation - {col}: filled with {mean_value:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "\n",
        "# Median Imputation\n",
        "for col in numerical_cols:\n",
        "    median_value = df[col].median()\n",
        "    df_median[col].fillna(median_value, inplace=True)\n",
        "    print(f\"Median Imputation - {col}: filled with {median_value:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "\n",
        "# Mode Imputation for categorical\n",
        "categorical_cols = ['Department', 'City']\n",
        "for col in categorical_cols:\n",
        "    mode_value = df[col].mode()[0]\n",
        "    df_mode[col].fillna(mode_value, inplace=True)\n",
        "    print(f\"Mode Imputation - {col}: filled with '{mode_value}'\")\n",
        "\n",
        "# Combine median for numerical and mode for categorical (best practice)\n",
        "df_imputed = df.copy()\n",
        "for col in numerical_cols:\n",
        "    df_imputed[col].fillna(df[col].median(), inplace=True)\n",
        "for col in categorical_cols:\n",
        "    df_imputed[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"Dataset after Imputation (Median + Mode):\")\n",
        "print(df_imputed)\n",
        "\n",
        "# Visualization: Compare distributions before and after imputation\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Distribution Comparison: Before vs After Imputation', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, col in enumerate(['Age', 'Salary']):\n",
        "    row = idx // 2\n",
        "    col_idx = idx % 2\n",
        "    \n",
        "    axes[row, col_idx].hist(df[col].dropna(), bins=10, alpha=0.5, label='Original', color='blue', edgecolor='black')\n",
        "    axes[row, col_idx].hist(df_imputed[col], bins=10, alpha=0.5, label='After Imputation', color='red', edgecolor='black')\n",
        "    axes[row, col_idx].set_xlabel(col, fontsize=11, fontweight='bold')\n",
        "    axes[row, col_idx].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    axes[row, col_idx].set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[row, col_idx].legend()\n",
        "    axes[row, col_idx].grid(alpha=0.3)\n",
        "\n",
        "# Department distribution\n",
        "dept_original = df['Department'].value_counts()\n",
        "dept_imputed = df_imputed['Department'].value_counts()\n",
        "x_pos = np.arange(len(dept_imputed))\n",
        "axes[1, 0].bar(x_pos - 0.2, [dept_original.get(d, 0) for d in dept_imputed.index], \n",
        "               0.4, label='Original', alpha=0.7, color='blue')\n",
        "axes[1, 0].bar(x_pos + 0.2, dept_imputed.values, 0.4, label='After Imputation', \n",
        "               alpha=0.7, color='red')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(dept_imputed.index, rotation=45)\n",
        "axes[1, 0].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_title('Department Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Statistics comparison\n",
        "stats_comparison = pd.DataFrame({\n",
        "    'Original_Mean': [df[col].mean() for col in numerical_cols[:2]],\n",
        "    'Imputed_Mean': [df_imputed[col].mean() for col in numerical_cols[:2]],\n",
        "    'Original_Std': [df[col].std() for col in numerical_cols[:2]],\n",
        "    'Imputed_Std': [df_imputed[col].std() for col in numerical_cols[:2]]\n",
        "}, index=['Age', 'Salary'])\n",
        "axes[1, 1].axis('off')\n",
        "table = axes[1, 1].table(cellText=stats_comparison.round(2).values,\n",
        "                          rowLabels=stats_comparison.index,\n",
        "                          colLabels=stats_comparison.columns,\n",
        "                          cellLoc='center',\n",
        "                          loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "axes[1, 1].set_title('Statistics Comparison', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"‚úÖ Imputation complete! Missing values: \", df_imputed.isnull().sum().sum())\n",
        "print(\"‚ö†Ô∏è NOTE: Imputation reduced variance in the data (see std deviation changes)\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07213b2f",
      "metadata": {},
      "source": [
        "### 2.2 Forward Fill and Backward Fill\n",
        "\n",
        "Replace missing values with the previous (forward fill) or next (backward fill) valid value. Useful for time-series data.\n",
        "\n",
        "**Forward Fill (ffill):** Use the last known value\n",
        "**Backward Fill (bfill):** Use the next known value\n",
        "\n",
        "**When to Use:**\n",
        "- Time-series data\n",
        "- Data with natural ordering\n",
        "- When recent/upcoming values are good predictors\n",
        "\n",
        "**Pros:** Preserves temporal patterns, no assumptions about distribution\n",
        "**Cons:** Only works with ordered data, may propagate errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828ea1a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward Fill and Backward Fill\n",
        "print(\"FORWARD FILL AND BACKWARD FILL\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Create a time-series style dataset\n",
        "time_data = {\n",
        "    'Date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
        "    'Temperature': [20, 22, np.nan, 24, np.nan, 26, 27, np.nan, 29, 30],\n",
        "    'Humidity': [65, np.nan, 68, 70, np.nan, 72, np.nan, 75, 76, np.nan]\n",
        "}\n",
        "df_time = pd.DataFrame(time_data)\n",
        "\n",
        "print(\"\\nOriginal Time-Series Data:\")\n",
        "print(df_time)\n",
        "\n",
        "# Forward Fill\n",
        "df_ffill = df_time.copy()\n",
        "df_ffill[['Temperature', 'Humidity']] = df_ffill[['Temperature', 'Humidity']].ffill()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"After Forward Fill (ffill):\")\n",
        "print(df_ffill)\n",
        "\n",
        "# Backward Fill\n",
        "df_bfill = df_time.copy()\n",
        "df_bfill[['Temperature', 'Humidity']] = df_bfill[['Temperature', 'Humidity']].bfill()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"After Backward Fill (bfill):\")\n",
        "print(df_bfill)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Temperature comparison\n",
        "axes[0].plot(df_time['Date'], df_time['Temperature'], 'o--', label='Original (with gaps)', \n",
        "             markersize=8, color='blue', alpha=0.6)\n",
        "axes[0].plot(df_ffill['Date'], df_ffill['Temperature'], 's-', label='Forward Fill', \n",
        "             markersize=6, color='green', alpha=0.7)\n",
        "axes[0].plot(df_bfill['Date'], df_bfill['Temperature'], '^-', label='Backward Fill', \n",
        "             markersize=6, color='red', alpha=0.7)\n",
        "axes[0].set_xlabel('Date', fontsize=11, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperature', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Temperature: Forward vs Backward Fill', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Humidity comparison\n",
        "axes[1].plot(df_time['Date'], df_time['Humidity'], 'o--', label='Original (with gaps)', \n",
        "             markersize=8, color='blue', alpha=0.6)\n",
        "axes[1].plot(df_ffill['Date'], df_ffill['Humidity'], 's-', label='Forward Fill', \n",
        "             markersize=6, color='green', alpha=0.7)\n",
        "axes[1].plot(df_bfill['Date'], df_bfill['Humidity'], '^-', label='Backward Fill', \n",
        "             markersize=6, color='red', alpha=0.7)\n",
        "axes[1].set_xlabel('Date', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylabel('Humidity', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Humidity: Forward vs Backward Fill', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üí° KEY OBSERVATIONS:\")\n",
        "print(\"  ‚Ä¢ Forward Fill: Missing values take the LAST known value\")\n",
        "print(\"  ‚Ä¢ Backward Fill: Missing values take the NEXT known value\")\n",
        "print(\"  ‚Ä¢ Best for time-series where recent values are relevant\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b21b5c",
      "metadata": {},
      "source": [
        "### 2.3 K-Nearest Neighbors (KNN) Imputation\n",
        "\n",
        "Use K-nearest neighbors algorithm to predict missing values based on similar observations.\n",
        "\n",
        "**How it Works:**\n",
        "1. Find K most similar observations (based on other features)\n",
        "2. Use their values to estimate the missing value\n",
        "3. Typically uses mean of K neighbors for numerical data\n",
        "\n",
        "**When to Use:**\n",
        "- Complex patterns in data\n",
        "- Relationships between features are important\n",
        "- Moderate amount of missing data\n",
        "\n",
        "**Pros:** Considers relationships between features, more accurate than simple imputation\n",
        "**Cons:** Computationally expensive, sensitive to outliers, requires feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43dc3d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN Imputation\n",
        "print(\"K-NEAREST NEIGHBORS (KNN) IMPUTATION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Prepare data for KNN (only numerical columns)\n",
        "df_knn = df[numerical_cols].copy()\n",
        "\n",
        "print(\"\\nOriginal Data (Numerical columns only):\")\n",
        "print(df_knn)\n",
        "print(f\"\\nMissing values: {df_knn.isnull().sum().sum()}\")\n",
        "\n",
        "# Apply KNN Imputation\n",
        "knn_imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
        "df_knn_imputed = pd.DataFrame(\n",
        "    knn_imputer.fit_transform(df_knn),\n",
        "    columns=df_knn.columns\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"After KNN Imputation (K=3):\")\n",
        "print(df_knn_imputed)\n",
        "print(f\"\\nMissing values: {df_knn_imputed.isnull().sum().sum()}\")\n",
        "\n",
        "# Compare with median imputation\n",
        "df_median_comp = df_knn.copy()\n",
        "for col in df_median_comp.columns:\n",
        "    df_median_comp[col].fillna(df_knn[col].median(), inplace=True)\n",
        "\n",
        "# Visualization: Compare KNN vs Median Imputation\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('KNN Imputation vs Median Imputation', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx, col in enumerate(['Age', 'Salary']):\n",
        "    row = idx\n",
        "    \n",
        "    # Distribution comparison\n",
        "    axes[row, 0].hist(df_knn[col].dropna(), bins=8, alpha=0.5, label='Original', \n",
        "                      color='blue', edgecolor='black')\n",
        "    axes[row, 0].hist(df_knn_imputed[col], bins=8, alpha=0.5, label='KNN Imputed', \n",
        "                      color='green', edgecolor='black')\n",
        "    axes[row, 0].hist(df_median_comp[col], bins=8, alpha=0.5, label='Median Imputed', \n",
        "                      color='red', edgecolor='black')\n",
        "    axes[row, 0].set_xlabel(col, fontsize=11, fontweight='bold')\n",
        "    axes[row, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "    axes[row, 0].set_title(f'{col} Distribution Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[row, 0].legend()\n",
        "    axes[row, 0].grid(alpha=0.3)\n",
        "    \n",
        "    # Box plot comparison\n",
        "    data_to_plot = [df_knn[col].dropna(), df_knn_imputed[col], df_median_comp[col]]\n",
        "    axes[row, 1].boxplot(data_to_plot, labels=['Original', 'KNN', 'Median'])\n",
        "    axes[row, 1].set_ylabel(col, fontsize=11, fontweight='bold')\n",
        "    axes[row, 1].set_title(f'{col} Box Plot Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[row, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistics comparison\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"STATISTICS COMPARISON: KNN vs Median Imputation\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for col in ['Age', 'Salary']:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Original - Mean: {df_knn[col].mean():.2f}, Std: {df_knn[col].std():.2f}\")\n",
        "    print(f\"  KNN      - Mean: {df_knn_imputed[col].mean():.2f}, Std: {df_knn_imputed[col].std():.2f}\")\n",
        "    print(f\"  Median   - Mean: {df_median_comp[col].mean():.2f}, Std: {df_median_comp[col].std():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üí° KEY INSIGHTS:\")\n",
        "print(\"  ‚Ä¢ KNN imputation preserves variance better than median imputation\")\n",
        "print(\"  ‚Ä¢ KNN considers relationships between features\")\n",
        "print(\"  ‚Ä¢ KNN produces more realistic imputed values\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13a2197",
      "metadata": {},
      "source": [
        "### 2.4 Iterative Imputation (MICE - Multivariate Imputation by Chained Equations)\n",
        "\n",
        "Uses machine learning models to predict missing values iteratively, modeling each feature with missing values as a function of other features.\n",
        "\n",
        "**How it Works:**\n",
        "1. Start with simple imputation (mean/median)\n",
        "2. For each feature with missing values:\n",
        "   - Use it as target variable\n",
        "   - Use other features as predictors\n",
        "   - Train a model and predict missing values\n",
        "3. Repeat multiple times until convergence\n",
        "\n",
        "**When to Use:**\n",
        "- Complex multivariate relationships\n",
        "- MAR (Missing At Random) data\n",
        "- Need high accuracy\n",
        "\n",
        "**Pros:** Most sophisticated, handles complex relationships, often most accurate\n",
        "**Cons:** Computationally intensive, can be slow, requires careful parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c981aa07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterative Imputation (MICE)\n",
        "print(\"ITERATIVE IMPUTATION (MICE)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Prepare data\n",
        "df_mice = df[numerical_cols].copy()\n",
        "\n",
        "print(\"\\nOriginal Data:\")\n",
        "print(df_mice)\n",
        "print(f\"\\nMissing values: {df_mice.isnull().sum().sum()}\")\n",
        "\n",
        "# Apply Iterative Imputation\n",
        "mice_imputer = IterativeImputer(max_iter=10, random_state=42, verbose=0)\n",
        "df_mice_imputed = pd.DataFrame(\n",
        "    mice_imputer.fit_transform(df_mice),\n",
        "    columns=df_mice.columns\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"After MICE Imputation:\")\n",
        "print(df_mice_imputed.round(2))\n",
        "print(f\"\\nMissing values: {df_mice_imputed.isnull().sum().sum()}\")\n",
        "\n",
        "# Compare all imputation methods\n",
        "comparison_data = {\n",
        "    'Original': df_mice['Age'].dropna().values,\n",
        "    'Median': df_median_comp['Age'].values,\n",
        "    'KNN': df_knn_imputed['Age'].values,\n",
        "    'MICE': df_mice_imputed['Age'].values\n",
        "}\n",
        "\n",
        "# Visualization: Compare all methods\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Comparison of All Imputation Methods', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Age distribution\n",
        "axes[0, 0].hist([df_mice['Age'].dropna(), df_median_comp['Age'], \n",
        "                 df_knn_imputed['Age'], df_mice_imputed['Age']], \n",
        "                bins=8, alpha=0.6, label=['Original', 'Median', 'KNN', 'MICE'],\n",
        "                color=['blue', 'red', 'green', 'purple'], edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Age', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_title('Age Distribution: All Methods', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Box plots\n",
        "data_to_plot = [df_mice['Age'].dropna(), df_median_comp['Age'], \n",
        "                df_knn_imputed['Age'], df_mice_imputed['Age']]\n",
        "bp = axes[0, 1].boxplot(data_to_plot, labels=['Original', 'Median', 'KNN', 'MICE'],\n",
        "                        patch_artist=True)\n",
        "colors = ['lightblue', 'lightcoral', 'lightgreen', 'plum']\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "axes[0, 1].set_ylabel('Age', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_title('Age Box Plot: All Methods', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Statistics comparison table\n",
        "stats_data = {\n",
        "    'Method': ['Original', 'Median', 'KNN', 'MICE'],\n",
        "    'Mean': [df_mice['Age'].mean(), df_median_comp['Age'].mean(), \n",
        "             df_knn_imputed['Age'].mean(), df_mice_imputed['Age'].mean()],\n",
        "    'Std': [df_mice['Age'].std(), df_median_comp['Age'].std(), \n",
        "            df_knn_imputed['Age'].std(), df_mice_imputed['Age'].std()],\n",
        "    'Min': [df_mice['Age'].min(), df_median_comp['Age'].min(), \n",
        "            df_knn_imputed['Age'].min(), df_mice_imputed['Age'].min()],\n",
        "    'Max': [df_mice['Age'].max(), df_median_comp['Age'].max(), \n",
        "            df_knn_imputed['Age'].max(), df_mice_imputed['Age'].max()]\n",
        "}\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "\n",
        "axes[1, 0].axis('off')\n",
        "table = axes[1, 0].table(cellText=stats_df.round(2).values,\n",
        "                          colLabels=stats_df.columns,\n",
        "                          cellLoc='center',\n",
        "                          loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "axes[1, 0].set_title('Statistics Comparison: Age', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "# Variance comparison\n",
        "variances = [df_mice['Age'].var(), df_median_comp['Age'].var(), \n",
        "             df_knn_imputed['Age'].var(), df_mice_imputed['Age'].var()]\n",
        "methods = ['Original', 'Median', 'KNN', 'MICE']\n",
        "colors_var = ['blue', 'red', 'green', 'purple']\n",
        "axes[1, 1].bar(methods, variances, color=colors_var, alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].set_ylabel('Variance', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_title('Variance Preservation Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SUMMARY: Method Comparison\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Method':<15} {'Mean':<10} {'Std Dev':<10} {'Variance':<10}\")\n",
        "print(\"-\" * 100)\n",
        "for method, mean_val, std_val, var_val in zip(\n",
        "    ['Original', 'Median', 'KNN', 'MICE'],\n",
        "    [df_mice['Age'].mean(), df_median_comp['Age'].mean(), \n",
        "     df_knn_imputed['Age'].mean(), df_mice_imputed['Age'].mean()],\n",
        "    [df_mice['Age'].std(), df_median_comp['Age'].std(), \n",
        "     df_knn_imputed['Age'].std(), df_mice_imputed['Age'].std()],\n",
        "    variances\n",
        "):\n",
        "    print(f\"{method:<15} {mean_val:<10.2f} {std_val:<10.2f} {var_val:<10.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üí° KEY FINDINGS:\")\n",
        "print(\"  ‚Ä¢ Median: Fastest but reduces variance significantly\")\n",
        "print(\"  ‚Ä¢ KNN: Good balance between accuracy and computation\")\n",
        "print(\"  ‚Ä¢ MICE: Most sophisticated, best preserves relationships\")\n",
        "print(\"  ‚Ä¢ Choose based on: data size, missing %, computational resources, accuracy needs\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98ddc13",
      "metadata": {},
      "source": [
        "## Decision Framework: Choosing the Right Method\n",
        "\n",
        "### Quick Reference Guide\n",
        "\n",
        "| Scenario | Recommended Method | Why? |\n",
        "|----------|-------------------|------|\n",
        "| < 5% missing, MCAR | Listwise Deletion | Simple, minimal data loss |\n",
        "| Time-series data | Forward/Backward Fill | Preserves temporal patterns |\n",
        "| Categorical variables | Mode Imputation | Most frequent value is reasonable |\n",
        "| Numerical, quick baseline | Median Imputation | Robust to outliers, fast |\n",
        "| Moderate missing, relationships matter | KNN Imputation | Balances accuracy and speed |\n",
        "| Complex patterns, high accuracy needed | MICE/Iterative | Most sophisticated |\n",
        "| > 70% missing in column | Column Deletion | Too much missing to impute reliably |\n",
        "\n",
        "### Step-by-Step Decision Process\n",
        "\n",
        "1. **Analyze Missing Data Pattern**\n",
        "   - Check percentage of missing values\n",
        "   - Identify type: MCAR, MAR, or MNAR\n",
        "   - Visualize missing data patterns\n",
        "\n",
        "2. **Consider Data Characteristics**\n",
        "   - Data type (numerical vs categorical)\n",
        "   - Dataset size\n",
        "   - Presence of relationships between features\n",
        "   - Time-series vs cross-sectional data\n",
        "\n",
        "3. **Evaluate Trade-offs**\n",
        "   - Computational resources available\n",
        "   - Accuracy requirements\n",
        "   - Data loss tolerance\n",
        "   - Model complexity\n",
        "\n",
        "4. **Implement and Validate**\n",
        "   - Apply chosen method\n",
        "   - Compare distributions before/after\n",
        "   - Validate with cross-validation\n",
        "   - Document assumptions made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "273cea16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Framework Visualization\n",
        "print(\"DECISION FRAMEWORK VISUALIZATION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Create a comparison summary\n",
        "methods_summary = {\n",
        "    'Method': ['Listwise Deletion', 'Column Deletion', 'Mean/Median', \n",
        "               'Forward/Backward Fill', 'KNN', 'MICE'],\n",
        "    'Speed': ['Fast', 'Fast', 'Fast', 'Fast', 'Medium', 'Slow'],\n",
        "    'Accuracy': ['Low', 'Low', 'Medium', 'Medium', 'High', 'Very High'],\n",
        "    'Preserves Variance': ['Yes', 'N/A', 'No', 'Partial', 'Yes', 'Yes'],\n",
        "    'Best For': ['MCAR, <5%', '>70% missing', 'Quick baseline', \n",
        "                 'Time-series', 'Moderate missing', 'Complex patterns'],\n",
        "    'Complexity': ['Low', 'Low', 'Low', 'Low', 'Medium', 'High']\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(methods_summary)\n",
        "\n",
        "print(\"\\nMethod Comparison Summary:\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Visualization: Method characteristics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Speed comparison\n",
        "speed_map = {'Fast': 3, 'Medium': 2, 'Slow': 1}\n",
        "speeds = [speed_map[s] for s in methods_summary['Speed']]\n",
        "colors_speed = ['green' if s==3 else 'orange' if s==2 else 'red' for s in speeds]\n",
        "axes[0].barh(methods_summary['Method'], speeds, color=colors_speed, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Speed', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Computational Speed', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xticks([1, 2, 3])\n",
        "axes[0].set_xticklabels(['Slow', 'Medium', 'Fast'])\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Accuracy comparison\n",
        "accuracy_map = {'Low': 1, 'Medium': 2, 'High': 3, 'Very High': 4}\n",
        "accuracies = [accuracy_map[a] for a in methods_summary['Accuracy']]\n",
        "colors_acc = ['red' if a==1 else 'orange' if a==2 else 'lightgreen' if a==3 else 'darkgreen' \n",
        "              for a in accuracies]\n",
        "axes[1].barh(methods_summary['Method'], accuracies, color=colors_acc, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_xlabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Imputation Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xticks([1, 2, 3, 4])\n",
        "axes[1].set_xticklabels(['Low', 'Medium', 'High', 'Very High'], rotation=45)\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Complexity comparison\n",
        "complexity_map = {'Low': 1, 'Medium': 2, 'High': 3}\n",
        "complexities = [complexity_map[c] for c in methods_summary['Complexity']]\n",
        "colors_comp = ['green' if c==1 else 'orange' if c==2 else 'red' for c in complexities]\n",
        "axes[2].barh(methods_summary['Method'], complexities, color=colors_comp, alpha=0.7, edgecolor='black')\n",
        "axes[2].set_xlabel('Complexity', fontsize=11, fontweight='bold')\n",
        "axes[2].set_title('Implementation Complexity', fontsize=12, fontweight='bold')\n",
        "axes[2].set_xticks([1, 2, 3])\n",
        "axes[2].set_xticklabels(['Low', 'Medium', 'High'])\n",
        "axes[2].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üìä INTERPRETATION:\")\n",
        "print(\"  ‚Ä¢ Green = Better/Faster/Simpler\")\n",
        "print(\"  ‚Ä¢ Orange = Moderate\")\n",
        "print(\"  ‚Ä¢ Red = Slower/Complex (but often more accurate)\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef693b2",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Missing data is common** - Nearly all real-world datasets have missing values\n",
        "2. **No one-size-fits-all solution** - Choose method based on data characteristics\n",
        "3. **Deletion is simple but risky** - Can lead to significant information loss\n",
        "4. **Simple imputation is fast** - Good for baseline, but reduces variance\n",
        "5. **Advanced methods are powerful** - KNN and MICE preserve relationships better\n",
        "6. **Validate your approach** - Always check impact on distributions and model performance\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "‚úÖ **DO:**\n",
        "- Analyze the pattern and extent of missing data first\n",
        "- Document why data is missing (if known)\n",
        "- Consider the type of missingness (MCAR, MAR, MNAR)\n",
        "- Compare distributions before and after imputation\n",
        "- Use domain knowledge to guide method selection\n",
        "- Test multiple methods and compare results\n",
        "- Consider computational constraints\n",
        "- Document your imputation strategy\n",
        "\n",
        "‚ùå **DON'T:**\n",
        "- Ignore missing values and hope the model handles them\n",
        "- Apply one method blindly to all features\n",
        "- Delete rows/columns without checking impact\n",
        "- Use mean imputation for skewed distributions\n",
        "- Impute categorical variables with numerical methods\n",
        "- Forget to validate imputation results\n",
        "- Assume imputed values are real observations\n",
        "\n",
        "### Real-World Recommendations\n",
        "\n",
        "**For Production ML Pipelines:**\n",
        "1. Start with exploratory analysis of missing patterns\n",
        "2. Use median/mode for quick baseline\n",
        "3. Implement KNN or MICE for final models\n",
        "4. Save imputation parameters from training data\n",
        "5. Apply same imputation to test/production data\n",
        "6. Monitor for new missing patterns in production\n",
        "\n",
        "**Common Mistakes to Avoid:**\n",
        "- Imputing before train-test split (data leakage!)\n",
        "- Using test data statistics for imputation\n",
        "- Not handling new missing patterns in production\n",
        "- Over-complicating when simple methods work well\n",
        "\n",
        "### When Missing Values Might Be Informative\n",
        "\n",
        "Sometimes the fact that a value is missing carries information:\n",
        "- \"Income\" missing might indicate unemployment\n",
        "- \"Age\" missing might indicate privacy concerns\n",
        "- \"Medical test\" missing might mean test wasn't needed\n",
        "\n",
        "**Solution:** Create a binary indicator feature:\n",
        "```python\n",
        "df['Age_was_missing'] = df['Age'].isnull().astype(int)\n",
        "```\n",
        "Then impute the original feature. This way you preserve both the information and can still use the feature!\n",
        "\n",
        "---\n",
        "\n",
        "**Remember:** Handling missing values is as much an art as it is science. Understanding your data and the context is crucial for making the right choice!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
