{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4bb11511",
      "metadata": {},
      "source": [
        "# Target Guided Ordinal Encoding\n",
        "\n",
        "Target Guided Ordinal Encoding (also called Target Encoding or Mean Encoding) is an advanced encoding technique that uses the **relationship between a categorical feature and the target variable** to assign numerical values.\n",
        "\n",
        "## What is Target Guided Ordinal Encoding?\n",
        "\n",
        "Instead of manually defining the order of categories (like in Ordinal Encoding), Target Encoding automatically creates an order based on the **target variable's statistics** (usually the mean) for each category.\n",
        "\n",
        "### How It Works\n",
        "\n",
        "1. For each category in a feature, calculate the mean of the target variable\n",
        "2. Order categories based on these means\n",
        "3. Assign ordinal values (0, 1, 2, ...) based on this ordering\n",
        "\n",
        "### Example\n",
        "\n",
        "Suppose you have a \"City\" feature and want to predict \"Salary\":\n",
        "\n",
        "| City | Average Salary |\n",
        "|------|---------------|\n",
        "| New York | $120,000 |\n",
        "| San Francisco | $115,000 |\n",
        "| Boston | $95,000 |\n",
        "| Chicago | $85,000 |\n",
        "\n",
        "Target Encoding would assign: Chicago â†’ 0, Boston â†’ 1, San Francisco â†’ 2, New York â†’ 3\n",
        "\n",
        "## Advantages\n",
        "\n",
        "- **Captures predictive power**: Categories are ordered by their relationship with the target\n",
        "- **Works for high cardinality**: Handles many categories efficiently\n",
        "- **Improves model performance**: Better than arbitrary ordering\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "- **Overfitting risk**: Model may memorize training data patterns\n",
        "- **Requires target variable**: Cannot be used for encoding test data without training reference\n",
        "- **Needs careful cross-validation**: Must prevent data leakage\n",
        "\n",
        "## When to Use\n",
        "\n",
        "âœ… **Use Target Encoding when:**\n",
        "- You have high cardinality categorical features (many unique values)\n",
        "- Categories don't have a natural order\n",
        "- You want to capture the predictive relationship between feature and target\n",
        "- Working with tree-based models or linear models\n",
        "\n",
        "âŒ **Avoid when:**\n",
        "- You have very few samples per category (unreliable means)\n",
        "- Your dataset is very small (high overfitting risk)\n",
        "- You need interpretability (relationship is indirect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3482b0e8",
      "metadata": {},
      "source": [
        "## Step 1: Create Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81698c1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a dataset with City and Department (high cardinality features)\n",
        "n_samples = 300\n",
        "\n",
        "cities = np.random.choice(['New York', 'San Francisco', 'Boston', 'Chicago', 'Seattle', \n",
        "                           'Austin', 'Denver', 'Portland', 'Atlanta', 'Miami'], \n",
        "                          n_samples, p=[0.15, 0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05])\n",
        "\n",
        "departments = np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', \n",
        "                                'Operations', 'Customer Support'], \n",
        "                               n_samples, p=[0.25, 0.2, 0.15, 0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "experience = np.random.randint(0, 20, n_samples)\n",
        "\n",
        "# Define salary based on city and department (hidden pattern)\n",
        "city_salary = {\n",
        "    'New York': 120000, 'San Francisco': 115000, 'Boston': 95000, \n",
        "    'Chicago': 85000, 'Seattle': 100000, 'Austin': 90000, \n",
        "    'Denver': 85000, 'Portland': 88000, 'Atlanta': 82000, 'Miami': 80000\n",
        "}\n",
        "\n",
        "department_salary = {\n",
        "    'Engineering': 110000, 'Sales': 90000, 'Marketing': 85000, \n",
        "    'HR': 75000, 'Finance': 95000, 'Operations': 80000, 'Customer Support': 70000\n",
        "}\n",
        "\n",
        "# Calculate salary with some noise\n",
        "salaries = []\n",
        "for city, dept, exp in zip(cities, departments, experience):\n",
        "    base_salary = (city_salary[city] + department_salary[dept]) / 2\n",
        "    experience_bonus = exp * 2000\n",
        "    noise = np.random.normal(0, 5000)\n",
        "    salaries.append(base_salary + experience_bonus + noise)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'City': cities,\n",
        "    'Department': departments,\n",
        "    'Experience': experience,\n",
        "    'Salary': salaries\n",
        "})\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Number of unique cities: {df['City'].nunique()}\")\n",
        "print(f\"Number of unique departments: {df['Department'].nunique()}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"First 10 rows:\")\n",
        "print(\"-\"*70)\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Statistical Summary:\")\n",
        "print(\"-\"*70)\n",
        "print(df.describe())\n",
        "\n",
        "# Visualize the distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Dataset Exploration', fontsize=16, fontweight='bold')\n",
        "\n",
        "# City distribution\n",
        "city_counts = df['City'].value_counts()\n",
        "axes[0, 0].barh(city_counts.index, city_counts.values, color='skyblue')\n",
        "axes[0, 0].set_xlabel('Count')\n",
        "axes[0, 0].set_title('City Distribution', fontweight='bold')\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Department distribution\n",
        "dept_counts = df['Department'].value_counts()\n",
        "axes[0, 1].barh(dept_counts.index, dept_counts.values, color='lightcoral')\n",
        "axes[0, 1].set_xlabel('Count')\n",
        "axes[0, 1].set_title('Department Distribution', fontweight='bold')\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Experience distribution\n",
        "axes[1, 0].hist(df['Experience'], bins=20, color='lightgreen', edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Years of Experience')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Experience Distribution', fontweight='bold')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Salary distribution\n",
        "axes[1, 1].hist(df['Salary'], bins=30, color='orange', edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Salary ($)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Salary Distribution (Target Variable)', fontweight='bold')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Dataset created successfully!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b4ad33",
      "metadata": {},
      "source": [
        "## Step 2: Calculate Target Statistics for Each Category\n",
        "\n",
        "Calculate the mean salary for each city and department to understand the relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9488532",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate mean salary for each category\n",
        "print(\"=\"*70)\n",
        "print(\"TARGET STATISTICS (Mean Salary by Category)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# City - Mean Salary\n",
        "city_means = df.groupby('City')['Salary'].mean().sort_values(ascending=False)\n",
        "print(\"\\n1. Average Salary by City (Sorted High to Low):\")\n",
        "print(\"-\"*70)\n",
        "for city, salary in city_means.items():\n",
        "    print(f\"   {city:20s} â†’ ${salary:,.2f}\")\n",
        "\n",
        "# Department - Mean Salary\n",
        "department_means = df.groupby('Department')['Salary'].mean().sort_values(ascending=False)\n",
        "print(\"\\n2. Average Salary by Department (Sorted High to Low):\")\n",
        "print(\"-\"*70)\n",
        "for dept, salary in department_means.items():\n",
        "    print(f\"   {dept:20s} â†’ ${salary:,.2f}\")\n",
        "\n",
        "# Visualize target statistics\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.suptitle('Mean Salary by Category (Target Statistics)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# City\n",
        "axes[0].barh(city_means.index, city_means.values, color='skyblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Average Salary ($)')\n",
        "axes[0].set_title('Average Salary by City', fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "for i, (city, salary) in enumerate(city_means.items()):\n",
        "    axes[0].text(salary + 1000, i, f'${salary:,.0f}', va='center', fontsize=9)\n",
        "\n",
        "# Department\n",
        "axes[1].barh(department_means.index, department_means.values, color='lightcoral', edgecolor='black')\n",
        "axes[1].set_xlabel('Average Salary ($)')\n",
        "axes[1].set_title('Average Salary by Department', fontweight='bold')\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "for i, (dept, salary) in enumerate(department_means.items()):\n",
        "    axes[1].text(salary + 1000, i, f'${salary:,.0f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š KEY INSIGHT:\")\n",
        "print(\"=\"*70)\n",
        "print(\"These mean salaries will be used to create the ordinal encoding order.\")\n",
        "print(\"Higher mean salary â†’ Higher ordinal value\")\n",
        "print(\"This captures the predictive relationship between category and target!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad98e7a5",
      "metadata": {},
      "source": [
        "## Step 3: Implement Target Guided Ordinal Encoding\n",
        "\n",
        "Now we'll encode categories based on their mean target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0a12b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Simple Target Encoding (using mean values directly)\n",
        "# This maps each category to its mean target value\n",
        "\n",
        "def target_guided_encoding(df, column, target):\n",
        "    \"\"\"\n",
        "    Encode categorical feature using target mean\n",
        "    \n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    - column: Categorical column to encode\n",
        "    - target: Target variable column\n",
        "    \n",
        "    Returns:\n",
        "    - Encoding dictionary\n",
        "    \"\"\"\n",
        "    # Calculate mean of target for each category\n",
        "    encoding_dict = df.groupby(column)[target].mean().to_dict()\n",
        "    return encoding_dict\n",
        "\n",
        "# Split data first to prevent data leakage\n",
        "X = df[['City', 'Department', 'Experience']]\n",
        "y = df['Salary']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create training DataFrame for encoding\n",
        "train_df = X_train.copy()\n",
        "train_df['Salary'] = y_train\n",
        "\n",
        "# Calculate target encoding on TRAINING data only\n",
        "city_encoding = target_guided_encoding(train_df, 'City', 'Salary')\n",
        "department_encoding = target_guided_encoding(train_df, 'Department', 'Salary')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TARGET GUIDED ENCODING DICTIONARIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. City Encoding (Mean Salary):\")\n",
        "print(\"-\"*70)\n",
        "for city, value in sorted(city_encoding.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"   {city:20s} â†’ {value:,.2f}\")\n",
        "\n",
        "print(\"\\n2. Department Encoding (Mean Salary):\")\n",
        "print(\"-\"*70)\n",
        "for dept, value in sorted(department_encoding.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"   {dept:20s} â†’ {value:,.2f}\")\n",
        "\n",
        "# Apply encoding to train and test sets\n",
        "X_train_encoded = X_train.copy()\n",
        "X_test_encoded = X_test.copy()\n",
        "\n",
        "# For training data\n",
        "X_train_encoded['City_TargetEncoded'] = X_train_encoded['City'].map(city_encoding)\n",
        "X_train_encoded['Department_TargetEncoded'] = X_train_encoded['Department'].map(department_encoding)\n",
        "\n",
        "# For test data (use training encodings)\n",
        "X_test_encoded['City_TargetEncoded'] = X_test_encoded['City'].map(city_encoding)\n",
        "X_test_encoded['Department_TargetEncoded'] = X_test_encoded['Department'].map(department_encoding)\n",
        "\n",
        "# Handle unknown categories in test set (if any) - use global mean\n",
        "global_mean = y_train.mean()\n",
        "X_test_encoded['City_TargetEncoded'].fillna(global_mean, inplace=True)\n",
        "X_test_encoded['Department_TargetEncoded'].fillna(global_mean, inplace=True)\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Encoded Training Data (First 10 rows):\")\n",
        "print(\"-\"*70)\n",
        "print(X_train_encoded[['City', 'City_TargetEncoded', 'Department', \n",
        "                        'Department_TargetEncoded', 'Experience']].head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Target Guided Encoding completed!\")\n",
        "print(\"=\"*70)\n",
        "print(\"Note: Encoding was calculated on TRAINING data only to prevent data leakage.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b8d18d",
      "metadata": {},
      "source": [
        "## Step 4: Compare with Label Encoding\n",
        "\n",
        "Let's compare Target Encoding with standard Label Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9890d8dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Label Encoding vs Target Guided Encoding\n",
        "\n",
        "# Label Encoding\n",
        "le_city = LabelEncoder()\n",
        "le_department = LabelEncoder()\n",
        "\n",
        "X_train_label = X_train.copy()\n",
        "X_test_label = X_test.copy()\n",
        "\n",
        "X_train_label['City_LabelEncoded'] = le_city.fit_transform(X_train_label['City'])\n",
        "X_train_label['Department_LabelEncoded'] = le_department.fit_transform(X_train_label['Department'])\n",
        "\n",
        "X_test_label['City_LabelEncoded'] = le_city.transform(X_test_label['City'])\n",
        "X_test_label['Department_LabelEncoded'] = le_department.transform(X_test_label['Department'])\n",
        "\n",
        "# Train models\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL COMPARISON: LABEL ENCODING vs TARGET ENCODING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Model 1: Label Encoding\n",
        "X_train_label_features = X_train_label[['City_LabelEncoded', 'Department_LabelEncoded', 'Experience']]\n",
        "X_test_label_features = X_test_label[['City_LabelEncoded', 'Department_LabelEncoded', 'Experience']]\n",
        "\n",
        "model_label = LinearRegression()\n",
        "model_label.fit(X_train_label_features, y_train)\n",
        "y_pred_label = model_label.predict(X_test_label_features)\n",
        "\n",
        "# Model 2: Target Encoding\n",
        "X_train_target_features = X_train_encoded[['City_TargetEncoded', 'Department_TargetEncoded', 'Experience']]\n",
        "X_test_target_features = X_test_encoded[['City_TargetEncoded', 'Department_TargetEncoded', 'Experience']]\n",
        "\n",
        "model_target = LinearRegression()\n",
        "model_target.fit(X_train_target_features, y_train)\n",
        "y_pred_target = model_target.predict(X_test_target_features)\n",
        "\n",
        "# Compare performance\n",
        "print(\"\\n1. Model with LABEL ENCODING:\")\n",
        "r2_label = r2_score(y_test, y_pred_label)\n",
        "mae_label = mean_absolute_error(y_test, y_pred_label)\n",
        "rmse_label = np.sqrt(mean_squared_error(y_test, y_pred_label))\n",
        "\n",
        "print(f\"   RÂ² Score:  {r2_label:.4f}\")\n",
        "print(f\"   MAE:       ${mae_label:,.2f}\")\n",
        "print(f\"   RMSE:      ${rmse_label:,.2f}\")\n",
        "\n",
        "print(\"\\n2. Model with TARGET GUIDED ENCODING:\")\n",
        "r2_target = r2_score(y_test, y_pred_target)\n",
        "mae_target = mean_absolute_error(y_test, y_pred_target)\n",
        "rmse_target = np.sqrt(mean_squared_error(y_test, y_pred_target))\n",
        "\n",
        "print(f\"   RÂ² Score:  {r2_target:.4f}\")\n",
        "print(f\"   MAE:       ${mae_target:,.2f}\")\n",
        "print(f\"   RMSE:      ${rmse_target:,.2f}\")\n",
        "\n",
        "improvement_r2 = ((r2_target - r2_label) / r2_label * 100)\n",
        "improvement_mae = ((mae_label - mae_target) / mae_label * 100)\n",
        "\n",
        "print(f\"\\nâœ… Improvement with Target Encoding:\")\n",
        "print(f\"   RÂ² Score improvement: {improvement_r2:.2f}%\")\n",
        "print(f\"   MAE reduction: {improvement_mae:.2f}%\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "fig.suptitle('Prediction Accuracy: Label Encoding vs Target Encoding', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Label Encoding predictions\n",
        "axes[0].scatter(y_test, y_pred_label, alpha=0.5, color='salmon')\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Salary ($)')\n",
        "axes[0].set_ylabel('Predicted Salary ($)')\n",
        "axes[0].set_title(f'Label Encoding\\nRÂ² = {r2_label:.4f}, MAE = ${mae_label:,.0f}', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Target Encoding predictions\n",
        "axes[1].scatter(y_test, y_pred_target, alpha=0.5, color='lightgreen')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Prediction')\n",
        "axes[1].set_xlabel('Actual Salary ($)')\n",
        "axes[1].set_ylabel('Predicted Salary ($)')\n",
        "axes[1].set_title(f'Target Encoding\\nRÂ² = {r2_target:.4f}, MAE = ${mae_target:,.0f}', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show encoding comparison for a few cities\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENCODING COMPARISON EXAMPLE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n{:20s} {:20s} {:20s}\".format(\"City\", \"Label Encoding\", \"Target Encoding\"))\n",
        "print(\"-\"*70)\n",
        "\n",
        "city_label_mapping = dict(zip(le_city.classes_, le_city.transform(le_city.classes_)))\n",
        "for city in ['New York', 'Chicago', 'Miami', 'San Francisco']:\n",
        "    if city in city_label_mapping and city in city_encoding:\n",
        "        print(\"{:20s} {:20d} ${:,.2f}\".format(\n",
        "            city, \n",
        "            city_label_mapping[city],\n",
        "            city_encoding[city]\n",
        "        ))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š KEY INSIGHT:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Target Encoding captures the relationship between city/department and salary,\")\n",
        "print(\"leading to significantly better predictions. Label Encoding uses arbitrary\")\n",
        "print(\"alphabetical order, which has no predictive value.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d96adec",
      "metadata": {},
      "source": [
        "## Step 5: Handling Overfitting with Smoothing\n",
        "\n",
        "Target Encoding can overfit, especially with small samples. We can use **smoothing** to regularize the encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94e2795",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoothed Target Encoding\n",
        "# Formula: Smoothed_Mean = (n * category_mean + m * global_mean) / (n + m)\n",
        "# where n = number of samples in category, m = smoothing parameter\n",
        "\n",
        "def smoothed_target_encoding(df, column, target, m=10):\n",
        "    \"\"\"\n",
        "    Encode categorical feature using smoothed target mean\n",
        "    \n",
        "    Parameters:\n",
        "    - df: DataFrame\n",
        "    - column: Categorical column to encode\n",
        "    - target: Target variable column\n",
        "    - m: Smoothing parameter (higher = more smoothing)\n",
        "    \n",
        "    Returns:\n",
        "    - Encoding dictionary\n",
        "    \"\"\"\n",
        "    # Global mean\n",
        "    global_mean = df[target].mean()\n",
        "    \n",
        "    # Calculate statistics for each category\n",
        "    agg = df.groupby(column)[target].agg(['mean', 'count'])\n",
        "    \n",
        "    # Apply smoothing formula\n",
        "    smoothed = (agg['count'] * agg['mean'] + m * global_mean) / (agg['count'] + m)\n",
        "    \n",
        "    return smoothed.to_dict()\n",
        "\n",
        "# Apply smoothed encoding\n",
        "city_smoothed = smoothed_target_encoding(train_df, 'City', 'Salary', m=10)\n",
        "department_smoothed = smoothed_target_encoding(train_df, 'Department', 'Salary', m=10)\n",
        "\n",
        "# Compare regular vs smoothed encoding\n",
        "print(\"=\"*70)\n",
        "print(\"REGULAR vs SMOOTHED TARGET ENCODING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. City Encoding Comparison:\")\n",
        "print(\"-\"*70)\n",
        "print(\"{:20s} {:>20s} {:>20s}\".format(\"City\", \"Regular\", \"Smoothed\"))\n",
        "print(\"-\"*70)\n",
        "\n",
        "for city in city_encoding.keys():\n",
        "    regular_val = city_encoding[city]\n",
        "    smoothed_val = city_smoothed.get(city, 0)\n",
        "    print(\"{:20s} ${:>19,.2f} ${:>19,.2f}\".format(city, regular_val, smoothed_val))\n",
        "\n",
        "print(\"\\n2. Department Encoding Comparison:\")\n",
        "print(\"-\"*70)\n",
        "print(\"{:20s} {:>20s} {:>20s}\".format(\"Department\", \"Regular\", \"Smoothed\"))\n",
        "print(\"-\"*70)\n",
        "\n",
        "for dept in department_encoding.keys():\n",
        "    regular_val = department_encoding[dept]\n",
        "    smoothed_val = department_smoothed.get(dept, 0)\n",
        "    print(\"{:20s} ${:>19,.2f} ${:>19,.2f}\".format(dept, regular_val, smoothed_val))\n",
        "\n",
        "# Apply smoothed encoding to datasets\n",
        "X_train_smoothed = X_train.copy()\n",
        "X_test_smoothed = X_test.copy()\n",
        "\n",
        "X_train_smoothed['City_Smoothed'] = X_train_smoothed['City'].map(city_smoothed)\n",
        "X_train_smoothed['Department_Smoothed'] = X_train_smoothed['Department'].map(department_smoothed)\n",
        "\n",
        "X_test_smoothed['City_Smoothed'] = X_test_smoothed['City'].map(city_smoothed)\n",
        "X_test_smoothed['Department_Smoothed'] = X_test_smoothed['Department'].map(department_smoothed)\n",
        "\n",
        "# Handle missing values\n",
        "X_test_smoothed['City_Smoothed'].fillna(global_mean, inplace=True)\n",
        "X_test_smoothed['Department_Smoothed'].fillna(global_mean, inplace=True)\n",
        "\n",
        "# Train model with smoothed encoding\n",
        "X_train_smoothed_features = X_train_smoothed[['City_Smoothed', 'Department_Smoothed', 'Experience']]\n",
        "X_test_smoothed_features = X_test_smoothed[['City_Smoothed', 'Department_Smoothed', 'Experience']]\n",
        "\n",
        "model_smoothed = LinearRegression()\n",
        "model_smoothed.fit(X_train_smoothed_features, y_train)\n",
        "y_pred_smoothed = model_smoothed.predict(X_test_smoothed_features)\n",
        "\n",
        "# Compare all three methods\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPLETE COMPARISON: LABEL vs TARGET vs SMOOTHED TARGET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n{:30s} {:>12s} {:>12s} {:>12s}\".format(\"Metric\", \"Label\", \"Target\", \"Smoothed\"))\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(\"{:30s} {:>12.4f} {:>12.4f} {:>12.4f}\".format(\n",
        "    \"RÂ² Score\", r2_label, r2_target, r2_score(y_test, y_pred_smoothed)))\n",
        "print(\"{:30s} ${:>11,.2f} ${:>11,.2f} ${:>11,.2f}\".format(\n",
        "    \"MAE\", mae_label, mae_target, mean_absolute_error(y_test, y_pred_smoothed)))\n",
        "print(\"{:30s} ${:>11,.2f} ${:>11,.2f} ${:>11,.2f}\".format(\n",
        "    \"RMSE\", rmse_label, rmse_target, np.sqrt(mean_squared_error(y_test, y_pred_smoothed))))\n",
        "\n",
        "# Visualize effect of smoothing\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "fig.suptitle('Effect of Smoothing on Target Encoding', fontsize=16, fontweight='bold')\n",
        "\n",
        "# City comparison\n",
        "cities_sorted = sorted(city_encoding.keys(), key=lambda x: city_encoding[x], reverse=True)\n",
        "regular_values = [city_encoding[c] for c in cities_sorted]\n",
        "smoothed_values = [city_smoothed.get(c, 0) for c in cities_sorted]\n",
        "\n",
        "x_pos = np.arange(len(cities_sorted))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].barh(x_pos - width/2, regular_values, width, label='Regular', color='skyblue', alpha=0.8)\n",
        "axes[0].barh(x_pos + width/2, smoothed_values, width, label='Smoothed', color='lightgreen', alpha=0.8)\n",
        "axes[0].set_yticks(x_pos)\n",
        "axes[0].set_yticklabels(cities_sorted)\n",
        "axes[0].set_xlabel('Encoded Value ($)')\n",
        "axes[0].set_title('City Encoding: Regular vs Smoothed', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Department comparison\n",
        "depts_sorted = sorted(department_encoding.keys(), key=lambda x: department_encoding[x], reverse=True)\n",
        "regular_values_dept = [department_encoding[d] for d in depts_sorted]\n",
        "smoothed_values_dept = [department_smoothed.get(d, 0) for d in depts_sorted]\n",
        "\n",
        "x_pos_dept = np.arange(len(depts_sorted))\n",
        "\n",
        "axes[1].barh(x_pos_dept - width/2, regular_values_dept, width, label='Regular', color='lightcoral', alpha=0.8)\n",
        "axes[1].barh(x_pos_dept + width/2, smoothed_values_dept, width, label='Smoothed', color='orange', alpha=0.8)\n",
        "axes[1].set_yticks(x_pos_dept)\n",
        "axes[1].set_yticklabels(depts_sorted)\n",
        "axes[1].set_xlabel('Encoded Value ($)')\n",
        "axes[1].set_title('Department Encoding: Regular vs Smoothed', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“Š SMOOTHING EFFECT:\")\n",
        "print(\"=\"*70)\n",
        "print(\"Smoothing reduces extreme values and pulls them toward the global mean.\")\n",
        "print(\"This helps prevent overfitting, especially for categories with few samples.\")\n",
        "print(\"The parameter 'm' controls the strength: higher m = more smoothing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97093674",
      "metadata": {},
      "source": [
        "## Best Practices for Target Guided Encoding\n",
        "\n",
        "### 1. **Prevent Data Leakage**\n",
        "- Always calculate encoding on **training data only**\n",
        "- Apply the same encoding to test/validation data\n",
        "- Never use test set information in encoding\n",
        "\n",
        "### 2. **Use Cross-Validation**\n",
        "- For more robust encoding, use K-Fold cross-validation\n",
        "- Calculate encoding separately for each fold\n",
        "- This reduces overfitting risk\n",
        "\n",
        "### 3. **Apply Smoothing**\n",
        "- Use smoothing for categories with few samples\n",
        "- Start with m=10 and tune based on validation performance\n",
        "- Higher smoothing for smaller datasets\n",
        "\n",
        "### 4. **Handle Unknown Categories**\n",
        "- New categories in test data should use global mean\n",
        "- Consider using `df.map()` with `fillna(global_mean)`\n",
        "\n",
        "### 5. **When to Use Target Encoding**\n",
        "\n",
        "| Scenario | Recommended? | Reason |\n",
        "|----------|-------------|--------|\n",
        "| High cardinality features (50+ categories) | âœ… Yes | Better than OHE, captures patterns |\n",
        "| Small dataset (< 1000 samples) | âš ï¸ Cautious | Use heavy smoothing or avoid |\n",
        "| Tree-based models (Random Forest, XGBoost) | âœ… Yes | Works very well |\n",
        "| Linear models | âœ… Yes | Captures non-linear relationships |\n",
        "| Features with no target relationship | âŒ No | Use label encoding or OHE |\n",
        "| Features with clear ordinal order | âŒ No | Use ordinal encoding instead |\n",
        "\n",
        "### 6. **Comparison Summary**\n",
        "\n",
        "| Method | Best For | Pros | Cons |\n",
        "|--------|----------|------|------|\n",
        "| **Label Encoding** | Binary features, tree models | Simple, fast | Random ordering |\n",
        "| **Ordinal Encoding** | Ordered categories | Preserves order | Requires domain knowledge |\n",
        "| **Target Encoding** | High cardinality | Uses target info | Overfitting risk |\n",
        "| **One-Hot Encoding** | Low cardinality nominal | No assumptions | High dimensionality |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb64876a",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Target Guided Ordinal Encoding**:\n",
        "   - Uses target variable statistics (mean) to encode categories\n",
        "   - Automatically captures predictive relationships\n",
        "   - Significantly better than arbitrary label encoding\n",
        "\n",
        "2. **How It Works**:\n",
        "   - Calculate mean target value for each category\n",
        "   - Map categories to these mean values\n",
        "   - Categories with higher target means get higher encoded values\n",
        "\n",
        "3. **Critical Requirements**:\n",
        "   - **Calculate on training data only** (prevent data leakage)\n",
        "   - Use global mean for unknown test categories\n",
        "   - Apply smoothing to prevent overfitting\n",
        "\n",
        "4. **Smoothing Formula**:\n",
        "   ```\n",
        "   Smoothed_Mean = (n Ã— category_mean + m Ã— global_mean) / (n + m)\n",
        "   ```\n",
        "   - n = number of samples in category\n",
        "   - m = smoothing parameter (typically 5-20)\n",
        "\n",
        "5. **Performance Benefits**:\n",
        "   - Better RÂ² scores compared to label encoding\n",
        "   - Lower prediction errors (MAE, RMSE)\n",
        "   - Captures meaningful category relationships\n",
        "\n",
        "6. **When to Use**:\n",
        "   - âœ… High cardinality categorical features\n",
        "   - âœ… Features with no natural ordering\n",
        "   - âœ… When target relationship exists\n",
        "   - âŒ Very small datasets (overfitting risk)\n",
        "   - âŒ Features with clear ordinal structure\n",
        "\n",
        "### Remember\n",
        "**Target Encoding is powerful but requires careful implementation to avoid data leakage and overfitting. Always use train-only encoding and consider smoothing!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
