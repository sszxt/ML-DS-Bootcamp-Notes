{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35fb354a",
      "metadata": {},
      "source": [
        "# Handling Imbalanced Datasets in Machine Learning\n",
        "\n",
        "## Introduction\n",
        "\n",
        "An **imbalanced dataset** is one where the distribution of classes is not uniform. One class (majority class) has significantly more samples than the other class(es) (minority class). This is one of the most common challenges in real-world machine learning applications.\n",
        "\n",
        "### What is Class Imbalance?\n",
        "\n",
        "Class imbalance occurs when the number of observations belonging to different classes is not equal. For example:\n",
        "- **Fraud Detection**: 99% legitimate transactions, 1% fraudulent\n",
        "- **Disease Diagnosis**: 95% healthy patients, 5% diseased\n",
        "- **Email Spam**: 80% legitimate emails, 20% spam\n",
        "- **Manufacturing Defects**: 99.5% good products, 0.5% defective\n",
        "\n",
        "### Why is Imbalance a Problem?\n",
        "\n",
        "**1. Biased Model Training**\n",
        "   - ML models optimize for overall accuracy\n",
        "   - Model learns to predict majority class always\n",
        "   - Minority class patterns are ignored\n",
        "\n",
        "**2. Poor Minority Class Performance**\n",
        "   - High accuracy but poor recall for minority class\n",
        "   - Example: 99% accuracy by always predicting \"No Fraud\" in 99:1 dataset!\n",
        "\n",
        "**3. Misleading Evaluation Metrics**\n",
        "   - Accuracy becomes meaningless\n",
        "   - Need to use precision, recall, F1-score instead\n",
        "\n",
        "### Example of the Problem\n",
        "\n",
        "```\n",
        "Dataset: 1000 samples\n",
        "- Class 0 (Healthy): 950 samples (95%)\n",
        "- Class 1 (Disease): 50 samples (5%)\n",
        "\n",
        "Model predicts everything as Class 0:\n",
        "- Accuracy: 95% ‚úì (looks great!)\n",
        "- Disease Detection: 0% ‚úó (completely fails at the important task!)\n",
        "```\n",
        "\n",
        "### Real-World Impact\n",
        "\n",
        "- **Healthcare**: Missing rare diseases can be fatal\n",
        "- **Finance**: Undetected fraud causes millions in losses\n",
        "- **Security**: Failed intrusion detection compromises systems\n",
        "- **Manufacturing**: Defective products reach customers\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "In this notebook, you will learn:\n",
        "1. How to detect and measure class imbalance\n",
        "2. Resampling techniques (Oversampling and Undersampling)\n",
        "3. When to use each technique\n",
        "4. Best practices for handling imbalanced data\n",
        "5. Proper evaluation metrics\n",
        "\n",
        "Let's dive in and master these techniques!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05f5275",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Create Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a092f64d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, \n",
        "                              accuracy_score, precision_score, recall_score, \n",
        "                              f1_score, roc_auc_score, roc_curve)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "# Simulating credit card fraud detection: 97% legitimate, 3% fraud\n",
        "X, y = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    weights=[0.97, 0.03],  # 97% class 0, 3% class 1\n",
        "    random_state=42,\n",
        "    flip_y=0.05  # Add some noise\n",
        ")\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "feature_names = [f'Feature_{i+1}' for i in range(20)]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['Target'] = y\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"IMBALANCED DATASET CREATED\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Number of Features: {X.shape[1]}\")\n",
        "print(f\"Number of Samples: {X.shape[0]}\")\n",
        "\n",
        "# Analyze class distribution\n",
        "class_dist = Counter(y)\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"CLASS DISTRIBUTION:\")\n",
        "print(\"-\" * 100)\n",
        "for cls, count in class_dist.items():\n",
        "    percentage = (count / len(y)) * 100\n",
        "    print(f\"Class {cls}: {count:,} samples ({percentage:.2f}%)\")\n",
        "\n",
        "imbalance_ratio = class_dist[0] / class_dist[1]\n",
        "print(f\"\\nImbalance Ratio (Majority:Minority): {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Bar plot of class distribution\n",
        "classes = ['Legitimate (0)', 'Fraud (1)']\n",
        "counts = [class_dist[0], class_dist[1]]\n",
        "colors = ['lightgreen', 'lightcoral']\n",
        "bars = axes[0].bar(classes, counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Class Distribution (Imbalanced)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{int(height):,}\\n({height/len(y)*100:.1f}%)',\n",
        "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(counts, labels=classes, colors=colors, autopct='%1.1f%%',\n",
        "            startangle=90, explode=(0, 0.1), shadow=True)\n",
        "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Sample distribution across first 100 samples\n",
        "sample_size = 100\n",
        "sample_y = y[:sample_size]\n",
        "axes[2].scatter(range(sample_size), sample_y, c=sample_y, cmap='RdYlGn_r', \n",
        "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "axes[2].set_xlabel('Sample Index', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel('Class Label', fontsize=12, fontweight='bold')\n",
        "axes[2].set_title('First 100 Samples (Visual Imbalance)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_yticks([0, 1])\n",
        "axes[2].set_yticklabels(['Legitimate', 'Fraud'])\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"‚ö†Ô∏è WARNING: This is a highly imbalanced dataset!\")\n",
        "print(\"   Training a model on this data without handling imbalance will lead to poor results.\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406dbd4b",
      "metadata": {},
      "source": [
        "## Step 2: Baseline Model (Without Handling Imbalance)\n",
        "\n",
        "Let's train a model on the imbalanced data to see why it's problematic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce95264",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"BASELINE MODEL (No Balancing)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Train logistic regression on imbalanced data\n",
        "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_baseline = baseline_model.predict(X_test)\n",
        "y_pred_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "precision_baseline = precision_score(y_test, y_pred_baseline)\n",
        "recall_baseline = recall_score(y_test, y_pred_baseline)\n",
        "f1_baseline = f1_score(y_test, y_pred_baseline)\n",
        "roc_auc_baseline = roc_auc_score(y_test, y_pred_proba_baseline)\n",
        "\n",
        "print(\"\\nTraining Set Class Distribution:\")\n",
        "print(f\"  Class 0: {Counter(y_train)[0]:,} samples\")\n",
        "print(f\"  Class 1: {Counter(y_train)[1]:,} samples\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"PERFORMANCE METRICS:\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Accuracy:  {accuracy_baseline:.4f} (Looks good but misleading!)\")\n",
        "print(f\"Precision: {precision_baseline:.4f} (Of predicted frauds, how many are correct?)\")\n",
        "print(f\"Recall:    {recall_baseline:.4f} ‚ö†Ô∏è (Of actual frauds, how many did we catch?)\")\n",
        "print(f\"F1-Score:  {f1_baseline:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_baseline:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 100)\n",
        "print(classification_report(y_test, y_pred_baseline, \n",
        "                          target_names=['Legitimate (0)', 'Fraud (1)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Baseline Model Performance (Imbalanced Data)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Confusion Matrix\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Pred Legitimate', 'Pred Fraud'],\n",
        "            yticklabels=['True Legitimate', 'True Fraud'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "axes[0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Metrics comparison\n",
        "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "metrics_values = [accuracy_baseline, precision_baseline, recall_baseline, f1_baseline, roc_auc_baseline]\n",
        "colors_metric = ['green' if m > 0.7 else 'orange' if m > 0.5 else 'red' for m in metrics_values]\n",
        "\n",
        "axes[1].bar(metrics_names, metrics_values, color=colors_metric, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[1].axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Poor Performance')\n",
        "axes[1].axhline(y=0.7, color='orange', linestyle='--', linewidth=2, label='Moderate')\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', linewidth=2, label='Good')\n",
        "axes[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Performance Metrics', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (name, value) in enumerate(zip(metrics_names, metrics_values)):\n",
        "    axes[1].text(i, value + 0.02, f'{value:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
        "axes[2].plot(fpr, tpr, color='darkorange', linewidth=2, label=f'ROC curve (AUC = {roc_auc_baseline:.3f})')\n",
        "axes[2].plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', label='Random Classifier')\n",
        "axes[2].set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[2].set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[2].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
        "axes[2].legend()\n",
        "axes[2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üö® PROBLEM IDENTIFIED:\")\n",
        "print(f\"   ‚Ä¢ High Accuracy ({accuracy_baseline:.1%}) gives FALSE confidence\")\n",
        "print(f\"   ‚Ä¢ Low Recall ({recall_baseline:.1%}) means we're MISSING most frauds!\")\n",
        "print(f\"   ‚Ä¢ Out of {Counter(y_test)[1]} frauds, we only detected ~{int(recall_baseline * Counter(y_test)[1])}\")\n",
        "print(\"   ‚Ä¢ This model is USELESS for fraud detection despite 'high accuracy'\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff2f66cd",
      "metadata": {},
      "source": [
        "## Method 1: Random Oversampling\n",
        "\n",
        "**Random Oversampling** increases the number of minority class samples by randomly duplicating existing minority samples until classes are balanced.\n",
        "\n",
        "### How It Works:\n",
        "1. Identify minority class samples\n",
        "2. Randomly select samples from minority class\n",
        "3. Duplicate them until reaching desired balance\n",
        "4. Combine with majority class\n",
        "\n",
        "### When to Use:\n",
        "- Small datasets where you can't afford to lose data\n",
        "- When minority class patterns need to be emphasized\n",
        "- As a baseline before trying more sophisticated methods\n",
        "\n",
        "### Advantages:\n",
        "‚úÖ Simple and easy to implement\n",
        "‚úÖ No information loss\n",
        "‚úÖ Works well with enough diverse minority samples\n",
        "\n",
        "### Disadvantages:\n",
        "‚ùå Creates exact copies (no new information)\n",
        "‚ùå Can lead to overfitting\n",
        "‚ùå May not generalize well to new data\n",
        "‚ùå Increases dataset size and training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3968f5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Oversampling\n",
        "print(\"RANDOM OVERSAMPLING\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Apply Random Oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nBefore Oversampling:\")\n",
        "print(f\"  Class 0: {Counter(y_train)[0]:,} samples\")\n",
        "print(f\"  Class 1: {Counter(y_train)[1]:,} samples\")\n",
        "print(f\"  Total: {len(y_train):,} samples\")\n",
        "\n",
        "print(\"\\nAfter Oversampling:\")\n",
        "print(f\"  Class 0: {Counter(y_train_ros)[0]:,} samples\")\n",
        "print(f\"  Class 1: {Counter(y_train_ros)[1]:,} samples\")\n",
        "print(f\"  Total: {len(y_train_ros):,} samples\")\n",
        "\n",
        "print(f\"\\nNew minority class samples added: {Counter(y_train_ros)[1] - Counter(y_train)[1]:,}\")\n",
        "print(f\"Dataset size increase: {((len(y_train_ros) - len(y_train)) / len(y_train) * 100):.1f}%\")\n",
        "\n",
        "# Train model on oversampled data\n",
        "model_ros = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_ros.fit(X_train_ros, y_train_ros)\n",
        "\n",
        "# Predictions\n",
        "y_pred_ros = model_ros.predict(X_test)\n",
        "y_pred_proba_ros = model_ros.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "accuracy_ros = accuracy_score(y_test, y_pred_ros)\n",
        "precision_ros = precision_score(y_test, y_pred_ros)\n",
        "recall_ros = recall_score(y_test, y_pred_ros)\n",
        "f1_ros = f1_score(y_test, y_pred_ros)\n",
        "roc_auc_ros = roc_auc_score(y_test, y_pred_proba_ros)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"PERFORMANCE AFTER OVERSAMPLING:\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Accuracy:  {accuracy_ros:.4f}\")\n",
        "print(f\"Precision: {precision_ros:.4f}\")\n",
        "print(f\"Recall:    {recall_ros:.4f} ‚úÖ (Much better!)\")\n",
        "print(f\"F1-Score:  {f1_ros:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_ros:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"CLASSIFICATION REPORT:\")\n",
        "print(\"-\" * 100)\n",
        "print(classification_report(y_test, y_pred_ros, \n",
        "                          target_names=['Legitimate (0)', 'Fraud (1)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_ros = confusion_matrix(y_test, y_pred_ros)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Random Oversampling Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Class distribution before and after\n",
        "dist_before = [Counter(y_train)[0], Counter(y_train)[1]]\n",
        "dist_after = [Counter(y_train_ros)[0], Counter(y_train_ros)[1]]\n",
        "x_pos = np.arange(2)\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 0].bar(x_pos - width/2, dist_before, width, label='Before', alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].bar(x_pos + width/2, dist_after, width, label='After', alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(['Class 0', 'Class 1'])\n",
        "axes[0, 0].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_title('Training Set: Before vs After Oversampling', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Confusion Matrix\n",
        "sns.heatmap(cm_ros, annot=True, fmt='d', cmap='Greens', ax=axes[0, 1],\n",
        "            xticklabels=['Pred Legitimate', 'Pred Fraud'],\n",
        "            yticklabels=['True Legitimate', 'True Fraud'])\n",
        "axes[0, 1].set_title('Confusion Matrix (Oversampled Model)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Metrics comparison: Baseline vs Oversampled\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "baseline_scores = [accuracy_baseline, precision_baseline, recall_baseline, f1_baseline, roc_auc_baseline]\n",
        "ros_scores = [accuracy_ros, precision_ros, recall_ros, f1_ros, roc_auc_ros]\n",
        "\n",
        "x_pos = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 2].bar(x_pos - width/2, baseline_scores, width, label='Baseline', alpha=0.7, color='lightcoral')\n",
        "axes[0, 2].bar(x_pos + width/2, ros_scores, width, label='Oversampled', alpha=0.7, color='lightgreen')\n",
        "axes[0, 2].set_xticks(x_pos)\n",
        "axes[0, 2].set_xticklabels(metrics, rotation=45, ha='right')\n",
        "axes[0, 2].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "axes[0, 2].set_title('Performance Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_ylim([0, 1])\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ROC Curves comparison\n",
        "fpr_ros, tpr_ros, _ = roc_curve(y_test, y_pred_proba_ros)\n",
        "axes[1, 0].plot(fpr, tpr, color='red', linewidth=2, label=f'Baseline (AUC={roc_auc_baseline:.3f})')\n",
        "axes[1, 0].plot(fpr_ros, tpr_ros, color='green', linewidth=2, label=f'Oversampled (AUC={roc_auc_ros:.3f})')\n",
        "axes[1, 0].plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', label='Random')\n",
        "axes[1, 0].set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_title('ROC Curve Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Recall improvement\n",
        "recall_improvement = ((recall_ros - recall_baseline) / recall_baseline) * 100\n",
        "f1_improvement = ((f1_ros - f1_baseline) / f1_baseline) * 100\n",
        "\n",
        "improvements = {\n",
        "    'Recall': recall_improvement,\n",
        "    'F1-Score': f1_improvement,\n",
        "    'Precision': ((precision_ros - precision_baseline) / precision_baseline) * 100\n",
        "}\n",
        "\n",
        "axes[1, 1].barh(list(improvements.keys()), list(improvements.values()), \n",
        "                color=['green' if v > 0 else 'red' for v in improvements.values()],\n",
        "                alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].axvline(x=0, color='black', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Improvement (%)', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_title('Metric Improvements vs Baseline', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Detected frauds comparison\n",
        "frauds_detected_baseline = int(recall_baseline * Counter(y_test)[1])\n",
        "frauds_detected_ros = int(recall_ros * Counter(y_test)[1])\n",
        "total_frauds = Counter(y_test)[1]\n",
        "\n",
        "detection_data = [frauds_detected_baseline, frauds_detected_ros, total_frauds]\n",
        "labels = ['Baseline\\nDetected', 'Oversampled\\nDetected', 'Total\\nFrauds']\n",
        "colors_det = ['red', 'green', 'blue']\n",
        "\n",
        "axes[1, 2].bar(labels, detection_data, color=colors_det, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[1, 2].set_ylabel('Number of Frauds', fontsize=11, fontweight='bold')\n",
        "axes[1, 2].set_title('Fraud Detection Count', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (label, value) in enumerate(zip(labels, detection_data)):\n",
        "    axes[1, 2].text(i, value + 2, f'{int(value)}', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üìä KEY IMPROVEMENTS:\")\n",
        "print(f\"   ‚Ä¢ Recall improved by {recall_improvement:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Now detecting {frauds_detected_ros}/{total_frauds} frauds vs {frauds_detected_baseline}/{total_frauds} before\")\n",
        "print(f\"   ‚Ä¢ F1-Score improved by {f1_improvement:.1f}%\")\n",
        "print(\"   ‚Ä¢ Much better balance between precision and recall\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd6f4dd",
      "metadata": {},
      "source": [
        "## Method 2: Random Undersampling\n",
        "\n",
        "**Random Undersampling** reduces the number of majority class samples by randomly removing them until classes are balanced.\n",
        "\n",
        "### How It Works:\n",
        "1. Identify majority class samples\n",
        "2. Randomly select a subset of majority class\n",
        "3. Keep only selected samples\n",
        "4. Combine with all minority class samples\n",
        "\n",
        "### When to Use:\n",
        "- Large datasets where losing majority samples is acceptable\n",
        "- When computational resources are limited\n",
        "- When majority class has redundant information\n",
        "\n",
        "### Advantages:\n",
        "‚úÖ Reduces dataset size (faster training)\n",
        "‚úÖ Reduces memory requirements\n",
        "‚úÖ Balances classes effectively\n",
        "\n",
        "### Disadvantages:\n",
        "‚ùå Loses potentially useful information\n",
        "‚ùå May discard important majority class patterns\n",
        "‚ùå Can lead to underfitting\n",
        "‚ùå Not suitable for small datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902c37cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Undersampling\n",
        "print(\"RANDOM UNDERSAMPLING\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Apply Random Undersampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nBefore Undersampling:\")\n",
        "print(f\"  Class 0: {Counter(y_train)[0]:,} samples\")\n",
        "print(f\"  Class 1: {Counter(y_train)[1]:,} samples\")\n",
        "print(f\"  Total: {len(y_train):,} samples\")\n",
        "\n",
        "print(\"\\nAfter Undersampling:\")\n",
        "print(f\"  Class 0: {Counter(y_train_rus)[0]:,} samples\")\n",
        "print(f\"  Class 1: {Counter(y_train_rus)[1]:,} samples\")\n",
        "print(f\"  Total: {len(y_train_rus):,} samples\")\n",
        "\n",
        "print(f\"\\nMajority class samples removed: {Counter(y_train)[0] - Counter(y_train_rus)[0]:,}\")\n",
        "print(f\"Dataset size reduction: {((len(y_train) - len(y_train_rus)) / len(y_train) * 100):.1f}%\")\n",
        "\n",
        "# Train model on undersampled data\n",
        "model_rus = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_rus.fit(X_train_rus, y_train_rus)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rus = model_rus.predict(X_test)\n",
        "y_pred_proba_rus = model_rus.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "accuracy_rus = accuracy_score(y_test, y_pred_rus)\n",
        "precision_rus = precision_score(y_test, y_pred_rus)\n",
        "recall_rus = recall_score(y_test, y_pred_rus)\n",
        "f1_rus = f1_score(y_test, y_pred_rus)\n",
        "roc_auc_rus = roc_auc_score(y_test, y_pred_proba_rus)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "print(\"PERFORMANCE AFTER UNDERSAMPLING:\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Accuracy:  {accuracy_rus:.4f}\")\n",
        "print(f\"Precision: {precision_rus:.4f}\")\n",
        "print(f\"Recall:    {recall_rus:.4f} ‚úÖ\")\n",
        "print(f\"F1-Score:  {f1_rus:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc_rus:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_rus = confusion_matrix(y_test, y_pred_rus)\n",
        "\n",
        "# Comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Random Undersampling Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Dataset size comparison\n",
        "methods = ['Original', 'Oversampling', 'Undersampling']\n",
        "sizes = [len(y_train), len(y_train_ros), len(y_train_rus)]\n",
        "colors_size = ['blue', 'coral', 'lightgreen']\n",
        "\n",
        "axes[0, 0].bar(methods, sizes, color=colors_size, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[0, 0].set_ylabel('Dataset Size', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_title('Training Set Size Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for i, (method, size) in enumerate(zip(methods, sizes)):\n",
        "    axes[0, 0].text(i, size + 100, f'{size:,}', ha='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Performance comparison: All three methods\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "baseline_scores = [accuracy_baseline, precision_baseline, recall_baseline, f1_baseline, roc_auc_baseline]\n",
        "ros_scores = [accuracy_ros, precision_ros, recall_ros, f1_ros, roc_auc_ros]\n",
        "rus_scores = [accuracy_rus, precision_rus, recall_rus, f1_rus, roc_auc_rus]\n",
        "\n",
        "x_pos = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "axes[0, 1].bar(x_pos - width, baseline_scores, width, label='Baseline', alpha=0.7, color='lightcoral')\n",
        "axes[0, 1].bar(x_pos, ros_scores, width, label='Oversampling', alpha=0.7, color='lightblue')\n",
        "axes[0, 1].bar(x_pos + width, rus_scores, width, label='Undersampling', alpha=0.7, color='lightgreen')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(metrics, rotation=45, ha='right')\n",
        "axes[0, 1].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_title('Performance Metrics: All Methods', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylim([0, 1.1])\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Confusion matrices comparison\n",
        "axes[1, 0].axis('off')\n",
        "fig_text = f\"\"\"\n",
        "CONFUSION MATRIX COMPARISON\n",
        "\n",
        "Baseline Model:\n",
        "  True Negatives:  {cm_baseline[0,0]:4d}  |  False Positives: {cm_baseline[0,1]:4d}\n",
        "  False Negatives: {cm_baseline[1,0]:4d}  |  True Positives:  {cm_baseline[1,1]:4d}\n",
        "\n",
        "Oversampling:\n",
        "  True Negatives:  {cm_ros[0,0]:4d}  |  False Positives: {cm_ros[0,1]:4d}\n",
        "  False Negatives: {cm_ros[1,0]:4d}  |  True Positives:  {cm_ros[1,1]:4d}\n",
        "\n",
        "Undersampling:\n",
        "  True Negatives:  {cm_rus[0,0]:4d}  |  False Positives: {cm_rus[0,1]:4d}\n",
        "  False Negatives: {cm_rus[1,0]:4d}  |  True Positives:  {cm_rus[1,1]:4d}\n",
        "\n",
        "Key: Lower False Negatives = Better fraud detection\n",
        "\"\"\"\n",
        "axes[1, 0].text(0.1, 0.5, fig_text, fontsize=10, family='monospace',\n",
        "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# ROC Curves - All methods\n",
        "fpr_rus, tpr_rus, _ = roc_curve(y_test, y_pred_proba_rus)\n",
        "axes[1, 1].plot(fpr, tpr, color='red', linewidth=2, label=f'Baseline (AUC={roc_auc_baseline:.3f})')\n",
        "axes[1, 1].plot(fpr_ros, tpr_ros, color='blue', linewidth=2, label=f'Oversampling (AUC={roc_auc_ros:.3f})')\n",
        "axes[1, 1].plot(fpr_rus, tpr_rus, color='green', linewidth=2, label=f'Undersampling (AUC={roc_auc_rus:.3f})')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', label='Random')\n",
        "axes[1, 1].set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "axes[1, 1].set_title('ROC Curves: All Methods', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"üìä COMPARISON SUMMARY:\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Metric':<15} {'Baseline':<12} {'Oversampling':<15} {'Undersampling':<15}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"{'Accuracy':<15} {accuracy_baseline:<12.4f} {accuracy_ros:<15.4f} {accuracy_rus:<15.4f}\")\n",
        "print(f\"{'Precision':<15} {precision_baseline:<12.4f} {precision_ros:<15.4f} {precision_rus:<15.4f}\")\n",
        "print(f\"{'Recall':<15} {recall_baseline:<12.4f} {recall_ros:<15.4f} {recall_rus:<15.4f}\")\n",
        "print(f\"{'F1-Score':<15} {f1_baseline:<12.4f} {f1_ros:<15.4f} {f1_rus:<15.4f}\")\n",
        "print(f\"{'ROC-AUC':<15} {roc_auc_baseline:<12.4f} {roc_auc_ros:<15.4f} {roc_auc_rus:<15.4f}\")\n",
        "print(f\"{'Training Size':<15} {len(y_train):<12,} {len(y_train_ros):<15,} {len(y_train_rus):<15,}\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de3e032",
      "metadata": {},
      "source": [
        "## Summary and Best Practices\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Imbalanced data is common** in real-world ML problems\n",
        "2. **Accuracy is misleading** - use precision, recall, F1-score, ROC-AUC instead\n",
        "3. **Both methods improve minority class detection** significantly\n",
        "4. **Choose based on your constraints**:\n",
        "   - **Oversampling**: When you have small dataset or can afford larger training time\n",
        "   - **Undersampling**: When you have large dataset and need faster training\n",
        "\n",
        "### Decision Framework\n",
        "\n",
        "| Factor | Favor Oversampling | Favor Undersampling |\n",
        "|--------|-------------------|---------------------|\n",
        "| Dataset Size | Small (<10K) | Large (>100K) |\n",
        "| Minority Class Diversity | High | Low |\n",
        "| Computational Resources | Available | Limited |\n",
        "| Training Time | Can afford longer | Need fast training |\n",
        "| Information Loss Tolerance | Low | High |\n",
        "| Overfitting Risk | Acceptable | Want to minimize |\n",
        "\n",
        "### Oversampling vs Undersampling: Quick Comparison\n",
        "\n",
        "**Oversampling Wins When:**\n",
        "- Dataset is small\n",
        "- Can't afford to lose any information\n",
        "- Minority class has good diversity\n",
        "- Computational resources are available\n",
        "\n",
        "**Undersampling Wins When:**\n",
        "- Dataset is very large\n",
        "- Training time is critical\n",
        "- Memory is limited\n",
        "- Majority class has redundant samples\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "‚úÖ **DO:**\n",
        "- Always split data BEFORE resampling (avoid data leakage!)\n",
        "- Use stratified split to maintain class proportions\n",
        "- Evaluate with multiple metrics (precision, recall, F1, ROC-AUC)\n",
        "- Try both oversampling and undersampling\n",
        "- Consider advanced techniques (SMOTE, Tomek Links, etc.)\n",
        "- Use cross-validation for robust evaluation\n",
        "- Monitor for overfitting\n",
        "\n",
        "‚ùå **DON'T:**\n",
        "- Resample before train-test split (major data leakage!)\n",
        "- Rely only on accuracy\n",
        "- Ignore the confusion matrix\n",
        "- Apply resampling to test set\n",
        "- Forget about class weights as an alternative\n",
        "- Ignore domain context and costs\n",
        "\n",
        "### Alternative Approaches\n",
        "\n",
        "Beyond random resampling:\n",
        "1. **SMOTE** (Synthetic Minority Over-sampling Technique) - Creates synthetic examples\n",
        "2. **Tomek Links** - Removes borderline majority samples\n",
        "3. **Class Weights** - Penalize model more for misclassifying minority class\n",
        "4. **Ensemble Methods** - Use algorithms like balanced Random Forest\n",
        "5. **Anomaly Detection** - Treat minority class as anomalies\n",
        "6. **Cost-Sensitive Learning** - Assign different costs to different errors\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "```\n",
        "Small Dataset + High Imbalance ‚Üí Oversampling (or SMOTE)\n",
        "Large Dataset + Moderate Imbalance ‚Üí Undersampling\n",
        "Very High Imbalance (>99:1) ‚Üí Anomaly Detection\n",
        "Real-time Prediction Needed ‚Üí Undersampling\n",
        "High Cost of False Negatives ‚Üí Oversampling + Tuned Threshold\n",
        "```\n",
        "\n",
        "### Real-World Tips\n",
        "\n",
        "1. **Start Simple**: Try random oversampling/undersampling first\n",
        "2. **Understand Costs**: What's more costly - false positive or false negative?\n",
        "3. **Domain Knowledge**: Use it to guide your choice\n",
        "4. **Iterate**: Try multiple approaches and compare\n",
        "5. **Production Monitoring**: Track performance on new data\n",
        "6. **Threshold Tuning**: Adjust prediction threshold based on business needs\n",
        "\n",
        "---\n",
        "\n",
        "**Next Step**: Learn about SMOTE, a more sophisticated oversampling technique that creates synthetic samples instead of duplicating!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
