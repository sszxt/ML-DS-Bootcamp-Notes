{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b5ee2cde",
      "metadata": {},
      "source": [
        "# Web Scraping with Multithreading\n",
        "\n",
        "Web scraping is a perfect use case for multithreading because it's I/O-bound (waiting for network responses). Using threads can significantly speed up scraping multiple URLs.\n",
        "\n",
        "## What We'll Learn\n",
        "\n",
        "1. Sequential vs Concurrent Web Scraping\n",
        "2. Using ThreadPoolExecutor for Scraping\n",
        "3. Practical Example with Multiple URLs\n",
        "4. Error Handling in Concurrent Scraping\n",
        "5. Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc140774",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Sequential Web Scraping (Slow)\n",
        "\n",
        "Traditional approach - fetching URLs one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e5eccc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Note: This example doesn't require actual web requests\n",
        "# We simulate HTTP requests to demonstrate the concept\n",
        "\n",
        "def fetch_url_sequential(url):\n",
        "    \"\"\"Simulate fetching a URL (replace with actual requests.get(url))\"\"\"\n",
        "    print(f\"Fetching: {url}\")\n",
        "    time.sleep(1)  # Simulate network delay\n",
        "    return f\"Content from {url}\"\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\n",
        "    \"https://example.com/page1\",\n",
        "    \"https://example.com/page2\",\n",
        "    \"https://example.com/page3\",\n",
        "    \"https://example.com/page4\",\n",
        "    \"https://example.com/page5\",\n",
        "]\n",
        "\n",
        "# Sequential scraping\n",
        "print(\"=== Sequential Scraping ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "results = []\n",
        "for url in urls:\n",
        "    result = fetch_url_sequential(url)\n",
        "    results.append(result)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nTime taken: {elapsed:.2f} seconds\")\n",
        "print(f\"URLs scraped: {len(results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298a484c",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Concurrent Web Scraping with ThreadPoolExecutor (Fast)\n",
        "\n",
        "Using multithreading to fetch multiple URLs simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabd2c0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "def fetch_url_concurrent(url):\n",
        "    \"\"\"Simulate fetching a URL with threading\"\"\"\n",
        "    print(f\"Fetching: {url}\")\n",
        "    time.sleep(1)  # Simulate network delay\n",
        "    return f\"Content from {url}\"\n",
        "\n",
        "urls = [\n",
        "    \"https://example.com/page1\",\n",
        "    \"https://example.com/page2\",\n",
        "    \"https://example.com/page3\",\n",
        "    \"https://example.com/page4\",\n",
        "    \"https://example.com/page5\",\n",
        "]\n",
        "\n",
        "# Concurrent scraping with ThreadPoolExecutor\n",
        "print(\"=== Concurrent Scraping with ThreadPoolExecutor ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    # Submit all tasks\n",
        "    future_to_url = {executor.submit(fetch_url_concurrent, url): url for url in urls}\n",
        "    \n",
        "    # Collect results as they complete\n",
        "    results = []\n",
        "    for future in as_completed(future_to_url):\n",
        "        url = future_to_url[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            results.append(result)\n",
        "            print(f\"✓ Completed: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error fetching {url}: {e}\")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nTime taken: {elapsed:.2f} seconds\")\n",
        "print(f\"URLs scraped: {len(results)}\")\n",
        "print(f\"Speed improvement: ~{5/elapsed:.1f}x faster!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75757b23",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1. **Web Scraping is I/O-Bound**: Perfect for multithreading\n",
        "2. **Speed Improvement**: Can be 5-10x faster with threads\n",
        "3. **ThreadPoolExecutor**: Simplest approach for concurrent scraping\n",
        "4. **Error Handling**: Always handle exceptions for individual URLs\n",
        "5. **Respectful Scraping**:\n",
        "   - Don't overwhelm servers (limit max_workers)\n",
        "   - Add delays if needed\n",
        "   - Respect robots.txt\n",
        "   - Use User-Agent headers\n",
        "\n",
        "**Real-World Usage:**\n",
        "```python\n",
        "# With actual requests library:\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def scrape_url(url):\n",
        "    response = requests.get(url)\n",
        "    # Parse with BeautifulSoup, extract data, etc.\n",
        "    return response.text\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    results = executor.map(scrape_url, urls)\n",
        "```\n",
        "\n",
        "**Benefits:**\n",
        "- Dramatically faster for multiple URLs\n",
        "- Simple to implement\n",
        "- Handles errors gracefully\n",
        "- Scales well for I/O-bound operations\n",
        "\n",
        "Multithreading makes web scraping practical and efficient!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
