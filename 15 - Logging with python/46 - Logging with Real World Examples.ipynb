{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a710f5de",
      "metadata": {},
      "source": [
        "# Logging with Real-World Examples\n",
        "\n",
        "This notebook demonstrates practical logging implementations for common real-world scenarios. We'll explore best practices and patterns used in production applications.\n",
        "\n",
        "## What We'll Cover\n",
        "\n",
        "1. Rotating File Handlers (for long-running applications)\n",
        "2. Logging in Web Applications\n",
        "3. Logging in Data Processing Pipelines\n",
        "4. Structured Logging (JSON format)\n",
        "5. Logging Best Practices\n",
        "6. Common Pitfalls to Avoid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3837c4",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Rotating File Handlers\n",
        "\n",
        "For applications that run continuously, log files can grow very large. Rotating file handlers automatically create new log files based on size or time.\n",
        "\n",
        "### RotatingFileHandler - Size-Based Rotation\n",
        "\n",
        "Rotates log files when they reach a certain size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1954d21",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "\n",
        "# Create logger\n",
        "logger = logging.getLogger('app')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create rotating file handler\n",
        "# maxBytes: maximum file size before rotation (1 MB = 1024*1024 bytes)\n",
        "# backupCount: number of backup files to keep\n",
        "handler = RotatingFileHandler(\n",
        "    'app.log',\n",
        "    maxBytes=1024*1024,  # 1 MB\n",
        "    backupCount=5  # Keep 5 backup files (app.log.1, app.log.2, etc.)\n",
        ")\n",
        "\n",
        "# Set format\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "# Generate logs\n",
        "for i in range(100):\n",
        "    logger.info(f\"Log message number {i}\")\n",
        "    logger.debug(f\"Debug info for iteration {i}\")\n",
        "\n",
        "print(\"Check app.log file. When it reaches 1MB, it will rotate to app.log.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29446f4",
      "metadata": {},
      "source": [
        "### TimedRotatingFileHandler - Time-Based Rotation\n",
        "\n",
        "Rotates log files at certain time intervals (hourly, daily, weekly, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679797bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from logging.handlers import TimedRotatingFileHandler\n",
        "import time\n",
        "\n",
        "# Create logger\n",
        "logger = logging.getLogger('timed_app')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create timed rotating handler\n",
        "# when: 'S' = seconds, 'M' = minutes, 'H' = hours, 'D' = days, 'midnight' = daily at midnight\n",
        "# interval: how many units to wait before rotating\n",
        "# backupCount: number of old log files to keep\n",
        "handler = TimedRotatingFileHandler(\n",
        "    'timed_app.log',\n",
        "    when='midnight',  # Rotate daily at midnight\n",
        "    interval=1,\n",
        "    backupCount=7  # Keep 7 days of logs\n",
        ")\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "# Log some messages\n",
        "logger.info(\"Application started\")\n",
        "logger.warning(\"This is a warning\")\n",
        "logger.error(\"This is an error\")\n",
        "\n",
        "print(\"Log file will rotate daily at midnight. Old logs: timed_app.log.YYYY-MM-DD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f585519",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Logging in Web Applications\n",
        "\n",
        "Example of logging in a Flask-like web application with request tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4fd0c47",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging for web app\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - [%(request_id)s] - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "class WebApp:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger('webapp')\n",
        "    \n",
        "    def handle_request(self, method, endpoint, user_id=None):\n",
        "        # Generate unique request ID for tracking\n",
        "        request_id = str(uuid.uuid4())[:8]\n",
        "        \n",
        "        # Create custom logger adapter to include request ID\n",
        "        logger_adapter = logging.LoggerAdapter(\n",
        "            self.logger, \n",
        "            {'request_id': request_id}\n",
        "        )\n",
        "        \n",
        "        logger_adapter.info(f\"{method} {endpoint} - User: {user_id}\")\n",
        "        \n",
        "        # Simulate processing\n",
        "        try:\n",
        "            # Simulate some work\n",
        "            if endpoint == \"/api/users\":\n",
        "                logger_adapter.debug(\"Fetching users from database\")\n",
        "                logger_adapter.info(\"Users retrieved successfully\")\n",
        "            elif endpoint == \"/api/error\":\n",
        "                raise ValueError(\"Simulated error\")\n",
        "        except Exception as e:\n",
        "            logger_adapter.error(f\"Error processing request: {e}\")\n",
        "        \n",
        "        logger_adapter.info(f\"Request completed - {endpoint}\")\n",
        "\n",
        "# Test web app logging\n",
        "app = WebApp()\n",
        "app.handle_request(\"GET\", \"/api/users\", user_id=123)\n",
        "app.handle_request(\"POST\", \"/api/orders\", user_id=456)\n",
        "app.handle_request(\"GET\", \"/api/error\", user_id=789)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe14cfe",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Logging in Data Processing Pipeline\n",
        "\n",
        "Example of logging in a data processing/ETL pipeline with progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b515cc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('pipeline.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "class DataPipeline:\n",
        "    def __init__(self, name):\n",
        "        self.logger = logging.getLogger(f'pipeline.{name}')\n",
        "        self.name = name\n",
        "    \n",
        "    def extract(self, source):\n",
        "        \"\"\"Extract data from source\"\"\"\n",
        "        self.logger.info(f\"Starting extraction from {source}\")\n",
        "        try:\n",
        "            # Simulate extraction\n",
        "            time.sleep(0.1)\n",
        "            self.logger.info(f\"Successfully extracted data from {source}\")\n",
        "            return [\"data1\", \"data2\", \"data3\"]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Extraction failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def transform(self, data):\n",
        "        \"\"\"Transform data\"\"\"\n",
        "        self.logger.info(f\"Starting transformation of {len(data)} records\")\n",
        "        try:\n",
        "            # Simulate transformation\n",
        "            transformed = [d.upper() for d in data]\n",
        "            self.logger.debug(f\"Transformation details: {data} -> {transformed}\")\n",
        "            self.logger.info(\"Transformation completed successfully\")\n",
        "            return transformed\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Transformation failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def load(self, data, destination):\n",
        "        \"\"\"Load data to destination\"\"\"\n",
        "        self.logger.info(f\"Loading {len(data)} records to {destination}\")\n",
        "        try:\n",
        "            # Simulate loading\n",
        "            time.sleep(0.1)\n",
        "            self.logger.info(f\"Successfully loaded data to {destination}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Loading failed: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def run(self, source, destination):\n",
        "        \"\"\"Run the complete ETL pipeline\"\"\"\n",
        "        self.logger.info(f\"===== Pipeline {self.name} Started =====\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            data = self.extract(source)\n",
        "            transformed_data = self.transform(data)\n",
        "            self.load(transformed_data, destination)\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            self.logger.info(f\"===== Pipeline {self.name} Completed in {elapsed:.2f}s =====\")\n",
        "        except Exception as e:\n",
        "            self.logger.critical(f\"Pipeline failed: {e}\")\n",
        "            raise\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline = DataPipeline(\"user_data_etl\")\n",
        "pipeline.run(\"database_source\", \"data_warehouse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d34de98b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Logging Best Practices\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "1. **Use Appropriate Log Levels**\n",
        "   - DEBUG: Detailed diagnostic information (development only)\n",
        "   - INFO: Confirmation things are working (milestones, checkpoints)\n",
        "   - WARNING: Something unexpected but not critical\n",
        "   - ERROR: Serious problem, but application can continue\n",
        "   - CRITICAL: Fatal error, application may not continue\n",
        "\n",
        "2. **Include Contextual Information**\n",
        "   - User IDs, request IDs, session IDs\n",
        "   - Timestamps (automatic with proper formatting)\n",
        "   - Function/module names\n",
        "   - Error details and stack traces\n",
        "\n",
        "3. **Don't Log Sensitive Data**\n",
        "   - Never log passwords, API keys, credit card numbers\n",
        "   - Be careful with personally identifiable information (PII)\n",
        "   - Mask sensitive data if necessary\n",
        "\n",
        "4. **Use Structured Logging**\n",
        "   - Makes logs easier to parse and analyze\n",
        "   - Better for log aggregation tools\n",
        "   - Enables programmatic log processing\n",
        "\n",
        "5. **Configure Logging Early**\n",
        "   - Set up logging at application startup\n",
        "   - Use configuration files for complex setups\n",
        "   - Don't call `basicConfig()` multiple times\n",
        "\n",
        "6. **Use Loggers, Not Root**\n",
        "   - Create named loggers: `logging.getLogger(__name__)`\n",
        "   - Avoid using `logging.info()` directly (uses root logger)\n",
        "   - Better control and organization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6dc9e9a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1. **Rotating Handlers**: Use for production applications to manage log file sizes\n",
        "   - `RotatingFileHandler`: Size-based rotation\n",
        "   - `TimedRotatingFileHandler`: Time-based rotation\n",
        "\n",
        "2. **Web Application Logging**: Include request IDs for tracking user journeys\n",
        "\n",
        "3. **Data Pipeline Logging**: Log each stage (extract, transform, load) with timing info\n",
        "\n",
        "4. **Best Practices**:\n",
        "   - Use appropriate log levels\n",
        "   - Include context (IDs, timestamps)\n",
        "   - Never log sensitive data\n",
        "   - Configure logging early\n",
        "   - Use named loggers\n",
        "\n",
        "5. **Common Patterns**:\n",
        "   - Multiple handlers (console + file)\n",
        "   - Different log levels for development vs production\n",
        "   - Structured logging for analysis\n",
        "   - Request/transaction tracking with unique IDs\n",
        "\n",
        "**Next Steps:**\n",
        "- Implement logging in your projects\n",
        "- Explore log aggregation tools (ELK stack, Splunk)\n",
        "- Learn about distributed tracing for microservices"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
