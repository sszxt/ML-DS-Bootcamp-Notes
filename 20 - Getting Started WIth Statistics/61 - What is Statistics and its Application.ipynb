{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Statistics and its Application\n",
    "\n",
    "Statistics is the science of collecting, organizing, analyzing, interpreting, and presenting data. It provides powerful tools and methods to extract meaningful insights from data, make informed decisions, and understand patterns in the world around us. In the context of machine learning and data science, statistics forms the foundational framework that enables us to build models, validate predictions, and draw conclusions from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Definition and Core Concepts](#definition)\n",
    "2. [Purpose and Importance of Statistics](#purpose)\n",
    "3. [Key Terminology](#terminology)\n",
    "4. [Real-World Applications](#applications)\n",
    "5. [Basic Statistical Operations with Python](#python-basics)\n",
    "6. [Practical Examples with Datasets](#practical-examples)\n",
    "7. [Visualizing Statistical Data](#visualization)\n",
    "8. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Definition and Core Concepts <a id='definition'></a>\n",
    "\n",
    "### What is Statistics?\n",
    "\n",
    "Statistics is a branch of mathematics that deals with:\n",
    "\n",
    "- **Collection**: Gathering data through surveys, experiments, or observations\n",
    "- **Organization**: Structuring data in meaningful ways (tables, databases)\n",
    "- **Analysis**: Examining data to identify patterns and relationships\n",
    "- **Interpretation**: Drawing conclusions from analyzed data\n",
    "- **Presentation**: Communicating findings through graphs, charts, and reports\n",
    "\n",
    "### Two Main Branches\n",
    "\n",
    "1. **Descriptive Statistics**: Summarizes and describes the main features of a dataset\n",
    "   - Measures of central tendency (mean, median, mode)\n",
    "   - Measures of dispersion (variance, standard deviation, range)\n",
    "   - Data visualization (histograms, box plots, scatter plots)\n",
    "\n",
    "2. **Inferential Statistics**: Makes predictions and inferences about a population based on sample data\n",
    "   - Hypothesis testing\n",
    "   - Confidence intervals\n",
    "   - Regression analysis\n",
    "   - Probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Import essential libraries for statistical analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2. Purpose and Importance of Statistics <a id='purpose'></a>\n\n### Why Statistics Matters\n\nStatistics plays a crucial role in modern society and data science:\n\n1. **Data-Driven Decision Making**\n   - Helps businesses make informed choices based on evidence\n   - Reduces uncertainty and risk in decision-making processes\n\n2. **Pattern Recognition**\n   - Identifies trends and patterns in complex datasets\n   - Reveals hidden relationships between variables\n\n3. **Prediction and Forecasting**\n   - Enables predictions about future events or outcomes\n   - Forms the basis for machine learning algorithms\n\n4. **Quality Control**\n   - Monitors and improves product and service quality\n   - Detects anomalies and outliers in processes\n\n5. **Scientific Research**\n   - Validates hypotheses and theories\n   - Determines the significance of research findings\n\n6. **Resource Optimization**\n   - Helps allocate resources efficiently\n   - Identifies areas for improvement and cost reduction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Demonstrating the importance of statistics in decision-making\n# Scenario: Two marketing campaigns with different performance metrics\n\n# Campaign A: Daily conversions over 30 days\ncampaign_a = np.array([45, 48, 52, 47, 50, 49, 51, 53, 48, 50,\n                       52, 51, 49, 50, 48, 52, 51, 50, 49, 53,\n                       50, 51, 52, 48, 50, 49, 51, 52, 50, 51])\n\n# Campaign B: Daily conversions over 30 days\ncampaign_b = np.array([38, 62, 45, 55, 42, 58, 48, 52, 40, 60,\n                       44, 56, 46, 54, 43, 57, 47, 53, 41, 59,\n                       45, 55, 44, 56, 46, 54, 42, 58, 48, 52])\n\n# Calculate statistics for both campaigns\nprint(\"Campaign A Statistics:\")\nprint(f\"  Mean (Average): {np.mean(campaign_a):.2f} conversions/day\")\nprint(f\"  Standard Deviation: {np.std(campaign_a):.2f}\")\nprint(f\"  Total Conversions: {np.sum(campaign_a)}\")\n\nprint(\"\\nCampaign B Statistics:\")\nprint(f\"  Mean (Average): {np.mean(campaign_b):.2f} conversions/day\")\nprint(f\"  Standard Deviation: {np.std(campaign_b):.2f}\")\nprint(f\"  Total Conversions: {np.sum(campaign_b)}\")\n\nprint(\"\\nInsight:\")\nprint(\"Both campaigns have similar average conversions, but Campaign A is more\")\nprint(\"consistent (lower std dev), while Campaign B is more volatile.\")\nprint(\"Statistics helps us understand not just the average, but the reliability!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3. Key Terminology <a id='terminology'></a>\n\nUnderstanding these fundamental terms is essential for working with statistics:\n\n### Data-Related Terms\n\n| Term | Definition | Example |\n|------|------------|----------|\n| **Population** | The entire group of individuals or items of interest | All customers of a company |\n| **Sample** | A subset of the population selected for analysis | 1,000 randomly selected customers |\n| **Variable** | A characteristic that can be measured or observed | Age, income, temperature |\n| **Observation** | A single measurement or data point | One customer's age |\n| **Dataset** | A collection of related observations | Customer database |\n\n### Statistical Measures\n\n| Term | Definition | Purpose |\n|------|------------|----------|\n| **Mean** | The arithmetic average of values | Central tendency |\n| **Median** | The middle value when data is sorted | Central tendency (robust to outliers) |\n| **Mode** | The most frequently occurring value | Identify common values |\n| **Variance** | Average squared deviation from mean | Measure of spread |\n| **Standard Deviation** | Square root of variance | Measure of spread (same units as data) |\n| **Correlation** | Measure of relationship between variables | Identify associations |\n\n### Types of Data\n\n1. **Quantitative (Numerical)**\n   - **Continuous**: Can take any value in a range (height, weight, temperature)\n   - **Discrete**: Can only take specific values (number of children, count of items)\n\n2. **Qualitative (Categorical)**\n   - **Nominal**: Categories without order (gender, color, country)\n   - **Ordinal**: Categories with a meaningful order (ratings, education level)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrating different types of data and basic terminology\n\n# Create a sample dataset representing student information\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma', 'Frank', 'Grace', 'Henry'],\n    'Age': [22, 25, 23, 22, 24, 25, 23, 22],  # Discrete quantitative\n    'GPA': [3.8, 3.2, 3.9, 3.5, 3.7, 3.3, 3.6, 3.8],  # Continuous quantitative\n    'Major': ['CS', 'Math', 'CS', 'Physics', 'CS', 'Math', 'Physics', 'CS'],  # Nominal categorical\n    'Year': ['Senior', 'Junior', 'Senior', 'Sophomore', 'Junior', 'Senior', 'Junior', 'Senior']  # Ordinal categorical\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"Sample Dataset (Population of 8 students):\")\nprint(df)\nprint(\"\\n\" + \"=\"*60)\n\n# Demonstrate statistical calculations\nprint(\"\\nStatistical Measures for GPA:\")\nprint(f\"Mean (Average GPA): {df['GPA'].mean():.2f}\")\nprint(f\"Median (Middle GPA): {df['GPA'].median():.2f}\")\nprint(f\"Mode (Most common GPA): {df['GPA'].mode().values[0]:.1f}\")\nprint(f\"Standard Deviation: {df['GPA'].std():.3f}\")\nprint(f\"Variance: {df['GPA'].var():.3f}\")\n\nprint(\"\\nData Types in our Dataset:\")\nprint(f\"Discrete Quantitative: Age (values: {sorted(df['Age'].unique())})\")\nprint(f\"Continuous Quantitative: GPA (range: {df['GPA'].min()} - {df['GPA'].max()})\")\nprint(f\"Nominal Categorical: Major (categories: {df['Major'].unique().tolist()})\")\nprint(f\"Ordinal Categorical: Year (categories: {df['Year'].unique().tolist()})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Real-World Applications <a id='applications'></a>\n\nStatistics is applied across virtually every industry and domain. Here are key applications:\n\n### Business and Economics\n\n- **Market Research**: Understanding customer preferences and behavior\n- **Financial Analysis**: Risk assessment, portfolio optimization, stock market prediction\n- **Quality Control**: Manufacturing defect detection and process improvement\n- **Sales Forecasting**: Predicting future sales trends and inventory needs\n- **A/B Testing**: Comparing different versions of products or marketing campaigns\n\n### Healthcare and Medicine\n\n- **Clinical Trials**: Testing the effectiveness of new drugs and treatments\n- **Disease Outbreak Prediction**: Modeling the spread of infectious diseases\n- **Patient Diagnosis**: Using statistical models to identify diseases\n- **Healthcare Analytics**: Optimizing hospital operations and patient care\n- **Genomics**: Analyzing genetic data to understand diseases\n\n### Sports Analytics\n\n- **Player Performance**: Evaluating and comparing athlete statistics\n- **Team Strategy**: Data-driven game planning and player selection\n- **Injury Prevention**: Predicting injury risks based on workload data\n- **Fan Engagement**: Understanding viewer preferences and behavior\n\n### Technology and Data Science\n\n- **Machine Learning**: Training algorithms on data patterns\n- **Recommendation Systems**: Suggesting products, movies, or content\n- **Natural Language Processing**: Analyzing text and speech data\n- **Computer Vision**: Image recognition and classification\n- **Anomaly Detection**: Identifying unusual patterns in cybersecurity\n\n### Social Sciences\n\n- **Survey Analysis**: Understanding public opinion and demographics\n- **Psychology Research**: Testing behavioral theories and interventions\n- **Education**: Measuring student performance and learning outcomes\n- **Political Polling**: Predicting election outcomes\n\n### Environmental Science\n\n- **Climate Modeling**: Predicting weather patterns and climate change\n- **Environmental Monitoring**: Tracking pollution levels and biodiversity\n- **Resource Management**: Optimizing water, energy, and land use",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Real-world application example: Sales forecasting\n\n# Simulate monthly sales data for a retail store\nnp.random.seed(42)\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n# Sales with seasonal trend (higher in holiday months)\nbase_sales = 50000\ntrend = np.linspace(0, 15000, 12)  # Upward trend\nseasonal = np.array([0, -5000, 0, 2000, 3000, 1000, \n                     2000, 1000, 0, 5000, 8000, 12000])  # Holiday boost\nnoise = np.random.normal(0, 2000, 12)  # Random variation\n\nsales = base_sales + trend + seasonal + noise\n\nsales_df = pd.DataFrame({\n    'Month': months,\n    'Sales': sales\n})\n\nprint(\"Monthly Sales Data:\")\nprint(sales_df)\nprint(\"\\n\" + \"=\"*60)\n\n# Statistical analysis\nprint(\"\\nSales Statistics:\")\nprint(f\"Average Monthly Sales: ${sales_df['Sales'].mean():,.2f}\")\nprint(f\"Median Monthly Sales: ${sales_df['Sales'].median():,.2f}\")\nprint(f\"Standard Deviation: ${sales_df['Sales'].std():,.2f}\")\nprint(f\"Best Month: {sales_df.loc[sales_df['Sales'].idxmax(), 'Month']} (${sales_df['Sales'].max():,.2f})\")\nprint(f\"Worst Month: {sales_df.loc[sales_df['Sales'].idxmin(), 'Month']} (${sales_df['Sales'].min():,.2f})\")\n\n# Growth rate\ntotal_h1 = sales_df.iloc[:6]['Sales'].sum()  # First half\ntotal_h2 = sales_df.iloc[6:]['Sales'].sum()  # Second half\ngrowth_rate = ((total_h2 - total_h1) / total_h1) * 100\n\nprint(f\"\\nH1 Total Sales: ${total_h1:,.2f}\")\nprint(f\"H2 Total Sales: ${total_h2:,.2f}\")\nprint(f\"Growth Rate (H1 to H2): {growth_rate:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Application example: Healthcare - Patient vital signs analysis\n\n# Simulate blood pressure readings for patients\nnp.random.seed(123)\nn_patients = 100\n\n# Generate normal and high blood pressure groups\nnormal_bp = np.random.normal(120, 10, 70)  # 70 patients with normal BP\nhigh_bp = np.random.normal(145, 12, 30)    # 30 patients with high BP\n\nall_bp = np.concatenate([normal_bp, high_bp])\nnp.random.shuffle(all_bp)\n\nbp_df = pd.DataFrame({\n    'Patient_ID': range(1, n_patients + 1),\n    'Systolic_BP': all_bp,\n    'Category': ['High' if bp > 140 else 'Normal' for bp in all_bp]\n})\n\nprint(\"Blood Pressure Analysis:\")\nprint(f\"Total Patients: {len(bp_df)}\")\nprint(f\"\\nAverage Systolic BP: {bp_df['Systolic_BP'].mean():.1f} mmHg\")\nprint(f\"Median Systolic BP: {bp_df['Systolic_BP'].median():.1f} mmHg\")\nprint(f\"Standard Deviation: {bp_df['Systolic_BP'].std():.1f} mmHg\")\n\nprint(\"\\nPatient Categories:\")\nprint(bp_df['Category'].value_counts())\n\nprint(\"\\nPercentage with High BP:\")\nhigh_bp_percentage = (bp_df['Category'] == 'High').sum() / len(bp_df) * 100\nprint(f\"{high_bp_percentage:.1f}%\")\n\n# Show sample of data\nprint(\"\\nSample Patient Data:\")\nprint(bp_df.head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Basic Statistical Operations with Python <a id='python-basics'></a>\n\nPython provides powerful libraries for statistical analysis. Let's explore the most common operations:\n\n### NumPy for Statistical Calculations\n\nNumPy offers fast numerical operations on arrays:\n\n- `np.mean()`: Calculate average\n- `np.median()`: Find middle value\n- `np.std()`: Compute standard deviation\n- `np.var()`: Calculate variance\n- `np.min()`, `np.max()`: Find minimum and maximum\n- `np.percentile()`: Calculate percentiles\n- `np.corrcoef()`: Compute correlation coefficient",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Basic statistical operations using NumPy\n\n# Create a sample dataset: exam scores\nexam_scores = np.array([78, 85, 92, 88, 76, 95, 89, 84, 91, 87, \n                        79, 93, 86, 90, 82, 88, 94, 85, 87, 89])\n\nprint(\"Exam Scores:\", exam_scores)\nprint(\"\\nBasic Statistics:\")\nprint(f\"Mean (Average): {np.mean(exam_scores):.2f}\")\nprint(f\"Median: {np.median(exam_scores):.2f}\")\nprint(f\"Standard Deviation: {np.std(exam_scores):.2f}\")\nprint(f\"Variance: {np.var(exam_scores):.2f}\")\nprint(f\"Minimum Score: {np.min(exam_scores)}\")\nprint(f\"Maximum Score: {np.max(exam_scores)}\")\nprint(f\"Range: {np.ptp(exam_scores)}\")\n\nprint(\"\\nPercentiles:\")\nprint(f\"25th Percentile: {np.percentile(exam_scores, 25):.2f}\")\nprint(f\"50th Percentile (Median): {np.percentile(exam_scores, 50):.2f}\")\nprint(f\"75th Percentile: {np.percentile(exam_scores, 75):.2f}\")\nprint(f\"90th Percentile: {np.percentile(exam_scores, 90):.2f}\")\n\n# Count values above average\nabove_avg = np.sum(exam_scores > np.mean(exam_scores))\nprint(f\"\\nStudents scoring above average: {above_avg} out of {len(exam_scores)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Pandas for Data Analysis\n\nPandas excels at working with structured data:\n\n- `df.describe()`: Get comprehensive statistics summary\n- `df.mean()`, `df.median()`, `df.std()`: Column-wise statistics\n- `df.corr()`: Correlation matrix\n- `df.groupby()`: Group data and calculate statistics\n- `df.value_counts()`: Count unique values",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Statistical operations using Pandas\n\n# Create a more complex dataset: employee information\nnp.random.seed(42)\n\nemployees = pd.DataFrame({\n    'Department': np.random.choice(['Sales', 'IT', 'HR', 'Marketing'], 50),\n    'Years_Experience': np.random.randint(1, 21, 50),\n    'Salary': np.random.randint(40000, 120000, 50),\n    'Performance_Score': np.random.randint(60, 100, 50),\n    'Age': np.random.randint(23, 60, 50)\n})\n\nprint(\"Employee Dataset Sample:\")\nprint(employees.head(10))\nprint(\"\\n\" + \"=\"*80)\n\n# Comprehensive statistical summary\nprint(\"\\nStatistical Summary:\")\nprint(employees.describe())\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nDepartment Distribution:\")\nprint(employees['Department'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Group statistics by department\n\nprint(\"Average Statistics by Department:\")\ndept_stats = employees.groupby('Department').agg({\n    'Salary': ['mean', 'median', 'std'],\n    'Years_Experience': 'mean',\n    'Performance_Score': 'mean',\n    'Age': 'mean'\n}).round(2)\n\nprint(dept_stats)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nCorrelation Analysis:\")\nprint(\"Correlation between numerical variables:\")\ncorrelation_matrix = employees[['Years_Experience', 'Salary', \n                                 'Performance_Score', 'Age']].corr()\nprint(correlation_matrix)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### SciPy for Advanced Statistical Functions\n\nSciPy provides scientific computing capabilities:\n\n- Probability distributions\n- Hypothesis testing (t-tests, chi-square tests)\n- Statistical tests\n- Correlation analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Using SciPy for statistical tests\nfrom scipy import stats\n\n# Compare salaries between two departments\nit_salaries = employees[employees['Department'] == 'IT']['Salary']\nsales_salaries = employees[employees['Department'] == 'Sales']['Salary']\n\nprint(\"Comparing IT vs Sales Salaries:\")\nprint(f\"IT Average Salary: ${it_salaries.mean():,.2f}\")\nprint(f\"Sales Average Salary: ${sales_salaries.mean():,.2f}\")\n\n# Perform independent t-test\nt_statistic, p_value = stats.ttest_ind(it_salaries, sales_salaries)\n\nprint(f\"\\nT-test Results:\")\nprint(f\"T-statistic: {t_statistic:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nif p_value < 0.05:\n    print(\"Result: Statistically significant difference (p < 0.05)\")\nelse:\n    print(\"Result: No statistically significant difference (p >= 0.05)\")\n\n# Calculate Pearson correlation\ncorr_coef, corr_p_value = stats.pearsonr(employees['Years_Experience'], \n                                          employees['Salary'])\n\nprint(f\"\\nCorrelation between Experience and Salary:\")\nprint(f\"Correlation Coefficient: {corr_coef:.4f}\")\nprint(f\"P-value: {corr_p_value:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Practical Examples with Datasets <a id='practical-examples'></a>\n\nLet's apply statistical concepts to real-world scenarios with practical datasets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: E-commerce Product Analysis\n\nnp.random.seed(100)\n\n# Create product sales dataset\nproducts = pd.DataFrame({\n    'Product_ID': range(1, 101),\n    'Category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 100),\n    'Price': np.random.uniform(10, 500, 100).round(2),\n    'Units_Sold': np.random.randint(50, 1000, 100),\n    'Customer_Rating': np.random.uniform(2.5, 5.0, 100).round(1),\n    'Reviews_Count': np.random.randint(5, 200, 100)\n})\n\n# Calculate revenue\nproducts['Revenue'] = products['Price'] * products['Units_Sold']\n\nprint(\"E-commerce Product Dataset:\")\nprint(products.head(10))\nprint(\"\\n\" + \"=\"*80)\n\n# Statistical Analysis\nprint(\"\\nOverall Statistics:\")\nprint(f\"Total Products: {len(products)}\")\nprint(f\"Total Revenue: ${products['Revenue'].sum():,.2f}\")\nprint(f\"Average Price: ${products['Price'].mean():.2f}\")\nprint(f\"Average Rating: {products['Customer_Rating'].mean():.2f}/5.0\")\nprint(f\"Total Units Sold: {products['Units_Sold'].sum():,}\")\n\nprint(\"\\nTop 5 Products by Revenue:\")\ntop_products = products.nlargest(5, 'Revenue')[['Product_ID', 'Category', \n                                                  'Price', 'Units_Sold', 'Revenue']]\nprint(top_products)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Category-wise analysis for e-commerce data\n\nprint(\"Category-wise Performance:\")\ncategory_analysis = products.groupby('Category').agg({\n    'Product_ID': 'count',\n    'Revenue': ['sum', 'mean'],\n    'Customer_Rating': 'mean',\n    'Units_Sold': 'sum',\n    'Price': 'mean'\n}).round(2)\n\ncategory_analysis.columns = ['Product_Count', 'Total_Revenue', 'Avg_Revenue_Per_Product',\n                              'Avg_Rating', 'Total_Units_Sold', 'Avg_Price']\nprint(category_analysis)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nBest Performing Category by Total Revenue:\")\nbest_category = category_analysis['Total_Revenue'].idxmax()\nprint(f\"{best_category}: ${category_analysis.loc[best_category, 'Total_Revenue']:,.2f}\")\n\nprint(\"\\nHighest Rated Category:\")\ntop_rated = category_analysis['Avg_Rating'].idxmax()\nprint(f\"{top_rated}: {category_analysis.loc[top_rated, 'Avg_Rating']:.2f}/5.0\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 2: Student Performance Analysis\n\nnp.random.seed(50)\n\n# Create student dataset\nstudents = pd.DataFrame({\n    'Student_ID': range(1, 201),\n    'Study_Hours': np.random.randint(5, 40, 200),\n    'Attendance_Percentage': np.random.uniform(60, 100, 200).round(1),\n    'Previous_Score': np.random.randint(40, 100, 200),\n    'Final_Score': np.random.randint(45, 100, 200),\n    'Class': np.random.choice(['A', 'B', 'C', 'D'], 200)\n})\n\nprint(\"Student Performance Dataset:\")\nprint(students.head(10))\nprint(\"\\n\" + \"=\"*80)\n\n# Statistical summary\nprint(\"\\nPerformance Statistics:\")\nprint(students[['Study_Hours', 'Attendance_Percentage', \n                'Previous_Score', 'Final_Score']].describe())\n\n# Categorize students by performance\ndef categorize_performance(score):\n    if score >= 90:\n        return 'Excellent'\n    elif score >= 75:\n        return 'Good'\n    elif score >= 60:\n        return 'Average'\n    else:\n        return 'Needs Improvement'\n\nstudents['Performance_Category'] = students['Final_Score'].apply(categorize_performance)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nPerformance Distribution:\")\nprint(students['Performance_Category'].value_counts().sort_index())\nprint(\"\\nPercentage Distribution:\")\nprint((students['Performance_Category'].value_counts(normalize=True) * 100).round(2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Correlation analysis for student data\n\nprint(\"Correlation Analysis - Factors affecting Final Score:\")\nprint(\"\\nCorrelation with Final Score:\")\n\nfactors = ['Study_Hours', 'Attendance_Percentage', 'Previous_Score']\nfor factor in factors:\n    corr = students[factor].corr(students['Final_Score'])\n    print(f\"{factor}: {corr:.4f}\")\n\n# Class-wise performance\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nClass-wise Average Scores:\")\nclass_performance = students.groupby('Class').agg({\n    'Final_Score': ['mean', 'median', 'std', 'min', 'max'],\n    'Study_Hours': 'mean',\n    'Attendance_Percentage': 'mean'\n}).round(2)\nprint(class_performance)\n\n# Find top performers\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nTop 10 Students:\")\ntop_students = students.nlargest(10, 'Final_Score')[['Student_ID', 'Study_Hours', \n                                                       'Attendance_Percentage', \n                                                       'Final_Score', 'Class']]\nprint(top_students)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. Visualizing Statistical Data <a id='visualization'></a>\n\nData visualization is crucial for understanding statistical patterns and communicating insights effectively. Let's explore common statistical visualizations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualization 1: Distribution plots\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Histogram\naxes[0, 0].hist(products['Price'], bins=20, color='skyblue', edgecolor='black')\naxes[0, 0].set_title('Distribution of Product Prices', fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('Price ($)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].axvline(products['Price'].mean(), color='red', \n                    linestyle='--', label=f\"Mean: ${products['Price'].mean():.2f}\")\naxes[0, 0].legend()\n\n# Box Plot\nproducts.boxplot(column='Revenue', by='Category', ax=axes[0, 1])\naxes[0, 1].set_title('Revenue Distribution by Category', fontsize=12, fontweight='bold')\naxes[0, 1].set_xlabel('Category')\naxes[0, 1].set_ylabel('Revenue ($)')\nplt.sca(axes[0, 1])\nplt.xticks(rotation=45)\n\n# Scatter Plot\naxes[1, 0].scatter(students['Study_Hours'], students['Final_Score'], \n                   alpha=0.6, color='green')\naxes[1, 0].set_title('Study Hours vs Final Score', fontsize=12, fontweight='bold')\naxes[1, 0].set_xlabel('Study Hours per Week')\naxes[1, 0].set_ylabel('Final Score')\n\n# Add trend line\nz = np.polyfit(students['Study_Hours'], students['Final_Score'], 1)\np = np.poly1d(z)\naxes[1, 0].plot(students['Study_Hours'], p(students['Study_Hours']), \n                \"r--\", alpha=0.8, label='Trend Line')\naxes[1, 0].legend()\n\n# Bar Chart\ncategory_revenue = products.groupby('Category')['Revenue'].sum().sort_values(ascending=False)\naxes[1, 1].bar(category_revenue.index, category_revenue.values, color='coral')\naxes[1, 1].set_title('Total Revenue by Category', fontsize=12, fontweight='bold')\naxes[1, 1].set_xlabel('Category')\naxes[1, 1].set_ylabel('Total Revenue ($)')\naxes[1, 1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualizations created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization 2: Advanced statistical plots using Seaborn\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Violin Plot\nsns.violinplot(data=products, x='Category', y='Customer_Rating', ax=axes[0, 0])\naxes[0, 0].set_title('Customer Rating Distribution by Category', \n                      fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('Category')\naxes[0, 0].set_ylabel('Rating')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Heatmap - Correlation Matrix\ncorr_data = students[['Study_Hours', 'Attendance_Percentage', \n                      'Previous_Score', 'Final_Score']].corr()\nsns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', ax=axes[0, 1], cbar_kws={'label': 'Correlation'})\naxes[0, 1].set_title('Correlation Heatmap - Student Performance', \n                      fontsize=12, fontweight='bold')\n\n# Count Plot\nsns.countplot(data=students, x='Performance_Category', \n              order=['Excellent', 'Good', 'Average', 'Needs Improvement'],\n              palette='viridis', ax=axes[1, 0])\naxes[1, 0].set_title('Student Performance Category Distribution', \n                      fontsize=12, fontweight='bold')\naxes[1, 0].set_xlabel('Performance Category')\naxes[1, 0].set_ylabel('Count')\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# KDE Plot (Kernel Density Estimate)\nfor category in products['Category'].unique():\n    subset = products[products['Category'] == category]['Price']\n    sns.kdeplot(data=subset, label=category, ax=axes[1, 1], fill=True, alpha=0.3)\naxes[1, 1].set_title('Price Distribution Density by Category', \n                      fontsize=12, fontweight='bold')\naxes[1, 1].set_xlabel('Price ($)')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Advanced visualizations created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization 3: Time series and trends\n\n# Create time series data for website traffic\nnp.random.seed(42)\ndates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\nbase_traffic = 5000\ntrend = np.linspace(0, 3000, len(dates))\nseasonal = 1000 * np.sin(2 * np.pi * np.arange(len(dates)) / 365)\nnoise = np.random.normal(0, 300, len(dates))\ntraffic = base_traffic + trend + seasonal + noise\n\ntraffic_df = pd.DataFrame({\n    'Date': dates,\n    'Visitors': traffic.astype(int)\n})\n\n# Create visualization\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\n# Line plot with trend\naxes[0].plot(traffic_df['Date'], traffic_df['Visitors'], \n             linewidth=1, alpha=0.7, label='Daily Visitors')\naxes[0].plot(traffic_df['Date'], traffic_df['Visitors'].rolling(30).mean(), \n             linewidth=2, color='red', label='30-Day Moving Average')\naxes[0].set_title('Website Traffic Over Time', fontsize=12, fontweight='bold')\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Number of Visitors')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Monthly aggregation\ntraffic_df['Month'] = traffic_df['Date'].dt.to_period('M')\nmonthly_traffic = traffic_df.groupby('Month')['Visitors'].agg(['mean', 'std'])\nmonthly_traffic.index = monthly_traffic.index.to_timestamp()\n\naxes[1].bar(monthly_traffic.index, monthly_traffic['mean'], \n            width=20, color='steelblue', alpha=0.7, label='Average Monthly Visitors')\naxes[1].errorbar(monthly_traffic.index, monthly_traffic['mean'], \n                 yerr=monthly_traffic['std'], fmt='none', \n                 color='black', alpha=0.5, capsize=5, label='Std Deviation')\naxes[1].set_title('Monthly Average Traffic with Variability', \n                   fontsize=12, fontweight='bold')\naxes[1].set_xlabel('Month')\naxes[1].set_ylabel('Average Visitors')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTraffic Statistics:\")\nprint(f\"Average Daily Visitors: {traffic_df['Visitors'].mean():.0f}\")\nprint(f\"Peak Day: {traffic_df.loc[traffic_df['Visitors'].idxmax(), 'Date'].date()} \"\n      f\"({traffic_df['Visitors'].max():,} visitors)\")\nprint(f\"Lowest Day: {traffic_df.loc[traffic_df['Visitors'].idxmin(), 'Date'].date()} \"\n      f\"({traffic_df['Visitors'].min():,} visitors)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary <a id='summary'></a>\n\n### Key Takeaways\n\n1. **Statistics is Essential**: Statistics provides the foundation for data analysis, machine learning, and informed decision-making. It helps us understand patterns, make predictions, and draw meaningful conclusions from data.\n\n2. **Two Main Branches**:\n   - **Descriptive Statistics**: Summarizes and describes data using measures like mean, median, standard deviation, and visualizations\n   - **Inferential Statistics**: Makes predictions and draws conclusions about populations based on sample data\n\n3. **Wide-Ranging Applications**: Statistics is applied across virtually every field including:\n   - Business and economics (market research, forecasting, quality control)\n   - Healthcare (clinical trials, disease prediction, patient analytics)\n   - Technology (machine learning, AI, recommendation systems)\n   - Social sciences (surveys, behavioral studies, education)\n   - Environmental science (climate modeling, resource management)\n\n4. **Python for Statistics**: Python offers powerful libraries for statistical analysis:\n   - **NumPy**: Fast numerical computations and basic statistics\n   - **Pandas**: Data manipulation and analysis with DataFrames\n   - **SciPy**: Advanced statistical functions and hypothesis testing\n   - **Matplotlib/Seaborn**: Data visualization and exploration\n\n5. **Visualization Matters**: Effective visualizations (histograms, box plots, scatter plots, heatmaps) help identify patterns, outliers, and relationships that might not be obvious from numerical summaries alone.\n\n### Next Steps for Learning\n\nTo deepen your understanding of statistics:\n\n1. **Study Probability Theory**: Understanding probability is crucial for inferential statistics and machine learning\n2. **Learn Distribution Types**: Explore normal, binomial, Poisson, and other probability distributions\n3. **Master Hypothesis Testing**: Learn t-tests, chi-square tests, ANOVA, and p-values\n4. **Explore Regression Analysis**: Linear regression, logistic regression, and other predictive models\n5. **Practice with Real Data**: Apply statistical methods to real-world datasets from Kaggle, UCI ML Repository, or your own projects\n6. **Understand Assumptions**: Learn when to use different statistical methods and their underlying assumptions\n7. **Study Experimental Design**: Learn how to design studies and experiments to collect meaningful data\n\n### Remember\n\n- Statistics is not just about calculating numbers; it's about asking the right questions and interpreting results correctly\n- Always visualize your data before applying statistical methods\n- Understand the context and limitations of your data and analyses\n- Correlation does not imply causation\n- Practice regularly with diverse datasets to build intuition",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Additional Resources\n\n### Recommended Libraries Documentation\n- [NumPy Documentation](https://numpy.org/doc/)\n- [Pandas Documentation](https://pandas.pydata.org/docs/)\n- [SciPy Stats Module](https://docs.scipy.org/doc/scipy/reference/stats.html)\n- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n- [Seaborn Documentation](https://seaborn.pydata.org/)\n\n### Practice Datasets\n- [Kaggle Datasets](https://www.kaggle.com/datasets)\n- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n- [Data.gov](https://www.data.gov/)\n\n### Topics to Explore Next\n- Probability and probability distributions\n- Hypothesis testing and statistical significance\n- Confidence intervals and margins of error\n- Regression analysis (linear and logistic)\n- ANOVA and experimental design\n- Time series analysis\n- Bayesian statistics",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}